+++
date = 2017-01-01T00:00:00  # Schedule page publish date.

title = "Relationship Between Intraclass Correlation (ICC) and Percent Agreement"
time_start = 2019-04-04T09:45:00
time_end = 2019-04-04T10:45:00
abstract = ""
abstract_short = ""
event = "The National Council on Measurement in Education"
event_url = "http://www.ncme.org/meetings/annualmeeting"
location = "Toronto, Canada"

# Is this a selected talk? (true/false)
selected = false

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter the filename (excluding '.md') of your project file in `content/project/`.
#projects = ["daacs"]

# Links (optional).
url_pdf = "/pdf/IRRsim-NCME2019.pdf"
url_slides = ""
url_video = ""
url_code = "https://github.com/jbryer/IRRsim"

# Does the content use math formatting?
math = true

# Does the content use source code highlighting?
highlight = true

# Featured image
# Place your image in the `static/img/` folder and reference its filename below, e.g. `image = "example.jpg"`.
#[header]
#image = "Sleeping_Empire.jpg"
#caption = "My caption :smile:"

+++

Inter-rater reliability (IRR) is a critical component of establishing the reliability of measures when more than one rater is necessary. There are numerous IRR statistics available to researchers including percent rater agreement, Cohen's Kappa, and several types of intraclass correlations (ICC). Several methodologists suggest using ICC over percent rater agreement (Hallgren, 2012; Koo & Li, 2016; McGraw & Wong, 1996; Shrout & Fleiss, 1979). However, the literature provides little guidance on the interpretation of ICC results. This article explores the relationship between ICC and percent rater agreement using simulations. Results suggest that ICC and percent rater agreement are highly correlated ($R^2 > 0.9$) for most designs.

______

