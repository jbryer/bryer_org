[
  {
    "objectID": "posts/2025-05-06-Downsampling.html",
    "href": "posts/2025-05-06-Downsampling.html",
    "title": "Downsampling for predictive modeling",
    "section": "",
    "text": "Note that this is cross posted with a vignette in the medley R package. For the most up-to-date version go here: https://jbryer.github.io/medley/articles/downsampling.html Comments can be directed to me on Mastodon at @vis.social@jbryer.\nTo install the development version of the medley package, use the following command:\nremotes::install_github('jbryer/medley')\nOne of the challenges in predictive modeling occurs when the dependent variable is imbalanced (i.e. the ratio of one class to the other is high, generally greater than 80-to-20). Several strategies have been proposed to address the imbalance including upsampling and downsampling. Upsampling involves duplicating data from the smaller class to better match the number of observations from the larger class. The disadvantage of upsampling is that new data is being created that could potentially cause overfitting. Additionally, by artificially increasing the sample size standard errors will also be artificially decreased. Downsampling involves randomly selecting from the larger class to achieve better balance. The disadvantage of downsampling is that some data, and sometimes a lot of data, is excluded from the model.\nThis paper introduces a procedure that downsamples while using all available data by training multiple models. For example, consider a dataset with 1,000 observations, 900 are of class A and 100 are of class B. Assuming we wish to have perfect balance between A and B, we would randomly assign the 900 class A observations to one of nine models. We can then pool the predictions across the nine models."
  },
  {
    "objectID": "posts/2025-05-06-Downsampling.html#working-example",
    "href": "posts/2025-05-06-Downsampling.html#working-example",
    "title": "Downsampling for predictive modeling",
    "section": "Working Example",
    "text": "Working Example\n\nlibrary(medley)\ndata('pisa', package = 'medley')\ndata('pisa_variables', package = 'medley')\n\nThe Programme of International Student Assessment (PISA) is international study conducted by the Organisation for Economic Co-operation and Development (OECD) every three years. It assesses 15-year-old students in mathematics, science, and reading while collecting information about the students and their schools. The pisa dataset included in the medley package comes from the 2009 administration and is used to demonstrate predicting private versus public school attendance. There are 5,233 observations across 44 variables with 93.4% public school students and 6.6% private school students.\nTo begin, we will split the data into a training and validation set using the splitstackshape::stratified() function to ensure that the ratio of public-to-private school students is the same in both datasets.\n\npisa_formu &lt;- Public ~ .\nnames(pisa) &lt;- pisa_variables[names(pisa)]\npisa_splits &lt;- splitstackshape::stratified(\n    pisa, group = \"Public\", size = 0.75, bothSets = TRUE)\npisa_train &lt;- pisa_splits[[1]] |&gt; as.data.frame()\npisa_valid &lt;- pisa_splits[[2]] |&gt; as.data.frame()\n\n\ntable(pisa$Public, useNA = 'ifany') |&gt; print() |&gt; prop.table()\n\n\nFALSE  TRUE \n  345  4888 \n\n\n\n     FALSE       TRUE \n0.06592777 0.93407223 \n\ntable(pisa_train$Public, useNA = 'ifany') |&gt; print() |&gt; prop.table()\n\n\nFALSE  TRUE \n  259  3666 \n\n\n\n     FALSE       TRUE \n0.06598726 0.93401274 \n\ntable(pisa_valid$Public, useNA = 'ifany') |&gt; print() |&gt; prop.table()\n\n\nFALSE  TRUE \n   86  1222 \n\n\n\n     FALSE       TRUE \n0.06574924 0.93425076 \n\n\nWe can estimate a logistic regression model and get the predicted probabilities for the validation dataset.\n\npisa_lr_out &lt;- glm(pisa_formu, data = pisa_train, family = binomial(link = 'logit'))\npisa_predictions &lt;- predict(pisa_lr_out, newdata = pisa_valid, type = 'response')\n\nThe figure below shows the distribution of predicted probabilities for the validation dataset. There is some separation between public and private school students, but the densities are clearly centered to the right side of the range.\n\nggplot(data.frame(Public = pisa_valid$Public, \n                  Prediction = pisa_predictions), \n       aes(x = Prediction, color = Public)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe figure below provides a receiver operator characteristic (ROC) curve along with a plot of the accuracy, sensitivity, and specificity.\n\ncalculate_roc(predictions = pisa_predictions, \n              observed = pisa_valid$Public) |&gt; plot()\n\n\n\n\n\n\n\n\nThe confusion matrix below, splitting at 0.5, indicates that this model is no better than the null model (i.e percent public school students is 93.4%). Of course we could adjust that cut value to optimize either the specificity or sensitivity.\n\nconfusion_matrix(observed = pisa_valid$Public, \n                 predicted = pisa_predictions &gt; 0.5)\n\n           predicted              \n  observed     FALSE          TRUE\n     FALSE 1 (0.08%)    85 (6.50%)\n      TRUE 4 (0.31%) 1218 (93.12%)\nAccuracy: 93.2%\nSensitivity: 1.16%\nSpecificity: 99.67%"
  },
  {
    "objectID": "posts/2025-05-06-Downsampling.html#shrinking-fitted-values",
    "href": "posts/2025-05-06-Downsampling.html#shrinking-fitted-values",
    "title": "Downsampling for predictive modeling",
    "section": "Shrinking Fitted Values",
    "text": "Shrinking Fitted Values\nIt turns out that the range of fitted values from logistic regression will shrink as the amount of imbalance in the dependent variable increases. I first encountered this issue when estimating propensity scores for my dissertation in a study of charter versus traditional public school students. In that study using the National Assessment of Educational Progress (NAEP) approximately 3% of students attended a charter school. In that study, the range of propensity scores were severely constrained. To explore why, the multilevel::psrange() function was developed The result of this function is the figure below. Starting at the bottom, 345 public school students were randomly selected so that the logistic regression could be estimated where there is perfect balance in the dependent variable. As we move up we increase the ratio from 1:1 to 1:13. For each ratio, 20 random samples are drawn, logistic regression model estimated, and the minimum and maximum fitted values (i.e. predicted probabilities) are recorded (they are represented by the black dots and green bars). The distributions across all models are also included.\n\n\n\n\n\n\n\n\n\nPlotting just the ranges along with the mean of the fitted values for public (blue) and private (green) school students shows that once the ratio is greater than 3-to-1 the mean of the fitted values for the zero class (private schools in this example) is greater than 0.5."
  },
  {
    "objectID": "posts/2025-05-06-Downsampling.html#downsampling",
    "href": "posts/2025-05-06-Downsampling.html#downsampling",
    "title": "Downsampling for predictive modeling",
    "section": "Downsampling",
    "text": "Downsampling\nAs discussed above one of the key disadvantages of downsampling is that in situations where there is significant imbalance we are excluding a lot of data from analysis. The downsample() function will first determine how many models need to be estimated such that each observation from the larger class is used exactly once. For this example we are using a public-to-private student ratio of 2-to-1 so that for each model estimated there are 259 private and 518 public student observations. Given there are 3925 observations in our training set, the dowmsample() function will estimate 7 models.\n\npisa_ds_out &lt;- downsample(\n  formu = pisa_formu,\n  data = pisa_train,\n  model_fun = glm,\n  ratio = 2,\n  family = binomial(link = 'logit'))\nlength(pisa_ds_out)\n\n[1] 7\n\n\nWe can use the predict() function to get a data frame of predictions. Each column corresponds to the predicted value for each of the 7 models.\n\npisa_predictions_ds &lt;- predict(pisa_ds_out,\n                               newdata = pisa_valid, \n                               type = 'response')\nhead(pisa_predictions_ds)\n\n     model1    model2    model3    model4    model5    model6    model7\n1 0.8511828 0.7437341 0.8921605 0.8424369 0.7347052 0.8697531 0.6928875\n2 0.7393116 0.6822714 0.9466815 0.7959953 0.8118642 0.9441840 0.9580830\n3 0.4944206 0.3813138 0.5575741 0.3586561 0.5023435 0.5805062 0.6281852\n4 0.8525691 0.8268514 0.8293386 0.8372777 0.9464037 0.8843848 0.9016058\n5 0.1823382 0.3670335 0.4556063 0.1408078 0.1899378 0.3578418 0.2657968\n6 0.9216160 0.8192096 0.9040353 0.9213184 0.8080822 0.9076342 0.8768295\n\n\nWe can average the predictions to get a single vector.\n\npisa_predictions_ds2 &lt;- pisa_predictions_ds |&gt; apply(1, mean)\n\nThe density distributions are provided below. These distributions are more like the distributions we expect when we have balanced data even though we did use all the observations to get these predicted probabilities.\n\nggplot(data.frame(Public = pisa_valid$Public, \n                  Prediction = pisa_predictions_ds2), \n       aes(x = Prediction, color = Public)) +\n  geom_density()\n\n\n\n\n\n\n\n\nAlthough the downsample() function appears to address the issue of shrinking and off centered fitted values, the model performance metrics provided below suggest that it did not improve the overall performance of the model predictions.\n\nroc &lt;- calculate_roc(predictions = pisa_predictions_ds2, \n                     observed = pisa_valid$Public)\nplot(roc)\n\n\n\n\n\n\n\n\n\nconfusion_matrix(observed = pisa_valid$Public, \n                 predicted = pisa_predictions_ds2 &gt; 0.5)\n\n              predicted              \n  observed        FALSE          TRUE\n     FALSE   45 (3.44%)    41 (3.13%)\n      TRUE 204 (15.60%) 1018 (77.83%)\nAccuracy: 81.27%\nSensitivity: 52.33%\nSpecificity: 83.31%"
  },
  {
    "objectID": "posts/2025-05-06-Downsampling.html#appendix-model-summaries",
    "href": "posts/2025-05-06-Downsampling.html#appendix-model-summaries",
    "title": "Downsampling for predictive modeling",
    "section": "Appendix: Model Summaries",
    "text": "Appendix: Model Summaries\nAbove we averaged the predicted values across all the models to get a single prediction for each observation in our validation dataset. However, it is possible to pool models using the mice::pool() function to get a single set of regression coefficients. The table below provides the pooled regression coefficients from the downsample function along with the coefficients from the logistic regression model using all the data.\n\n\n\n\nPooled from downsamplesComplete data\n\n(Intercept)5.516 * (2.376)7.862 *** (1.657)\n\nSexMale-0.823 ** (0.244)-0.762 *** (0.149)\n\n`Attend &lt;ISCED 0&gt;`Yes, one year or less0.495 (0.286)0.498 ** (0.190)\n\n`Age at &lt;ISCED 1&gt;`0.087 (0.186)0.075 (0.104)\n\n`Repeat &lt;ISCED 1&gt;`Yes, once0.693 (0.548)0.645 (0.345)\n\n`At Home - Mother`TRUE-0.888 (0.843)-0.921 (0.493)\n\n`At Home - Father`TRUE-0.672 (0.414)-0.620 * (0.277)\n\n`At Home - Brothers`TRUE0.184 (0.313)0.155 (0.146)\n\n`At Home - Sisters`TRUE0.563 * (0.237)0.454 ** (0.146)\n\n`At Home - Grandparents`TRUE-0.725 * (0.328)-0.648 ** (0.201)\n\n`At Home - Others`TRUE-0.094 (0.332)-0.136 (0.221)\n\n`Mother  &lt;Highest Schooling&gt;`&lt;ISCED level 3A&gt;0.101 (0.631)0.070 (0.388)\n\n`Mother Current Job Status`Other-0.457 (0.609)-0.303 (0.364)\n\n`Mother Current Job Status`Working Full-time-0.608 (0.537)-0.443 (0.339)\n\n`Mother Current Job Status`Working Part-Time-0.586 (0.650)-0.446 (0.369)\n\n`Father  &lt;Highest Schooling&gt;`&lt;ISCED level 2&gt;0.072 (1.219)0.077 (0.832)\n\n`Father  &lt;Highest Schooling&gt;`&lt;ISCED level 3A&gt;-0.579 (1.121)-0.658 (0.754)\n\n`Father Current Job Status`Other-0.019 (0.737)-0.065 (0.394)\n\n`Father Current Job Status`Working Full-time0.356 (0.604)0.236 (0.324)\n\n`Father Current Job Status`Working Part-Time1.260 (0.892)0.998 (0.529)\n\n`Language at home`Language of test0.137 (0.489)-0.104 (0.263)\n\n`Possessions desk`TRUE-0.583 (0.404)-0.531 * (0.265)\n\n`Possessions own room`TRUE0.521 (0.384)0.600 * (0.238)\n\n`Possessions study place`TRUE-0.056 (0.488)-0.223 (0.303)\n\n`Possessions  computer`TRUE-0.077 (0.855)-0.038 (0.592)\n\n`Possessions software`TRUE0.365 (0.332)0.358 * (0.161)\n\n`Possessions Internet`TRUE-1.416 (0.917)-1.177 (0.602)\n\n`Possessions literature`TRUE-0.619 * (0.295)-0.551 ** (0.175)\n\n`Possessions poetry`TRUE-0.369 (0.308)-0.250 (0.176)\n\n`Possessions art`TRUE-0.402 (0.333)-0.273 (0.196)\n\n`Possessions textbooks`TRUE-0.021 (0.356)-0.007 (0.214)\n\n`Possessions dictionary`TRUE0.100 (0.583)-0.000 (0.422)\n\n`Possessions dishwasher`TRUE-0.074 (0.336)-0.078 (0.234)\n\n`How many cellular phones`Three or more-0.851 (0.987)-0.906 (0.741)\n\n`How many cellular phones`Two-0.192 (0.985)-0.478 (0.771)\n\n`How many televisions`Three or more1.378 * (0.645)0.995 *** (0.302)\n\n`How many televisions`Two0.816 (0.640)0.519 (0.324)\n\n`How many computers`One0.589 (1.188)0.343 (0.823)\n\n`How many computers`Three or more0.174 (1.168)-0.072 (0.838)\n\n`How many computers`Two0.079 (1.143)-0.120 (0.832)\n\n`How many cars`Three or more0.038 (0.458)-0.041 (0.295)\n\n`How many cars`Two-0.223 (0.427)-0.264 (0.291)\n\n`How many rooms bath or shower`Three or more-1.001 * (0.427)-0.703 ** (0.238)\n\n`How many rooms bath or shower`Two-0.223 (0.371)-0.107 (0.217)\n\n`How many books at home`101-200 books-0.255 (0.440)-0.367 (0.327)\n\n`How many books at home`11-25 books-0.158 (0.431)-0.158 (0.339)\n\n`How many books at home`201-500 books-0.998 * (0.477)-0.985 ** (0.334)\n\n`How many books at home`26-100 books-0.498 (0.395)-0.489 (0.302)\n\n`How many books at home`More than 500 books-1.082 (0.558)-1.042 ** (0.366)\n\n`Reading Enjoyment Time`30 minutes or less a day-0.071 (0.477)-0.183 (0.251)\n\n`Reading Enjoyment Time`Between 30 and 60 minutes0.494 (0.457)0.311 (0.283)\n\n`Reading Enjoyment Time`I don't read for enjoyment-0.019 (0.466)-0.118 (0.259)\n\n`Reading Enjoyment Time`More than 2 hours a day0.245 (0.747)0.208 (0.406)\n\n`&lt;Enrich&gt; in &lt;test lang&gt;`TRUE-0.012 (0.671)0.242 (0.413)\n\n`&lt;Enrich&gt; in &lt;mathematics&gt;`TRUE-0.236 (0.569)-0.276 (0.323)\n\n`&lt;Enrich&gt; in &lt;science&gt;`TRUE0.576 (0.634)0.367 (0.410)\n\n`&lt;Remedial&gt; in &lt;test lang&gt;`TRUE0.311 (0.964)0.041 (0.524)\n\n`&lt;Remedial&gt; in &lt;mathematics&gt;`TRUE-0.511 (0.685)-0.614 (0.384)\n\n`&lt;Remedial&gt; in &lt;science&gt;`TRUE-0.197 (0.789)0.245 (0.496)\n\n`Out of school lessons &lt;test lang&gt;`Do not attend-0.279 (0.815)-0.106 (0.493)\n\n`Out of school lessons &lt;test lang&gt;`Less than 2 hours a week-0.685 (0.712)-0.533 (0.465)\n\n`Out of school lessons &lt;maths&gt;`4 up to 6 hours per week0.237 (0.902)0.386 (0.541)\n\n`Out of school lessons &lt;maths&gt;`Do not attend-0.254 (0.798)-0.160 (0.410)\n\n`Out of school lessons &lt;maths&gt;`Less than 2 hours a week-0.117 (0.618)0.122 (0.369)\n\n`Out of school lessons &lt;science&gt;`4 up to 6 hours per week-1.091 (0.789)-0.837 (0.549)\n\n`Out of school lessons &lt;science&gt;`Do not attend-0.565 (0.717)-0.456 (0.472)\n\n`Out of school lessons &lt;science&gt;`Less than 2 hours a week-0.534 (0.751)-0.513 (0.464)\n\nn783     3925.000 \n\n *** p &lt; 0.001;  ** p &lt; 0.01;  * p &lt; 0.05."
  },
  {
    "objectID": "posts/2025-02-06-Waiting_to_pass_exam.html",
    "href": "posts/2025-02-06-Waiting_to_pass_exam.html",
    "title": "How many times do I need to take a test to randomly get all questions correct?",
    "section": "",
    "text": "Darrin Rogers asked on Mastadon what are the “number of tries it would take, guessing randomly, to get 100% on a quiz if you had unlimited retries.” Here we will outline two ways to solve this problem: using a simulation and using a combination of the binomial and geometric distributions. Let’s consider an example of a 5 question test where each question has four options, hence the probability of getting any one question correct is 1/4.\n\nsize &lt;- 5 # Test size (i.e. number of questions)\np &lt;- 1/4 # Probability of randomly getting correct answer\n\nWe can use the sample function to simulate on test attempt.\n\ntest &lt;- sample(c(TRUE, FALSE), size = size, prob = c(p, 1 - p), replace = TRUE)\ntest\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\n\nNext, let’s write a function that will simulate repeatedly take a test until all the questions are correct. I have added an additional parameter stop_score which specifies the mean score on the test before stopping. This will allow us to modify the question to answer how many tests do I need to take to pass. For now, stop_score = 1 will continue until all questions are correct.\n\n#' Simulate how long until a specified number of responses are correct\n#' @param size test size.\n#' @param prob probability of randomly getting correct answer\n#' @param stop_score the score on the test we wish to achieve. Value of 1\n#'        indicates a perfect score.\nsimulate_test &lt;- function(size, p, stop_score = 1) {\n    n &lt;- 0\n    repeat{\n        n &lt;- n + 1\n        test &lt;- sample(c(TRUE, FALSE),\n                       size = size,\n                       prob = c(p, 1 - p),\n                       replace = TRUE)\n        if(mean(test) &gt;= stop_score) {\n            break\n        }\n    }\n    return(n)\n}\n\nWe can run one test to see how long we need to wait until all questions on the test were answered correctly.\n\n(num_tests &lt;- simulate_test(size = size, p = p))\n\n[1] 158\n\n\nFor this one simulation, it took 158 to randomly get all the questions correct. Let’s now run this simulation 1,000 times.\n\nsimulations &lt;- integer(1000)\nfor(i in 1:length(simulations)) {\n    simulations[i] &lt;- simulate_test(size = size, p = p)\n}\nmean(simulations)\n\n[1] 977.858\n\nmedian(simulations)\n\n[1] 687\n\n\nFor this simulation the average “wait time” until all questions were answered correctly is 977.858. Since the distribution is not symmetrical it may be more appropriate to use the median. Here, 50% of the simulations returned a perfect score in fewer than 687 attempts.\n\nggplot(data.frame(x = simulations), aes(x = x)) +\n    geom_histogram(aes(y = ..density..), bins = 50, fill = 'grey70') +\n    geom_density(color = 'blue') +\n    ggtitle('Distribution of simulation results')\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nLet’s return to a single test attempt. We can use the binomial distribution to calculate the probability of getting k questions correct on this 5 question test.\n\ndist &lt;- dbinom(x = 0:size, size = size, prob = p)\nggplot(data.frame(x = 0:size,                          \n                  prob = dist,\n                  label = paste0(round(100 * dist, digits = 2), '%')),\n       aes(x = x, y = prob, label = label)) +\n    geom_bar(stat = 'identity', fill = 'grey50') +\n    geom_text(vjust = -0.5)\n\n\n\n\n\n\n\n\nThe probability of getting all 5 questions on this test is 0.0009766. We can now treat each test attempt as a Bernoulli trial where the probability of success is 0.0009766. The geometric distribution gives us the number of Bernoulli trials we need to get one success. The mean for the geometric distribution are:\n\\[ \\mu = \\frac{1}{p} \\]\nTherefore, it will take an average of 1024 test attempts before getting all questions correct on the attempt.\n\n(p_all_correct &lt;- dbinom(x = size, size = size, prob = p))\n\n[1] 0.0009765625\n\n1 / p_all_correct\n\n[1] 1024\n\n\nHowever, the geometric distribution is not symmetrical so using the mean not be desirable. Here is the geometric distribution for where the probability of success is 0.0009766.\n\ngeom_dist &lt;- data.frame(x = 0:5000,\n                        y = dgeom(0:5000, prob = dbinom(x = size, size = size, prob = p)))\ncut_point50 &lt;- qgeom(0.50, prob = dbinom(x = size, size = size, prob = p))\ncut_point95 &lt;- qgeom(0.95, prob = dbinom(x = size, size = size, prob = p))\nggplot(geom_dist, aes(x = x, y = y)) +\n    geom_polygon(data = rbind(data.frame(x = 0, y = 0),\n                              geom_dist[geom_dist$x &lt; cut_point95,],\n                              data.frame(x = cut_point95, y = 0)),\n                 fill = 'grey70') +\n    geom_polygon(data = rbind(data.frame(x = 0, y = 0),\n                              geom_dist[geom_dist$x &lt; cut_point50,],\n                              data.frame(x = cut_point50, y = 0)),\n                 fill = 'grey50') +\n    geom_path(stat = 'identity', color = 'blue')\n\n\n\n\n\n\n\n\nThe shaded area corresponds to 50% of the area. That is, if we conduct 709 tests we are 50% likely to get a test with all the answers correct. Want to be 95% sure to get a test with all answers correct, then administer 3066 tests.\nWe can tweak the question slightly: What is the average number of tests I would have to take before passing if the answers are randomly selected? For this example, I am considering getting 4 or 5 questions correct passing. We can get the probability of getting 4 or 5 questions correct from the binomial distribution, which is 0.015625.\n\np_pass &lt;- dbinom(x = 4:5, size = size, prob = p) |&gt; sum()\n1 / p_pass\n\n[1] 64\n\n\nTo just pass, we have to wait much less. We can also calculate this using the simulate_test function defined above.\n\nsimulations2 &lt;- integer(1000)\nfor(i in 1:length(simulations2)) {\n    simulations2[i] &lt;- simulate_test(size = size, p = p, stop_score = 0.8)\n}\nmean(simulations2)\n\n[1] 62.291\n\nmedian(simulations2)\n\n[1] 44\n\n\nOr using the geometric distribution:\n\nqgeom(0.50, prob = p_pass)\n\n[1] 44\n\nqgeom(0.95, prob = p_pass)\n\n[1] 190"
  },
  {
    "objectID": "posts/2025-03-04-chi_squared_sample_sizes.html",
    "href": "posts/2025-03-04-chi_squared_sample_sizes.html",
    "title": "Sample size and statistical significance for chi-squared tests",
    "section": "",
    "text": "In this post we are going to explore the relationship between sample size (n) and statistical significance for the chi-squared (\\(\\chi^2\\)) test. Recall that from the normal distribution, we construct a confidence interval using:\n\\[ CI = \\bar{x} \\pm z \\cdot SE\\]\nwhere z is the test statistic and:\n\\[ SE =  \\frac{s}{\\sqrt{n}} \\]\nwhere s is the sample standard deviation. Typically our null is zero in which case we reject the null hypothesis when the confidence does not span zero. If we wish to construct a 95% confidence interval, then \\(z = 1.96\\). Assuming the sample standard deviation is constant regardless of sample size (a fair assumption), then as n increases the standard error decreases. The following calculates the confidence interval for n ranging from 10 to 400 assuming a sample standard deviation of 0.15 and 95% confidence level. When \\(n &gt; 171\\) then \\(p &lt; 0.05\\).\n\n# Define some parameters\nsig_level &lt;- .95  # Significance level, 95% here\nes &lt;- 0.15        # Effect size in standard units\nnull_val &lt;- 0     # null value\n\n#' Calculate the standard error\n#' \n#' This function will calculate the standard error from a vector of observations or with a given\n#' sample standard deviation and sample size.\n#' \n#' @param x numeric vector of observations.\n#' @param sigma the sample standard deviation.\n#' @param n sample size.\nstandard_error &lt;- function(x, sigma = sd(x), n = length(x)) {\n    if(!missing(x)) { # Some basic error checking\n        if(sigma != sd(x)) { warning('The sample standard deviation (sigma) is not equal to sd(x)')}\n        if(n != length(x)) { warning('The sample size (n) is not equal to length(x).' )}\n    }\n    return(sigma / sqrt(n))\n}\n# Create a data.frame with varying sample sizes and the corresponding standard error\ndf &lt;- data.frame(\n    n = 10:400,\n    se = standard_error(sigma = 1, n = 10:400)\n)\ncv &lt;- abs(qnorm((1 - sig_level) / 2)) # Critical value (z test statistic)\ndf$ci_low &lt;- es - cv * df$se\ndf$ci_high &lt;- es + cv * df$se\ndf$sig &lt;- null_val &lt; df$ci_low | null_val &gt; df$ci_high\nmin_n &lt;- df$n[df$sig] |&gt; min()\nggplot(df, aes(x = n, y = se, color = sig)) + \n    geom_path() +\n    geom_point() +\n    scale_color_brewer(paste0('p &lt; ', (1 - sig_level)), type = 'qual', palette = 6) +\n    ggtitle(paste0('Minumum n for p &lt; ', (1 - sig_level), ': ', min_n),\n            subtitle = paste0('effect size: ', es, '; null value: ', null_val))\n\n\n\n\n\n\n\n\nThe chi-squared (\\(\\chi^2\\)) test statistic is defined as:\n\\[ \\chi^2 = \\sum{\\frac{(O - E)^2}{E}} \\]\nwhere O is the observed count and E is the expected count. Unlike the standard error for numerical data, n is not explicitly in the formula and therefore makes it a bit more challenging to determine the impact sample size has rejecting the null hypothesis. Moreover, since the chi-squared is calculated from the cell counts in a table of varying length and dimension (one- or two-dimensions specifically) determining how n impacts rejecting the null or not requires more parameters.\nAnswering the question of how large does n need to be to detect a statistically significant result (i.e. to reject the null hypothesis) is refereed to as power. Whereas for calculating the power for numerical data had one parameter, the sample standard deviation, here we need to consider the proportion of observations within different cells. For example, consider we have a variable with three levels and we expect the proportion of observations in the three groups to be 33%, 25%, and 42%, respectively. If our sample size is 100 then we expect there to be 33, 25, and 42 and observations for the three categories. This function will, for varying sample sizes, calculate the counts for the categories to achieve that sample size, estimate the chi-squared statistic and record the p-value. There are other parameters that are documented below. A plot function is also defined using the S3 objected oriented framework.\n\n#' Calculate p-value from a chi-squared test with varying sample sizes\n#'\n#' This algorithm will start with an initial sample size (`n_start`) and perform a chi-squared test\n#' with a vector of counts equal to `n * probs`. This will repeat increasing the sample size by\n#' `n_step` until the p-value from the chi-squared test is less than `p_stop`.\n#'\n#' @param vector of cell probabilities. The sum of the values must equal 1.\n#' @param sig_level significance level.\n#' @param p_stop the p-value to stop estimating chi-squared tests.\n#' @param max_n maximum n to attempt if `p_value` is never less than `p_stop`.\n#' @param min_cell_size minimum size per cell to perform the chi-square test.\n#' @param n_start the starting sample size.\n#' @param n_step the increment for each iteration.\n#' @return a data.frame with three columns: n (sample size), p_value, and sig (TRUE if\n#'         p_value &lt; sig_level).\n#' @importFrom DescTools power.chisq.test CramerV\nchi_squared_power &lt;- function(\n        probs,\n        sig_level = 0.05,\n        p_stop = 0.01,\n        power = 0.80,\n        power_stop = 0.90,\n        max_n = 100000,\n        min_cell_size = 10,\n        n_start = 10,\n        n_step = 10\n) {\n    if(sum(probs) != 1) { # Make sure the sum is equal to 1\n        stop('The sum of the probabilities must equal 1.')\n    } else if(length(unique(probs)) == 1) {\n        stop('All the probabilities are equal.')\n    }\n\n    n &lt;- n_start\n    p_values &lt;- numeric()\n    power_values &lt;- numeric()\n    df &lt;- ifelse(is.vector(probs),\n                 length(probs) - 1,\n                 min(dim(probs)) - 1) # Degrees of freedom\n    repeat {\n        x &lt;- (probs * n) |&gt; round()\n        if(all(x &gt; min_cell_size)) {\n            cs &lt;- chisq.test(x, rescale.p = TRUE, simulate.p.value = FALSE)\n            p_values[n / n_step] &lt;- cs$p.value\n            pow &lt;- DescTools::power.chisq.test(\n                n = n,\n                w = DescTools::CramerV(as.table(x)),\n                df = df,\n                sig.level = sig_level\n            )\n            power_values[n / n_step] &lt;- pow$power\n            if((cs$p.value &lt; p_stop & pow$power &gt; power_stop) | n &gt; max_n) {\n                break;\n            }\n        } else {\n            p_values[n / n_step] &lt;- NA\n            power_values[n / n_step] &lt;- NA\n        }\n        n &lt;- n + n_step\n    }\n    result &lt;- data.frame(n = seq(10, length(p_values) * n_step, n_step),\n                         p_value = p_values,\n                         sig = p_values &lt; sig_level,\n                         power = power_values)\n    class(result) &lt;- c('chisqpower', 'data.frame')\n    attr(result, 'probs') &lt;- probs\n    attr(result, 'sig_level') &lt;- sig_level\n    attr(result, 'p_stop') &lt;- p_stop\n    attr(result, 'power') &lt;- power\n    attr(result, 'power_stop') &lt;- power_stop\n    attr(result, 'max_n') &lt;- max_n\n    attr(result, 'n_step') &lt;- n_step\n    return(result)\n}\n\n#' Plot the results of chi-squared power estimation\n#'\n#' @param x result of [chi_squared_power()].\n#' @param plot_power whether to plot the power curve.\n#' @param plot_p whether to plot p-values.\n#' @param digits number of digits to round to.\n#' @param segement_color color of the lines marking where power and p values exceed threshold.\n#' @param sgement_linetype linetype of the lines marking where power and p values exceed threshold.\n#' @param p_linetype linetype for the p-values.\n#' @param power_linetype linetype for the power values.\n#' @param title plot title. If missing a title will be automatically generated.\n#' @parma ... currently not used.\n#' @return a ggplot2 expression.\nplot.chisqpower &lt;- function(\n        x,\n        plot_power = TRUE,\n        plot_p = TRUE,\n        digits = 4,\n        segment_color = 'grey60',\n        segment_linetype = 1,\n        p_linetype = 1,\n        power_linetype = 2,\n        title,\n        ...\n) {\n    pow &lt;- attr(x, 'power')\n\n    p &lt;- ggplot(x[!is.na(x$p_value),], aes(x = n, y = p_value))\n\n    if(plot_power) {\n        if(any(x$power &gt; pow, na.rm = TRUE)) {\n            min_n_power &lt;- min(x[x$power &gt; pow,]$n, na.rm = TRUE)\n            p &lt;- p +\n                geom_segment(\n                    x = 0,\n                    xend = min_n_power,\n                    y = pow,\n                    yend = pow,\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = 0,\n                    y =  pow,\n                    label = paste0('Power = ',  pow),\n                    vjust = -1,\n                    hjust = 0) +\n                geom_segment(\n                    x = min_n_power,\n                    xend = min_n_power,\n                    y = pow,\n                    yend = 0,\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = min_n_power, y = 0,\n                    label = paste0('n = ', prettyNum(min_n_power, big.mark = ',')),\n                    vjust = 1,\n                    hjust = -0.1)\n        }\n        p &lt;- p +\n            geom_path(\n                aes(y = power),\n                color = '#7570b3',\n                linetype = power_linetype)\n    }\n    if(plot_p) {\n        if(any(x$sig, na.rm = TRUE)) {\n            p &lt;- p +\n                geom_segment(\n                    x = 0,\n                    xend = min(x[x$sig,]$n, na.rm = TRUE),\n                    y = attr(x, 'sig_level'),\n                    yend = attr(x, 'sig_level'),\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = 0,\n                    y =  attr(x, 'sig_level'),\n                    label = paste0('p = ',  attr(x, 'sig_level')),\n                    vjust = -1,\n                    hjust = 0) +\n                geom_segment(\n                    x = min(x[x$sig,]$n, na.rm = TRUE),\n                    xend = min(x[x$sig,]$n, na.rm = TRUE),\n                    y = attr(x, 'sig_level'),\n                    yend = 0,\n                    color = segment_color,\n                    linetype = segment_linetype) +\n                ggplot2::annotate(\n                    geom = 'text',\n                    x = min(x[x$sig,]$n, na.rm = TRUE),\n                    y = 0,\n                    label = paste0('n = ', prettyNum(min(x[x$sig,]$n, na.rm = TRUE), big.mark = ',')),\n                    vjust = 1,\n                    hjust = -0.1)\n        }\n        p &lt;- p +\n            geom_path(\n                alpha = 0.7,\n                linetype = p_linetype)\n            # geom_point(aes(color = sig), size = 1) +\n            # scale_color_brewer(paste0('p &lt; ', attr(x, 'sig_level')), type = 'qual', palette = 6)\n    }\n\n    if(missing(title)) {\n        if(any(x$power &gt; pow, na.rm = TRUE) & any(x$sig, na.rm = TRUE)) {\n            min_n &lt;- min(x[x$sig & x$power &gt; pow,]$n, na.rm = TRUE)\n            title &lt;- paste0('Smallest n where p &lt; ', attr(x, 'sig_level'), ' and power &gt; ', pow, ': ',\n                            prettyNum(min_n, big.mark = ','))\n        } else {\n            title &lt;- paste0('No n found where p &lt; ', attr(x, 'sig_level'), ' and power &gt; ', pow)\n        }\n    }\n\n    p &lt;- p +\n        ylim(c(0, 1)) +\n        ylab('') +\n        xlab('Sample Size') +\n        ggtitle(title,\n                subtitle = paste0('Probabilities: ', paste0(round(attr(x, 'probs'), digits = digits), collapse = ', ')))\n\n    return(p)\n}\n\nReturning to our example above where the cell proportions are 33%, 25%, and 42%, we would need \\(n \\ge 130\\) to reject the null hypothesis.\n\ncsp1 &lt;- chi_squared_power(probs =  c(.33, .25, .42))\ncsp1[csp1$sig,]$n |&gt; min(na.rm = TRUE) # Smallest n that results in p &lt; 0.05\n\n[1] 130\n\nplot(csp1)\n\n\n\n\n\n\n\n\nIn the next example we have much smaller differences between the cells with 25%, 25%, 24%, and 26%. In this example \\(n \\ge 9,710\\) before rejecting the null hypothesis.\n\ncsp3 &lt;- chi_squared_power(probs = c(.25, .25, .24, .26), max_n = 20000)\ncsp3[csp3$sig,]$n |&gt; min(na.rm = TRUE) # Smallest n that results in p &lt; 0.05\n\n[1] 9710\n\nplot(csp3)\n\n\n\n\n\n\n\n\nThis function will work with two-dimensional data as well (i.e. across two variables). The following example from Agresti (2007) looks at the political affiliation across sex (see the help documentation for chisq.test().).\n\nM &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\ndimnames(M) &lt;- list(gender = c(\"Femal\", \"Male\"),\n                    party = c(\"Democrat\", \"Independent\", \"Republican\"))\nM\n\n       party\ngender  Democrat Independent Republican\n  Femal      762         327        468\n  Male       484         239        477\n\nsum(M)\n\n[1] 2757\n\n\nThe chi-squared test suggests we should reject the null hypothesis test.\n\nchisq.test(M)\n\n\n    Pearson's Chi-squared test\n\ndata:  M\nX-squared = 30.07, df = 2, p-value = 2.954e-07\n\nDescTools::CramerV(M) # Effect size\n\n[1] 0.1044358\n\nDescTools::power.chisq.test(n = sum(M),\n                            w = DescTools::CramerV(M),\n                            df = min(dim(M)) - 1,\n                            sig.level = 1 - sig_level)\n\n\n     Chi squared power calculation \n\n              w = 0.1044358\n              n = 2757\n             df = 1\n      sig.level = 0.05\n          power = 0.9997872\n\nNOTE: n is the number of observations\n\n\nAgresti had a sample size of 2757, but we can ask the question what is the minimum sample size would they need to detect statistical significance? First, we convert the counts to proportions, then we can use the chi_squared_power() function to find the minimum sample size to reject the null hypothesis test.\n\nM_prob &lt;- M / sum(M) # Convert the counts to percentages\ncsp4 &lt;- chi_squared_power(probs = M_prob)\nplot(csp4)\n\n\n\n\n\n\n\n\nFor a more robust application for estimating power for many statistical tests, check out the pwsrr R package and corresponding Shiny application."
  },
  {
    "objectID": "posts/2012-04-27-Graphic_Parameters.html",
    "href": "posts/2012-04-27-Graphic_Parameters.html",
    "title": "Graphic Parameters (symbols, line types, and colors) for ggplot2",
    "section": "",
    "text": "Following up on John Mount’s post on remembering symbol parameters in ggplot2, I decided to give it a try and included symbols, line types, and colors (based upon Earl Glynn’s wonderful color chart).  Code follows below.\n\nlibrary(ggplot2)\nlibrary(grid)\n\n#Borrowed (i.e. stollen) from http://research.stowers-institute.org/efg/R/Color/Chart/ColorChart.R\ngetColorHexAndDecimal &lt;- function(color) {\n    if(is.na(color)) {\n        return(NA)\n    } else {\n        c &lt;- col2rgb(color)\n        return(sprintf(\"#%02X%02X%02X   %3d %3d %3d\", c[1],c[2],c[3], c[1], c[2], c[3]))\n    }\n}\n\n\nSymbols\n\nggplot(data=data.frame(x=c(0:25))) + geom_point(size=10, aes(x=x,y=x,shape=x)) +\n    facet_wrap(~ x, scales='free') + xlab('') + ylab('') +\n    scale_shape_identity() +\n    theme_void()\n\n\n\n\n\n\n\n\n\n\nLine types\n\nggplot(data=data.frame(x=c(1:6))) + geom_hline(size=2, aes(yintercept=x, linetype=x)) +\n    scale_linetype_identity() +\n    xlab(NULL) + ylab(NULL) + xlim(c(0,100)) +\n    theme_void()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nColors\n\ndf = data.frame(x=rep(1:26, 26), y=rep(1:26, each=26))\ndf$c = NA\ndf[1:length(colors()),'c'] = colors()\ndf$n = NA\ndf[1:length(colors()),'n'] = 1:length(colors())\ndf$r = df$g = df$b = NA\ndf[1:length(colors()),c('r','g','b')] = t(col2rgb(colors()))\ndf$text = ifelse(apply(df[,c('r','g','b')], 1, sum) &gt; (255*3/2), 'black', 'white')\ndf$hex = lapply(df$c, getColorHexAndDecimal)\ndf$hex2 = paste(format(df$n, width = 3), \n                format(df$c, width = (max(nchar(df$c), na.rm = TRUE) + 1)), \n                format(df$hex, width = (max(nchar(df$hex), na.rm = TRUE) + 1)))\n\nggplot(df, aes(x=x, y=y, fill=c, label=n)) + geom_tile() + geom_text(aes(colour=text), size=3) +\n    scale_fill_identity() +\n    scale_colour_identity() +\n    xlab(NULL) + ylab(NULL) +\n    theme_void()\n\n\n\n\n\n\n\n\nThis last one is only the first 100 elements in colors(). Use the script file to generate the remaining plots if you like.\n\nggplot(df[1:100,], aes(x=1, y=n, fill=c, label=hex2, colour=text)) +\n    geom_tile() + geom_text(family = 'mono') +\n    scale_fill_identity() +\n    scale_colour_identity() +\n    xlab(NULL) + ylab(NULL) +\n    theme_void()"
  },
  {
    "objectID": "posts/2025-04-11-ShinyConf2025.html",
    "href": "posts/2025-04-11-ShinyConf2025.html",
    "title": "ShinyQDA: R Package and Shiny Application for the Analysis of Qualitative Data",
    "section": "",
    "text": "The ShinyQDA R package is designed to assist researchers with the analysis of qualitative data. As the name suggests, the premise is that much of the interaction with the package will be done through a Shiny application. However, all the functionality in the Shiny application is accessible through R commands. The core functionality of ShinyQDA allows researchers to highlight passages and code passages. The application also allows for scoring text documents using rubrics. Tools for conducting validity analysis using co-occurrence plots and code frequency is provided. In addition to traditional qualitative data analysis, ShinyQDA utilizes natural language processing to conduct sentiment analysis, topic modeling, and text encoding (i.e. tokenization). ShinyQDA can be used locally by a single researcher or be deployed to a Shiny server so that multiple researchers can access the application to code and/or score documents.\nTo register for the (free) conference, go to https://www.shinyconf.com\nFor more information about the project, visit: https://github.com/jbryer/ShinyQDA"
  },
  {
    "objectID": "projects/ipeds.html",
    "href": "projects/ipeds.html",
    "title": "ipeds",
    "section": "",
    "text": "Github: https://github.com/jbryer/ipeds\nThe following R script will download all available IPEDS data files.\n\nlibrary(xml2)\nlibrary(rvest)\n\nout_dir &lt;- '~/Downloads/IPEDS/' # Location to put the downloaded files\n\nyears &lt;- 2023:1980\n\nipeds_base &lt;- 'https://nces.ed.gov/ipeds/datacenter/'\nipeds_url &lt;- 'https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx?year='\n\nerror_links &lt;- c() # Save any links that could not be downloaded.\nfor(year in years) {\n    cat(paste0('Downloading year ', year, '...\\n'))\n    dir.create(paste0(out_dir, '/', year), showWarnings = FALSE, recursive = TRUE)\n\n    page &lt;- read_html(paste0(ipeds_url, year))\n    tables &lt;- page |&gt; html_nodes(\"table\") |&gt; html_table(convert = FALSE)\n    # Guessing the one with the most rows is the one we want to keep as the index\n    tab_index &lt;- lapply(tables, nrow) |&gt; unlist() |&gt; which.max()\n    write.csv(tables[[tab_index]],\n              file = paste0(out_dir, year, '/_TOC_', year, '.csv'),\n              row.names = FALSE)\n\n    links &lt;- html_attr(html_nodes(page, \"a\"), \"href\")\n    zip_files &lt;- links[grep(\"*.zip\", links)]\n    for(i in zip_files) {\n        dest &lt;- paste0(out_dir, '/', year, '/', basename(i))\n        if(!file.exists(dest)) {\n            cat(paste0('Downloading ', basename(i), '...\\n'))\n            tryCatch({\n                download.file(url = paste0(ipeds_base, i), dest = dest)\n            }, error = function(e) {\n                error_links &lt;&lt;- c(error_links, paste0(ipeds_base, i))\n                print(e)\n            })\n        }\n    }\n}\nerror_links # Print any links that could not download"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "DATA606 - Statistics and Probability - City University of New York, School of Professional Studies. Website for the most recent course is available at http://data606.net. See also the DATA606 R package to support this course.\nDAV5300 - Computational Math and Statistics - Yeshiva University. Website for the most recent course is available at http://dav5300.net.\n\nThe VisualStats website contains articles and Shiny applications to support the teaching of statistics."
  },
  {
    "objectID": "teaching.html#current-courses",
    "href": "teaching.html#current-courses",
    "title": "Teaching",
    "section": "",
    "text": "DATA606 - Statistics and Probability - City University of New York, School of Professional Studies. Website for the most recent course is available at http://data606.net. See also the DATA606 R package to support this course.\nDAV5300 - Computational Math and Statistics - Yeshiva University. Website for the most recent course is available at http://dav5300.net.\n\nThe VisualStats website contains articles and Shiny applications to support the teaching of statistics."
  },
  {
    "objectID": "teaching.html#past-courses",
    "href": "teaching.html#past-courses",
    "title": "Teaching",
    "section": "Past courses",
    "text": "Past courses\n\nFall 2022 - DATA 661 Benchmarking Predictive Models, CUNY School of Professional Studies. Developed an R package with students to benchmark predictive models. Package website: https://github.com/jbryer/mldash\nSpring 2022 - DATA 661 Independent Study, CUNY School of Professional Studies. Worked with students on mapping data in R, sentiment analysis, topic modeling, and creating Shiny modules for data import and analysis.\nSpring 2020, 2021 - EPSY 630 Statistics II, University at Albany. Emphasis is on statistical inference. Topics include one- and two-way analysis of variance, multiple comparison tests, correlation and regression techniques, chi square, and nonparametric statistics. Course website: https://epsy630.bryer.org\nFall 2019 - EPSY 887 Intro to R for Academic Researchers, University at Albany. This course will explore the skills and tools necessary for conducting data preparation and analysis with R. The first third of the course will focus on learning R. The middle third will explore some of the more common statistical procedures in R including: classification and regression trees; logistic regression; propensity score analysis; missing data imputation; and other topics as time permits. The final third of the class will be left for topics of special interest to\nFall 2013, 2014, 2015 - EPSY 530 Statistics I, University at Albany. Descriptive statistics including measures of central tendency and variability, correlation and regression. Introduction to statistical inference, including sampling distributions, significance tests, confidence intervals, and power of tests of significance. Course website: https://github.com/jbryer/EPSY530Summer2015\nFall 2014 - EPSY 887 Data Science Institute, University at Albany. Data Science is the intersection of statistics, computer science, and research. This seminar will introduce the key concepts of data science with an emphasis on data science in education. We will cover the important statistical and programming concepts necessary for conducting reproducible research on large datasets. The open source program R will be used throughout the course. No programming experience is required but at least two semesters of graduate statistics is highly recommended. Course website: https://github.com/jbryer/EPSY887DataScience\nSpring 2013 - EPSY 887 Institute in Education: Computational Statistics, University at Albany. This seminar will provide an introduction to statistical programming for data analysis with an emphasis on the analysis of large datasets. With the increased availability of large national and international datasets (e.g. PISA, TIMMS, NAEP, ECLS) there is a great opportunity and potential for researchers to address important questions. However, the analysis of large datasets requires special analytical procedures not found in commercial statistics software. Utilizing the open source statistical software R, students will be introduced to the tools and procedures for analyzing large datasets with an emphasis on conducting transparent and reproducible research. Course website: https://github.com/jbryer/CompStats\nSpring 2009, Fall 2008 - EPSY 420 Child & Adolescent Development, University at Albany. This course covers theory and research in social, emotional, physical, and intellectual development and its application to instruction with an emphasis on late childhood through middle adolescence."
  },
  {
    "objectID": "teaching.html#workshops",
    "href": "teaching.html#workshops",
    "title": "Teaching",
    "section": "Workshops",
    "text": "Workshops\n\nApril/May 2014 - Applied Propensity Score Analysis with R, Workshop given at the University at Albany. This two day workshop provided an introduction to propensity score methods using R as well as more advanced topics including multilevel PSA, non-binary treatmentmatching, and bootstrap- ping. Workshop website: http://psa.bryer.org\nJuly 2013 - Introduction to Propensity Score Methods with R, useR! 2013 Pre-Conference Workshop. This workshop will provide participants with a theoretical overview of propensity score methods as well as illustrations and discussion of PSA applications using R.\n2011, 2012, 2013, 2015, 2016 - Introduction to R and LaTeX for Institutional Research, Workshop given at the Northeast Association for Institutional Research. This workshop provides an overview as well as hands-on exercises for using R and LaTeX to perform data analysis and report generation. Participants learn to perform basic statistical analyses in R and to generate reports with LaTeX in spreadsheet, presentation, and document formats."
  },
  {
    "objectID": "projects/timeline.html",
    "href": "projects/timeline.html",
    "title": "timeline",
    "section": "",
    "text": "Github: https://github.com/jbryer/timeline"
  },
  {
    "objectID": "projects/ruca.html",
    "href": "projects/ruca.html",
    "title": "ruca",
    "section": "",
    "text": "Github: https://github.com/jbryer/ruca"
  },
  {
    "objectID": "projects/psa.html",
    "href": "projects/psa.html",
    "title": "PSA",
    "section": "",
    "text": "Github: github.com/jbryer/psa\nWebsite: psa.bryer.org"
  },
  {
    "objectID": "projects/naep.html",
    "href": "projects/naep.html",
    "title": "naep",
    "section": "",
    "text": "Github: https://github.com/jbryer/naep"
  },
  {
    "objectID": "projects/mldash.html",
    "href": "projects/mldash.html",
    "title": "mldash",
    "section": "",
    "text": "Github: https://github.com/jbryer/mldash"
  },
  {
    "objectID": "projects/login.html",
    "href": "projects/login.html",
    "title": "login",
    "section": "",
    "text": "Github: https://github.com/jbryer/login"
  },
  {
    "objectID": "projects/daacs.html",
    "href": "projects/daacs.html",
    "title": "DAACS",
    "section": "",
    "text": "The Diagnostic Assessment and Achievement of College Skills (DAACS) is a suite of technological and social supports to optimize student learning. DAACS provides personalized feedback about students’ strengths and weaknesses in terms of key academic and self-regulated learning skills, linking them to the resources to help them be successful students.\nDAACS Websites:\n\ndaacs.net - General information about the project.\nmy.daacs.net - Public version of DAACS. Create an account, take the assessments, and view the feedback.\nsrl.daacs.net - Self-Regulated Learning open education resource.\ndocs.daacs.net - This site is fir administrators, instructors, or anyone who is intereted in using DAACS with their students. Site contains training materials and technical information.\ngithub.com/daacs - Github organization where source code is hosted."
  },
  {
    "objectID": "projects/brickset.html",
    "href": "projects/brickset.html",
    "title": "brickset",
    "section": "",
    "text": "Github: https://github.com/jbryer/brickset"
  },
  {
    "objectID": "projects/TriMatch.html",
    "href": "projects/TriMatch.html",
    "title": "TriMatch",
    "section": "",
    "text": "Github: https://github.com/jbryer/TriMatch\nWebsite: https://jbryer.github.io/TriMatch\nThe use of propensity score methods (Rosenbaum and Rubin, 1983) have become popular for estimating causal inferences in observational studies in medical research (Austin, 2008) and in the social sciences (Thoemmes and Kim, 2011). In most cases however, the use of propensity score methods have been confined to a single treatment. Several researchers have suggested using propensity score methods with multiple control groups, or to simply perform two separate analyses, one between treatment one and the control and another between treatment two and control. This talk introduces the TriMatch package for R that provides a method for determining matched triplets. Examples from educational and medical contexts will be discussed.\nConsider two treatments, T r1 and T r2, and a control, C. We estimate propensity scores with three separate logistic regression models where model one predicts T r1 with C, model two predicts T r2 with C, and model three predicts T r1 with T r2. The triangle plot in Figure 1 represents the fitted values (i.e. propensity scores) from the three models on each edge. Since each unit has a propensity score in two models, their scores are connected. The TriMatch algorithm will find matched triplets where the sum of the distances within each model is minimized. In Figure 1, the black lines illustrate one matched triplet.\nPropensity score analysis of two groups typically use dependent sample t-tests. The analogue for matched triplets include Figure 1: Triangle Plot repeated measures ANOVA and the Freidman Rank Sum Test. The TriMatch package provides utility functions for conducting and visualizing these statistical tests. Moreover, a set of functions extending PSAgraphics (Helmreich and Pruzek, 2009) for matched triplets to check covariate balance are provided."
  },
  {
    "objectID": "projects/PSAgraphics.html",
    "href": "projects/PSAgraphics.html",
    "title": "PSAgraphics",
    "section": "",
    "text": "Github: https://github.com/jbryer/PSAgraphics"
  },
  {
    "objectID": "projects/IRRsim.html",
    "href": "projects/IRRsim.html",
    "title": "IRRsim",
    "section": "",
    "text": "Github: https://github.com/jbryer/IRRsim\nInter-rater reliability (IRR) is a critical component of establishing the reliability of measures when more than one rater is necessary. There are numerous IRR statistics available to researchers including percent rater agreement, Cohen’s Kappa, and several types of intraclass correlations (ICC). Several methodologists suggest using ICC over percent rater agreement (Hallgren, 2012; Koo & Li, 2016; McGraw & Wong, 1996; Shrout & Fleiss, 1979). However, the literature provides little guidance on the interpretation of ICC results. This article explores the relationship between ICC and percent rater agreement using simulations. Results suggest that ICC and percent rater agreement are highly correlated (R² &gt; 0.9) for most designs used in education. When raters are involved in scoring procedures, inter-rater reliability (IRR) measures are used to establish the reliability of instruments. Commonly used IRR measures include Percent Agreement, Intraclass Correlation Coefficient (ICC) and Cohen’s Kappa (see Table 1). Several researchers recommend using ICC and Cohen’s Kappa over Percent Agreement (Hallgren, 2012; Koo & Li, 2016; McGraw & Wong, 1996; Shrout & Fleiss, 1979). Although it may appear that IRR measures are interchangeable, they reflect different information (AERA, NCME, & APA, 2014). For instance, Cohen’s Kappa and Percent Agreement reflect absolute agreement, while ICC (3, 1) reflect consistency between the raters (see Table XX). Inter-rater reliability is defined differently in terms of either consistency, agreement, or a combination of both. Yet, there are misconceptions and inconsistencies when it comes to proper application, interpretation and reporting of these measures (Kottner et al., 2011; Trevethan, 2017). In addition, researchers tend to recommend different thresholds for poor, moderate and good level of reliability (see Table 2). These inconsistencies, and the paucity of detailed reports of test methods, research designs and results perpetuate the misconceptions in the application and interpretation of IRR measures."
  },
  {
    "objectID": "projects/DataCache.html",
    "href": "projects/DataCache.html",
    "title": "DataCache",
    "section": "",
    "text": "Github: https://github.com/jbryer/DataCache"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Jason Bryer",
    "section": "",
    "text": "This is a list of some of the projects I have worked on over the years. Most of these link to either the project website or Github. The vast majority of my projects are hosted on Github.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDAACS\n\n\n\nResearch\n\n\n\nDiagnostic Assessment and Achievement of College Skills\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDTedit\n\n\n\nr-package\n\n\n\nEditable DataTables for shiny apps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataCache\n\n\n\nr-package\n\n\n\nAn R package to maintain data caches.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFutureMapping\n\n\n\nr-package\n\n\n\nAmplify Youth Ask Youth Survey Data Dashboard and Mapping Project.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIRRsim\n\n\n\nr-package\n\n\n\nThis package provides functions and a shiny application to simulate inter-rater reliability statistics based on various scoring and response models. The initial motivation…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSA\n\n\n\nr-package\n\n\nbook\n\n\n\nPropensity Score Analysis (PSA) is a statistical approach for estimating causal effects from observational studies. This project includes materials from workshops taught, an…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSAboot\n\n\n\nr-package\n\n\npsa\n\n\n\nR Package and Shiny Application for the Analysis of Qualitative Data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPSAgraphics\n\n\n\nr-package\n\n\npsa\n\n\n\nAn R Package to Support Propensity Score Analysis. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShinyQDA\n\n\n\nr-package\n\n\n\nThe ShinyQDA package is designed to assist researchers with the analysis of qualitative data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTriMatch\n\n\n\nr-package\n\n\npsa\n\n\n\nPropensity score matching for non-binary treatments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualStats\n\n\n\nr-package\n\n\nbook\n\n\n\nVisual Statistics package and book.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrickset\n\n\n\nr-package\n\n\n\nAn R package to interface with the Brickset.com API for getting data about LEGO sets \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclav\n\n\n\nr-package\n\n\n\nCluster Analysis Validation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nipeds\n\n\n\nr-package\n\n\n\nAn R package to interface with the Integrated Postsecondary Education Data System.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlikert\n\n\n\nr-package\n\n\n\nAn R package designed to help analyzing and visualizing Likert type items. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogin\n\n\n\nr-package\n\n\nshiny\n\n\n\nThis package provides a framework for adding user authentication to Shiny applications. This is unique to other authentication frameworks such as ShinyManager and shinyauthr…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmedley\n\n\n\nr-package\n\n\n\nPredictive Modeling with Missing Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmldash\n\n\n\nr-package\n\n\n\nMashine learning dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmultilevelPSA\n\n\n\nr-package\n\n\npsa\n\n\n\nAn R package for estimating and visualizing multilevel propensity score models. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnaep\n\n\n\nr-package\n\n\n\nAn R package to interface with the National Assessment of Educational Progress (NAEP) restricted use databases. This includes access any analyzing data using the replicate…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npisa\n\n\n\nr-package\n\n\n\nA data-only R package for the 2009 Programme of International Student Assessment (PISA) conducted by Organisation for Economic Co-operation and Development (OECD).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nqualtrics\n\n\n\nr-package\n\n\n\nAn R package to interface with the Qualtrics.com survey system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nruca\n\n\n\nr-package\n\n\n\nAn R package to get Rural-Urban Commuting Area (RUCA) Codes from zip codes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsqlutils\n\n\n\nr-package\n\n\n\nThe sqlutils package provides a set of utility functions to help manage a library of structured query language (SQL) files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntimeline\n\n\n\nr-package\n\n\n\nAn R package to create timeline figures. \n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-02-22-Assessments_with_Shiny.html",
    "href": "posts/2016-02-22-Assessments_with_Shiny.html",
    "title": "Conducting Assessments and Surveys with Shiny",
    "section": "",
    "text": "This post describes a framework for using Shiny for conducting, grading, and providing feedback for assessments. This framework supports any multiple choice format including multiple choice tests or Likert type surveys. A demo is available at jbryer.shinyapps.io/ShinyAssessmentTest or can be run locally as a Github Gist:\nrunGist('a6fb5a3b1d5fd56cff64')\nKey features of this framework include:\n\nAssessments take over the entire user interface for a distraction free assessment.\nCreating an assessment requires:\n\nA vector of item stems.\nA data frame with item choices.\nA function that will process the results.\n\nButton or link to start the assessment.\n\n\nExample\nLet’s walk through the statistics assessment example. The first step is to define the multiple choice items, here defined in a CSV file.\n&gt; math.items &lt;- read.csv('items.csv', stringsAsFactors=FALSE)\n&gt; names(math.items)\n[1] \"Item\"   \"Stem\"   \"Answer\" \"A\"      \"B\"      \"C\"      \"D\"      \"E\"     \nWe will also define a function that will be called when the user completes the assessment. This function needs to have one parameter named results. This parameter is a character vector of the user responses. The values are either NA if there was no response, or the column name of the item.choices defined below (here A through E). In this example, the results will be stored in a reactiveValues object so that the UI will refresh with new results.\nassmt.results &lt;- reactiveValues(\n    math = logical(),\n    mass = integer(),\n    reading = logical()\n)\n\nsaveResults &lt;- function(results) {\n    assmt.results$math &lt;- results == math.items$Answer\n}\nNext, we create an assessment by calling the ShinyAssessment function.\ntest &lt;- ShinyAssessment(input, output, session,\n        name = 'Statistics',\n        item.stems = math.items$Stem,\n        item.choices = math.items[,c(4:8)],\n        callback = saveResults,\n        start.label = 'Start the Statistics Assessment',\n        itemsPerPage = 1,\n        inline = FALSE)\nThe first three parameters, input, output, and session are simply passed from shinyServer. The other parameters you can set are:\n\nname The name of the assessment. This should be a name that follows R’s naming rules (i.e. does not start with a number, no spaces, etc).\ncallback The function called when the user submits the assessment. Used for saving the results.\nitem.stems A character vector or list with the item stems. If a list, any valid Shiny UI output is allowed (e.g. p, div, fluidRow, etc.). For character vectors HTML is allowed.\nitem.choices A data frame with the item answers. For items that have fewer choices than the total number of columns, place in that column’s value. The results will be passed to the function as named list where the value is the name of the column selected.\nstart.label The label used for the link and button created to start the assessment.\nitemsPerPage The number of items to display per page.\ninline If TRUE, render the choices inline (i.e. horizontally).\nwidth The width of the radio button input.\ncancelButton Should a cancel button be displayed on the assessment.\n\nUsers start an assessment with a link or button using uiOutput(test$link.name) or uiOutput(test$button.name), respectively.\nIn order for the assessment to take over the entire user interface, the UI must be built on the server side in the server.R file. In this case, the UI resides in the output$ui object:\noutput$ui &lt;- renderUI({\n    if(SHOW_ASSESSMENT$show) { # The assessment will take over the entire page.\n        fluidPage(width = 12, uiOutput(SHOW_ASSESSMENT$assessment))\n    } else { \n        # This is the normal Shiny UI code here.\n    }\n})\nAs a result, the ui.r script has only one line of code.\nshinyUI(fluidPage( uiOutput('ui') ))\nThis is one of two limitations of this approach. The other limitation is the creation of the SHOW_ASSESSMENT object. In order for the UI to know to show the assessment, a global variable must be set (i.e. SHOW_ASSESSMENT$show). To accomplish this, the ShinyAssessment function creates and sets the value of an object in the calling environment. This is generally considered bad practice (Note: if you know of another approach to avoid this behavior, please let me know in the comments below). Multiple assessments are supported as subsequent calls to ShinyAssessment first look to see if the SHOW_ASSESSMENT object has been created.\n\n\nConclusion\nIt is up to the developer to define the callback function is to score and save results. There are a lot of R packages that support databases including RODB, RMySQL, ROracle, RJDBC, rsqlite, and RPostgreSQL). Be sure to check out Dean Attali’s article about persisting data storage in Shiny apps, especially if you plan to deploy to shinyapps.io.\nI have also modified Huidong Tian’s R script for adding user authentication to the open source version of Shiny to allow for users to create accounts. With authenticated user accounts users can retrieve their assessment results across different sessions. The source code is here: gist.github.com/jbryer/e17c5587a43188258ee5\nThis function represents the first version of an assessment framework for Shiny. Since this is in place that might be useful for other Shiny users, especially those using R and teaching, I wanted to share to get feedback and suggestions on improvement. For instance, currently this function only supports a fixed number of items presented in predefined order. In the future, this function will be modified to utilize IRT models and allow for computer adaptive testing."
  },
  {
    "objectID": "posts/2014-07-29-Data_Caching.html",
    "href": "posts/2014-07-29-Data_Caching.html",
    "title": "Data Caching",
    "section": "",
    "text": "Data caching is not new. It is often necessary to save intermediate data files when the process of loading and/or manipulating data takes a considerable amount of time. This problem is further complicated when working with dynamic data that changes regularly. In these situations it often sufficient to use data that is current with in some time frame (e.g. hourly, daily, weekly, monthly). One solution is to use a time-based job scheduler such as cron. However, that requires access and knowledge of Unix systems. The alternative, is to check for the “freshness” of a cached dataset each time it is requested. If is “stale,” then the data cached is refreshed with more up-to-date data. The DataCache package implements this approach in R. Moreover, on Unix systems (including Mac OS X), the refreshing will be done in the background. That is, when requesting data from the cache, if it is stale, the function will return the latest available data while the cache is updated in the background. This is particularly useful when using R in a web environment (e.g. Shiny Apps) where it is not ideal to have the user wait for data be loaded to begin interacting with the app.\nThe latest version of the DataCache package can be downloaded from Github using the devtools package.\ndevtools::install_github('jbryer/DataCache')\nFor this example, we wish to periodically load weather data using the weatherData package. The getDetailedWeather function provides hourly temperature updates. To start, we will load the DataCache and weatherData packages.\nlibrary('DataCache')\nlibrary('weatherData')\nThe only required parameter for the data.cache function is the FUN parameter which defines the data to be loaded. This function should return a named list where each element of the list will be assigned to specified environment when loaded. That is, if the function returns list(foo='bar') then the object foo will be assigned in the working envirnoment (note that this can be modified using the envir parameter).\n#' Load data for a single day for the given airport.\n#' @param station_id three letter airport code.\n#' @return a list with a data frame names `weather.XXX` where `XXX` is the three\n#'         letter airport code.\nloadWeatherData &lt;- function(station_id='ALB') {\n    results &lt;- list(getDetailedWeather(station_id, Sys.Date()))\n    names(results) &lt;- paste0('weather.', station_id)\n    return(results)\n}\nTo get started, simply call data.cache with the data loading function. This function will block on the first execution (i.e. you will have to wait until the first dataset is loaded). On subsequent executions, the data.cache function will check to see if the most recent cached data is stale. If it is stale, it will start a new background process to load the data and return the most recent data. Once the background process completes, data.cache will start returning the updated data.\nNote for Windows users: Forking is not available on Windows systems using the parallel package. Therefore data cannot be loaded in the background. As a result, the data.cache function will wait for the refreshed data to load each time it becomes stale.\nThe data.cache returns invisibly (i.e. will not be printed if not assinged to a variable, see ?invisible for more details) the timestamp of the data returned.\n(cache.date1 &lt;- data.cache(loadWeatherData))\nNo cached data found. Loading intial data...\n[1] \"2014-07-29 18:03:53 EDT\"\nhead(weather.ALB)\n                 Time TemperatureF\n1 2014-07-29 00:51:00         62.1\n2 2014-07-29 01:51:00         61.0\n3 2014-07-29 02:51:00         61.0\n4 2014-07-29 03:51:00         61.0\n5 2014-07-29 04:51:00         60.1\n6 2014-07-29 05:51:00         59.0\nThe cache.info function provides a summary of all the cached data files. It will also provide columns (which can be set using the stale parameter) indicating whether that data file is stale according to various time periods.\ncache.info()\n                          file             created age_mins hourly_stale\n1 Cache2014-07-29 18:03:53.rda 2014-07-29 18:03:53   0.1842        FALSE\n  daily_stale weekly_stale monthly_stale yearly_stale\n1       FALSE        FALSE         FALSE        FALSE\nOld data caches can easily be loaded this way. For example, the following will load the first data cache created:\ncinfo &lt;- cache.info()\nload(cinfo[nrow(cinfo),]$file)\nThere are a number of frequencies available for defining when a dataset becomes stale. They are:\n\nhourly - Data will become stale each hour. This uses the hour function from the lubrdiate package. Therefore, data will become stale at the top of each hour.\ndaily - Data will become stale each day (i.e. at midnight).\nweekly - Data will become stale each week. The day of week will vary depending on what day of the week January 1st occurs.\nmontly - Data will become stale each month (i.e. on the 1st of the month).\nyearly - Data will become stale each year (i.e. on January 1st).\nnMinutes - Data will become stale if last loaded more than n minutes ago.\nnHours - Data will become stale if last loaded more than n hours ago.\nnDays - Data will become stale if last laoded more than n days ago.\n\nYou can define your own frequency. Simply define a function that takes one parameter, timestamp, and returns a logical where TRUE indicates the data loaded at timestamp is stale.\nWe can see that the weather.ALB data frame is now available in the working environent.\nls()\n[1] \"cache.date1\"     \"loadWeatherData\" \"package\"         \"weather.ALB\"\nWe will wait 60 seconds so that the cache becomes stale. We will also call data.cache twice in a row to show that they will each return the same cached data, but the second call will not spawn a new background process to refresh the data.\nSys.sleep(60)\n(cache.date2 &lt;- data.cache(loadWeatherData, frequency=nMinutes(1)))\nLoading more recent data, returning lastest available.\n[1] \"2014-07-29 18:03:53 EDT\"\n(cache.date3 &lt;- data.cache(loadWeatherData, frequency=nMinutes(1)))\nData is being loaded by another process. The process has been running for 0.121761083602905 seconds. If this is an error delete cache/Cache.lck\nLoading more recent data, returning lastest available.\n[1] \"2014-07-29 18:03:53 EDT\"\nIt is easy to have multiple data caches. Using the same loadWeatherData function we will create a spearate data cache for weather data from JFK.\ndata.cache(loadWeatherData, cache.name='NRT', station_id='NRT')\nNo cached data found. Loading intial data...\nhead(weather.NRT)\n                 Time TemperatureF\n1 2014-07-29 00:00:00         69.8\n2 2014-07-29 00:30:00         68.0\n3 2014-07-29 01:00:00         68.0\n4 2014-07-29 01:30:00         68.0\n5 2014-07-29 02:00:00         66.2\n6 2014-07-29 02:30:00         66.2\ncache.info(cache.name='NRT')\n                        file             created age_mins hourly_stale\n1 NRT2014-07-29 18:05:04.rda 2014-07-29 18:05:04  0.03819        FALSE\n  daily_stale weekly_stale monthly_stale yearly_stale\n1       FALSE        FALSE         FALSE        FALSE"
  },
  {
    "objectID": "posts/2014-07-03-useR_2014_PSAboot_Slides.html",
    "href": "posts/2014-07-03-useR_2014_PSAboot_Slides.html",
    "title": "useR 2014 Slides for PSAboot and version 1.1. on CRAN",
    "section": "",
    "text": "PSAboot is an R package to assist with bootstrapping propensity score methods. I gave a talk today at the useR! 2014 Conference. The slides can be downloaded from the PSAboot Github page or directly here. The package is described at jason.bryer.org/PSAboot and maintained on Github at github.com/jbryer/PSAboot/.\nAlso, version 1.1 of the package was just released to CRAN."
  },
  {
    "objectID": "posts/2014-04-22-Rgitbook_Package.html",
    "href": "posts/2014-04-22-Rgitbook_Package.html",
    "title": "Rgitbook Package for Using R Markdown with Gitbook",
    "section": "",
    "text": "Last week I published an R script to interface with Gitbook. I received some positive feedback and decided to include all the code in an R package. This also allowed me to make some nice additions including default support for MathJax. It is currently available on Github and can be installed using devtools:\ndevtools::install_github('jbryer/Rgitbook')\nI have only tested this on Mac OS X, so please provide suggestions or issues on other systems. And of course, I wrote the documentation using the Gitbook framework. That is available here: jason.bryer.org/Rgitbook"
  },
  {
    "objectID": "posts/2014-02-20-Loading_and_Installing_Packages.html",
    "href": "posts/2014-02-20-Loading_and_Installing_Packages.html",
    "title": "Function to Simplify Loading and Installing Packages",
    "section": "",
    "text": "One of the more tedious parts of working with R is maintaining my R library. To make my R scripts reproducible and sharable, I will install packages if they are not available. For example, the top of my R scripts tend to look something like this:\nif(!require(devtools) | !require(ggplot2) | !require(psych) | !require(lme4) | !require(benchmark)) {\n    install.packages(c('devtools','ggplot2','psych','lme4','benchmark'))\n}\nThis has worked fine for some time, but I felt there was a better approach. First, note that if any one package doesn’t load (usually because it is not installed), all the packages are installed. I could separate the if statement so there is one per package, but then I have even more lines in my R script. Instead, I have written a function that will load each package separately and install only those that are not present. And optionally will even update packages using the update parameter. For example, I can now replace the above with one call to package:\n&gt; package(c('devtools','ggplot2','psych','lme4','benchmark'))\nThe output is minimal by default (set quiet=FALSE to get all the messages printed by require and install.packages). Even though verbose=TRUE by default, the only messages it will print is to indicate that a newer version of a package is available or that the package is not available on the repositories. In place of output to the console, a data frame is returned with a summary of what packages have been loaded and/or installed along with the loaded and available versions. Here are the results from the command above:\nA newer version of lme4 is available (current=1.0.5; available=1.0.6)\n\n          loaded installed loaded.version available.version\ndevtools    TRUE     FALSE          1.4.1             1.4.1\nggplot2     TRUE     FALSE        0.9.3.1           0.9.3.1\npsych       TRUE     FALSE        1.4.2.3           1.4.2.3\nlme4        TRUE     FALSE          1.0.5             1.0.6\nbenchmark   TRUE      TRUE          0.3.5             0.3.5\nNote that if I had specified update=TRUE (it is FALSE by default) the lme4 package would have been automatically updated.\nIn summary, I have collapsed what usually takes several lines within my R scripts to just one line, or two if you need to source this function. However, I just source this function in my .Rprofile so that it is always available. The only potential downside is that this is not part of base R and requires anyone you share your R scripts with to also have this function available.\nThe sourse code is on Gist here: https://gist.github.com/jbryer/9112634 With devtools installed you can just source function using:\n&gt; source_gist(9112634)"
  },
  {
    "objectID": "posts/2013-11-12-Workshop_and_Talk_Slides_from_NEAIR.html",
    "href": "posts/2013-11-12-Workshop_and_Talk_Slides_from_NEAIR.html",
    "title": "Workshop and Talk Slides from NEAIR Conference",
    "section": "",
    "text": "I am about to head home from my fifth time attending the North East Association for Institutional Research (NEAIR), this year in Newport, RI, which was just fantastic. Really great people, interesting talks, and good food. I again taught an Introduction to R and LaTeX for Institutional Research pre-conference workshop and also gave a talk on Propensity Score Analysis for Institutional Research which was an brief version of a workshop I taught at the 2013 useR! Conference in Spain. Here are links to my slides and materials:\n\nIntroduction to R and LaTeX for Institutional Research: Slides, Github Repository Abstract: This workshop will provide an overview as well as hands-on exercises for using R and LaTeX to perform data analysis and report generation. Participants will learn to perform basic statistical analyses in R and to generate reports with LaTeX in spreadsheet, presentation, and document formats.\nIntroduction to Propensity Score Analysis for Institutional Researchers Slides, Github Repository Abstract: This workshop will provide an overview as well as hands-on exercises for using R and LaTeX to perform data analysis and report generation.\n\nYou find many more resources at the Github repositories including R scripts and demos.\nI would also like to thank NEAIR as I was recipient of their Ambassador Grant which help defray some of the cost for me to attend this year’s useR! Conference in Spain."
  },
  {
    "objectID": "posts/2013-04-18-Cut_Dates_Into_Quarters.html",
    "href": "posts/2013-04-18-Cut_Dates_Into_Quarters.html",
    "title": "Cut Dates into Quarters",
    "section": "",
    "text": "Frequently I need to recode a date column to quarters. For example, at Excelsior College we have continuous enrollment so we report new enrollments per quarter. To complicate things a bit, our fiscal year starts in July so that July, August, and September represent the first quarter, January, February, and March are actually the third quarter. But sometimes we do need need to report out based upon calendar years (i.e. where January is in the first quarter). I am sure this is pretty common practice in many disciplines. There are probably other ways to do this in R (please comment below about other methods), but could not find one that satisfies my needs.\nWe can begin by sourceing the function from Gist using the devtools package.\nrequire(devtools)\nsource_gist(5412193)\nCreate a vector of Dates.\n&gt; dates &lt;- as.Date(c('2013-04-03','2012-03-30','2011-10-31',\n                   '2011-04-14','2010-04-22','2004-10-04',\n                   '2000-02-29','1997-12-05','1997-04-23',\n                   '1997-04-01'))\nThe default is to use the typical academic fiscal year with the year staring July 1.\n&gt; getYearQuarter(dates)\n [1] FY2013-Q4 FY2012-Q3 FY2012-Q2 FY2011-Q4 FY2010-Q4 FY2005-Q2 FY2000-Q3 FY1998-Q2 FY1997-Q4\n[10] FY1997-Q4\n65 Levels: FY1997-Q4 &lt; FY1998-Q1 &lt; FY1998-Q2 &lt; FY1998-Q3 &lt; FY1998-Q4 &lt; FY1999-Q1 &lt; ... &lt; FY2013-Q4\nHowever, it easy to use get a quarters within a calendar year.\n&gt; getYearQuarter(dates, firstMonth=1)\n [1] FY2013-Q2 FY2012-Q1 FY2011-Q4 FY2011-Q2 FY2010-Q2 FY2004-Q4 FY2000-Q1 FY1997-Q4 FY1997-Q2\n[10] FY1997-Q2\n65 Levels: FY1997-Q2 &lt; FY1997-Q3 &lt; FY1997-Q4 &lt; FY1998-Q1 &lt; FY1998-Q2 &lt; FY1998-Q3 &lt; ... &lt; FY2013-Q2\nYou can also alter the format of the levels using the fy.prefix, quarter.prefix, and sep parameters.\n&gt; getYearQuarter(dates, 1, '', '', '')\n [1] 20132 20121 20114 20112 20102 20044 20001 19974 19972 19972\n65 Levels: 19972 &lt; 19973 &lt; 19974 &lt; 19981 &lt; 19982 &lt; 19983 &lt; 19984 &lt; 19991 &lt; 19992 &lt; ... &lt; 20132\nLastly, the function by default will create a level for each quarter between the minimum and maximum dates in the date vector passed in. You can override the range for defining the levels with the level.range parameter. If the specified range is smaller than the range of the passed in vector, the function will print a warning because values outside that range will be returned as NA.\n&gt; getYearQuarter(dates, level.range=as.Date(c('2010-01-01','2013-01-01')))\n [1] &lt;NA&gt;      FY2012-Q3 FY2012-Q2 FY2011-Q4 FY2010-Q4 &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;\n[10] &lt;NA&gt;\n13 Levels: FY2010-Q3 &lt; FY2010-Q4 &lt; FY2011-Q1 &lt; FY2011-Q2 &lt; FY2011-Q3 &lt; FY2011-Q4 &lt; ... &lt; FY2013-Q3\nWarning message:\nIn getYearQuarter(dates, level.range = as.Date(c(\"2010-01-01\", \"2013-01-01\"))) :\n  The range of x is greater than level.range. Values outside level.range will be returned as NA.\nHere is a link to the Gist or copy-and-paste from below.\n#' Returns the year (fiscal or calendar) and quarter in which the date appears.\n#'\n#' This function will cut the given date vector into quarters (i.e. three month\n#' increments) and return an ordered factor with levels defined to be the quarters\n#' between the minimum and maximum dates in the given vector. The levels, by\n#' default, will be formated as \\code{FY2013-Q1}, however the \\code{FY} and \\code{Q}\n#' can be changed using the \\code{fy.prefix} and \\code{quarter.prefix} parameters,\n#' respectively.\n#'\n#' @param x vector of type \\code{\\link{Date}}.\n#' @param firstMonth the month corresponding to the first month of the fiscal year.\n#'        Setting \\code{firstMonth=1} is equivalent calenadar years.\n#' @param fy.prefix the character string to paste before the year.\n#' @param quarter.prefix the character string to paste before the quarter.\n#' @param sep the separater between the year and quarter.\n#' @param level.range the range to use for defining the levels in the returned\n#'        factor.\n#' @export\n#' @examples\n#'  dates &lt;- as.Date(c('2013-04-03','2012-03-30','2011-10-31',\n#'                     '2011-04-14','2010-04-22','2004-10-04',\n#'                     '2000-02-29','1997-12-05','1997-04-23',\n#'                     '1997-04-01'))\n#'  getYearQuarter(dates)\n#'  getYearQuarter(dates, firstMonth=1)\n#'  getYearQuarter(dates, 1, '', '', '')\n#'  \\dontrun{\n#'  getYearQuarter(dates, level.range=as.Date(c('2010-01-01','2013-01-01')))\n#'  }\ngetYearQuarter &lt;- function(x,\n                       firstMonth=7,\n                       fy.prefix='FY',\n                       quarter.prefix='Q',\n                       sep='-',\n                       level.range=c(min(x), max(x)) ) {\n    if(level.range[1] &gt; min(x) | level.range[2] &lt; max(x)) {\n        warning(paste0('The range of x is greater than level.range. Values ',\n                       'outside level.range will be returned as NA.'))\n    }\n    quarterString &lt;- function(d) {\n        year &lt;- as.integer(format(d, format='%Y'))\n        month &lt;- as.integer(format(d, format='%m'))\n        y &lt;- ifelse(firstMonth &gt; 1 & month &gt;= firstMonth, year+1, year)\n        q &lt;- cut( (month - firstMonth) %% 12, breaks=c(-Inf,2,5,8,Inf),\n              labels=paste0(quarter.prefix, 1:4))\n        return(paste0(fy.prefix, y, sep, q))\n    }\n    vals &lt;- quarterString(x)\n    levels &lt;- unique(quarterString(seq(\n        as.Date(format(level.range[1], '%Y-%m-01')),\n        as.Date(format(level.range[2], '%Y-%m-28')), by='month')))\n    return(factor(vals, levels=levels, ordered=TRUE))\n}"
  },
  {
    "objectID": "posts/2013-01-30-Converting_a_list_to_a_data_frame.html",
    "href": "posts/2013-01-30-Converting_a_list_to_a_data_frame.html",
    "title": "Converting a list to a data frame",
    "section": "",
    "text": "There are many situations in R where you have a list of vectors that you need to convert to a data.frame. This question has been addressed over at StackOverflow and it turns out there are many different approaches to completing this task. Since I encounter this situation relatively frequently, I wanted my own S3 method for as.data.frame that takes a list as its parameter. I should note that it only works with atomic vectors (i.e. logical, integer, numeric, complex, character and raw). If any one of the elements in the list are of some other class type, the function will call NextMethod. However, on my R instance at least, this will end up calling as.data.frame.default which will in turn throw an error.\nTo use the function you can source the function directly from Gist using the source_gist function in the devtools package.\nrequire(devtools)\nsource_gist(4676064)\nOr you can download the code at https://gist.github.com/4676064\n\nExample One\nIn this first example we have a list with two vectors, each with the same length and the same names.\n&gt; test1 &lt;- list( c(a='a',b='b',c='c'), c(a='d',b='e',c='f'))\n&gt; as.data.frame(test1)\n  a b c\n1 a b c\n2 d e f\n\n\nExample Two\nIn this example we have a list of two vectors, same length, but only one has names. The function in this case will use the names from the first vector with names for the column names of the data frame.\n&gt; test2 &lt;- list( c('a','b','c'), c(a='d',b='e',c='f'))\n&gt; as.data.frame(test2)\n  a b c\n1 a b c\n2 d e f\n\n\nExample Three\nThis example has two named vectors, but only have one overlapping named element.\n&gt; test3 &lt;- list('Row1'=c(a='a',b='b',c='c'), 'Row2'=c(a='d',var2='e',var3='f'))\n&gt; as.data.frame(test3)\n     a    b    c var2 var3\nRow1 a    b    c &lt;NA&gt; &lt;NA&gt;\nRow2 d &lt;NA&gt; &lt;NA&gt;    e    f\n\n\nExample Four\nThis is an example of what to avoid, three vectors of differing lengths and not named. The number of columns in the resulting data frame will be equal to the longest vector. For vectors less than that, NAs will be filled in on the right most columns. This method will also print a warning.\n&gt; test4 &lt;- list('Row1'=letters[1:5], 'Row2'=letters[1:7], 'Row3'=letters[8:14])\n&gt; as.data.frame(test4)\n     Col1 Col2 Col3 Col4 Col5 Col6 Col7\nRow1    a    b    c    d    e &lt;NA&gt; &lt;NA&gt;\nRow2    a    b    c    d    e    f    g\nRow3    h    i    j    k    l    m    n\nWarning message:\nIn as.data.frame.list(test4) :\n  The length of vectors are not the same and do not are not named, the results may not be correct.\n\n\nExample Five\nAnother example of equal length vectors.\n&gt; test5 &lt;- list(letters[1:10], letters[11:20])\n&gt; as.data.frame(test5)\n  X1 X2 X3 X4 X5 X6 X7 X8 X9 X10\n1  a  b  c  d  e  f  g  h  i   j\n2  k  l  m  n  o  p  q  r  s   t\n\n\nExample Six\nThis example shows the warning (and likely error too) that occurs when all of the elements of the list are not atomic vectors.\n&gt; test6 &lt;- list(list(letters), letters)\n&gt; as.data.frame(test6)\nError in as.data.frame.default(test6, row.names = NULL, optional = FALSE) :\n  cannot coerce class '\"list\"' into a data.frame\nIn addition: Warning message:\nIn as.data.frame.list(test6) : All elements of the list must be a vector."
  },
  {
    "objectID": "posts/2013-01-15-Version_1_sqlutils.html",
    "href": "posts/2013-01-15-Version_1_sqlutils.html",
    "title": "Version 1.0 of sqlutils available on CRAN",
    "section": "",
    "text": "Version 1.0 of sqlutils has been released to CRAN. The sqlutils package is designed to manage a library of SQL files. This package grew out of the needs of an Office of Institutional Research where the vast majority of analysis is conducted on data from our Student Information System (SIS) which is stored in an Oracle database. A lot of our analyses and reports are derived from the same types of datasets but from easily extracted parameters (e.g. date range, program name, status, etc.). We used to store SQL commands in our R scripts but that can become quite cumbersome and in many ways, reduced the ease of reusability which is a major reason for using R in the first place, hence the birth of sqlutils. For our purposes we currently have over 40 SQL files that have been well vetted and documented. To share the library we simply add the following to our .Rprofile script:\nrequire(sqlutils)\nsqlPaths('/Path/to/shared/directory')\nA full introduction to the squtils package is available here as well as on the Github project page. A key advantage to using sqlutils is that you can store your queries in plain text files (with a .sql file extension) and document them using roxygen2 style comments. Moreover, R function parameters are used to set parameters within the SQL command. Parameters are defined in SQL files using colon, parameter name, colon (i.e. :paramName:) format. Using this framework, it is easy to create a data dictionary of the library of SQL files.\nLastly, I wrote about an interactive SQL mode in R a few days ago. The isql function is included in the sqlutils package."
  },
  {
    "objectID": "posts/2013-01-10-Function_for_Reading_Codebooks_in_R.html",
    "href": "posts/2013-01-10-Function_for_Reading_Codebooks_in_R.html",
    "title": "Reading Codebook Files in R",
    "section": "",
    "text": "One issue I continuously encounter when starting to work with a new dataset is that of the codebook. In general, I prefer to load a codebook into R like any other data source, specifically as a data frame. And ideally, one data frame to provides the variable names with descriptions and any other meta data available, and a separate list of named vectors that can be used to recode factors. Although there is no standard format for codebooks, most follow a similar format. This post outlines the parse.codebook function that will read codebooks that have the following features:\n\nEach line in the file provides information about a variable (which I refer to as a variable row), or the mapping of factor (which I refer to as a level row).\nVariable rows start on the left edge (that is, there is a non-whitespace character at position 1 of the row).\nLevel rows do not start on the left edge (that is, there is a whitespace character at position 1 of the row, for example a tab or space).\nRows are either fixed (see ?read.fwf for more information as to specifics) or character delimited (e.g. comma, colon, etc.).\n\nAlthough all codebooks may not strictly adhere to these rules, it is often trivial, even if not a bit tedious, to reformat the file to adhere to these rules. Also, blank lines are permissible and will simply be ignored.\nIf the codebook file adheres to these rules, the parse.codebook function will parse the file and return an object of type codebook that inherits from data.frame, therefore all the data frame functions are valid (e.g. head, nrow, names, etc.). This data frame contains all the information about the variables vis-a-vis the variable rows. Information about factor levels are stored in a list as an attribute of the returned object which can be retrieved using attr(mycodebook, 'levels'). Example from the Common Core of Data and the American Community Survey are provided below.\n\nInstallation\nThe source.codebook function is currently provided on Gist. You can either download the R script file or source it directly from Gist using the devtools package.\nrequire(devtools)\nsource_gist(4497585)\n\nParameters\nThe parse.codebook has a number of parameters to indicate the format of variable and level rows. The function will handle both character delimited rows and fixed with rows. Therefore, either var.sep or var.widths must be specified as well as level.sep or level.widths. The available parameters are:\n\nfile codebook file name.\nvar.names the name of the columns for variable rows.\nlevel.names the name of the columns for level rows.\nvar.sep the separator for variable rows.\nlevel.sep the separator for level rows.\nlevel.indent character vector providing character(s) at the beginning of the line that indicate the line represents a factor level. Each element should have 1 character as only the first character of the line is compared.\nvar.name the name in var.names that represents the variable name. This should be a valid R variable name as this will be the column name in the corresponding data file, as well as the name used in the list of levels stored as an attribute to the returned object.\n\n\n\n\nExample One: Common Core of Data\nThe Common Core of Data (CCD) is a dataset provided by the National Center for Education Statistics that provides information about K-12 schools in the United States. The codebook provided is in plain text and required two modifications: One, general file information at the top of the file was deleted, and two, any descriptions that spanned lines need to be modified so the are on only one line. Here are the first 15 lines of the modified file, the full file can be downloaded at here\nSURVYEAR      1      AN     Year corresponding to survey record.\n\nNCESSCH       2      AN     Unique NCES public school ID (7-digit NCES agency ID (LEAID) + 5-digit NCES school ID (SCHNO).\n\nFIPST         3      AN     American National Standards Institute (ANSI) state code..\n\n                             01  =  Alabama\n                             02  =  Alaska\n                             04  =  Arizona\n                             05  =  Arkansas\n                             06  =  California\n                             08  =  Colorado\n                             09  =  Connecticut\n                             10  =  Delaware\n                             11  =  District of Columbia\nThis codebook uses fixed withs for variable rows, and separators (using the equal sign) for level rows (although it also possible to use fixed with for level rows as well). First, we will parse the file:\nccd.codebook &lt;- parse.codebook('ccdCodebook.txt',\n                var.names=c('variable','order','type','description'),\n                level.names=c('level','label'),\n                level.sep='=',\n                var.widths=c(13, 7, 7, Inf) )\nHere are the first six rows of the returned data frame.\n&gt; head(ccd.codebook)\n  linenum variable order type                                                                                    description isfactor\n1       1 SURVYEAR     1   AN                                                           Year corresponding to survey record.    FALSE\n2       3  NCESSCH     2   AN Unique NCES public school ID (7-digit NCES agency ID (LEAID) + 5-digit NCES school ID (SCHNO).    FALSE\n3       5    FIPST     3   AN                                      American National Standards Institute (ANSI) state code..     TRUE\n4      67    LEAID     4   AN                                                          NCES local education agency (LEA) ID.    FALSE\n5      69    SCHNO     5   AN                                                                                NCES school ID.    FALSE\n6      71     STID     6   AN                                                       State?s own ID for the education agency.    FALSE\nIn addition to the columns corresponding to var.names, the function also returns a linenum and isfactor column. The former is an integer corresponding to the line number in the original file from which this row was parsed. This is useful for tracking down issues in the parsing or text formatting. The isfactor is a logical column indicating whether there are factor levels specified for that variable. Factor levels can be retrieved as follows:\n&gt; ccd.var.levels &lt;- attr(ccd.codebook, 'levels')\n&gt; names(ccd.var.levels)\n[1] \"FIPST\"  \"TYPE\"   \"STATUS\" \"TITLEI\" \"STITLI\" \"MAGNET\" \"CHARTR\" \"SHARED\"\n&gt; ccd.var.levels[['TYPE']]\n  linenum level                    label\n1     103     1           Regular school\n2     105     2 Special education school\n3     107     3        Vocational school\n4     109     4 Other/alternative school\n5     111     5       Reportable program\n\n\nExample Two: American Community Survey\nThe American Community Survey is the current version of the Census Long Form. The codebook provided by the United Census Bureau is in PDF format, but is easily converted to a plain text file. This file required more modification that the CCD file described above, mostly removing line numbers that pasted over from the PDF as well as ensuring that descriptions did not span lines. The final modified version can be downloaded (here)[http://jason.bryer.org/codebook/acsPersonCodebook.txt]. Here are the first 10 lines of the file:\nSPORDER .Person number\nST .State Code\n    01 .Alabama/AL\n    02 .Alaska/AK\n    04 .Arizona/AZ\n    05 .Arkansas/AR\n    06 .California/CA\n    08 .Colorado/CO\n    09 .Connecticut/CT\n    10 .Delaware/DE\nFor this codebook file, all rows are character delimited on . (space period). We parse the file as follows:\nacs.codebook &lt;- parse.codebook('acsPersonCodebook.txt',\n                   var.names=c('var','desc'),\n                   level.names=c('level','label'),\n                   var.sep=' .', level.sep=' .')\nThe first six lines of the returned data frame are:\n&gt; head(acs.codebook)\n      var                                                                                desc linenum isfactor\n1 SPORDER                                                                       Person number       1    FALSE\n2      ST                                                                          State Code       2     TRUE\n3  ADJINC Adjustment factor for income and earnings dollar amounts (6 implied decimal places)      55    FALSE\n4   PWGTP                                                                     Person's weight      56    FALSE\n5    AGEP                                                                                 Age      57    FALSE\n6     CIT                                                                  Citizenship status      58     TRUE\nAnd factor levels:\n&gt; var.levels &lt;- attr(acs.codebook, 'levels')\n&gt; names(var.levels)\n [1] \"ST\"      \"CIT\"     \"COW\"     \"DRAT\"    \"ENG\"     \"GCM\"     \"JWRIP\"   \"JWTR\"    \"MAR\"     \"MARHM\"\n[11] \"MARHT\"   \"MARHW\"   \"MIG\"     \"MIL\"     \"NWAV\"    \"RELP\"    \"SCH\"     \"SCHG\"    \"SCHL\"    \"SEX\"\n[21] \"WKL\"     \"WKW\"     \"WRK\"     \"ANC\"     \"ANC1P\"   \"ANC2P\"   \"DECADE\"  \"DIS\"     \"DRIVESP\" \"ESP\"\n[31] \"ESR\"     \"FOD1P\"   \"6402\"    \"FOD2P\"   \"HICOV\"   \"HISP\"    \"INDP\"    \"JWAP\"    \"JWDP\"    \"LANP\"\n[41] \"MIGSP\"   \"MSP\"     \"NAICSP\"  \"NOP\"     \"OCCP02\"  \"OCCP10\"  \"PAOC\"    \"POBP\"    \"POWSP\"   \"PRIVCOV\"\n[51] \"PUBCOV\"  \"QTRBIR\"  \"RAC1P\"   \"RAC2P\"   \"RAC3P\"   \"SFN\"     \"SFR\"     \"SOCP00\"  \"SOCP10\"  \"VPS\"\n[61] \"WAOB\"    \"FHINS3C\" \"FHINS4C\" \"FHINS5C\"\n&gt; var.levels[['CIT']]\n  linenum level                                                                        label\n1      59     1                                                             Born in the U.S.\n2      60     2 Born in Puerto Rico, Guam, the U.S. Virgin Islands, or the Northern Marianas\n3      61     3                                            Born abroad of American parent(s)\n4      62     4                                               U.S. citizen by naturalization\n5      63     5                                                    Not a citizen of the U.S.\n\n\nConclusion\nAlthough a standard codebook format doesn’t exist, most adopt a similar format. I have outlined the parse.codebook function that, with minimal reformatting of the original codebook file, be used to read a codebook into R. This is tremendously useful as we can now merge in variable descriptions when creating tables and figures, as well as recode factors with their longer descriptions in an automated fashion."
  },
  {
    "objectID": "posts/2012-12-10-Markdown_Jekyll_R_for_Blogging.html",
    "href": "posts/2012-12-10-Markdown_Jekyll_R_for_Blogging.html",
    "title": "Using (R) Markdown, Jekyll, & Github for a Website",
    "section": "",
    "text": "Introduction\nMarkdown has been growing in popularity for writing documents on the web. With the introduction of R Markdown (see also Jeromy Anglim’s post on getting started with R Markdown) and knitr, R Markdown has simplified the publishing of R analysis on the web. I recently converted my website from Wordpress to Jekyll. Jekyll is a “static site generator” and is the framework used by GitHub Pages. You can view the complete source for this website on Github at https://github.com/jbryer/jbryer.github.com.\nI have outlined two approaches for integrating R Markdown within the Jekyll framework. The first approach implements a Jekyll Converter that will convert rmd files (the default but configurable) when Jekyll processes the site. The second uses a shell script and R function to convert rmd files to a plain Markdown file that Jekyll can then process. This approach is necessary when using GitHub Pages because user plugins are not supported.\n\n\nApproach One: Using a Jekyll Converter\nFirst, we need to install RinRuby to call R from Ruby. In the terminal, execute:\ngem install rinruby\nCreate rmarkdown.rb and place it in the _plugins folder. The convert class follows and can be downloaded here.\nmodule Jekyll\n    class RMarkdownConverter &lt; Converter\n        safe true\n        priority :low\n\n    def setup\n      return if @setup\n      require 'rinruby'\n      @setup = true\n    rescue\n      STDERR.puts 'do `gem install rinruby`'\n      raise FatalException.new(\"Missing dependency: rinruby\")\n    end\n\n    def matches(ext)\n      ext =~ /rmd/i\n    end\n\n    def output_ext(ext)\n      '.html'\n    end\n\n    def convert(content)\n      setup\n      R.eval \"require(knitr)\"\n      R.assign \"content\", content\n      R.eval \"content &lt;- (knitr::knit2html(text = content, fragment.only = TRUE))\"\n      R.pull \"content\"\n    end\n  end\nend\nIn order to use the rmd file extension (see the ext =~ /rmd/i line to change the extension used) you need to specify the markdown file extension in the _config.yml configuration file. Otherwise Jekyll will attempt to process rmd files as plain Markdown files. This also means that you cannot use md file extension for markdown files. See this discussion on StackOverflow.\nmarkdown_ext: markdown\nOnce created, RMarkdownConverter will convert rmd files to html each time Jekyll runs.\n\n\nApproach Two: Pre-process R Markdown Files\nThis approach is necessary for Github Pages since plugins are not supported. Using this approach, we can convert the R Mardown file to plain Markdown using the R script below. The converted Markdown file will be saved in the same directory so that Jekyll can then convert the resulting file. For simplicity, I place the rmarkdown.r function in the root directory of my site (alternatively you can place this in your .Rprofile file in your home directory). I then call rmd.sh (also located in the root directory) to first, determine the directory where the script is be executed from, and two, call the convertRMarkdown function. This function will process all R Markdown files (.rmd by default) in the current working directory (which can be set explicitly with the dir parameter or by the rmd.sh script) and convert them to plain markdown (with .markdown file extension by default). Once converted, Jekyll will the process the resulting file(s). This file can be downloaded here.\n#' This R script will process all R mardown files (those with in_ext file extention,\n#' .rmd by default) in the current working directory. Files with a status of\n#' 'processed' will be converted to markdown (with out_ext file extention, '.markdown'\n#' by default). It will change the published parameter to 'true' and change the\n#' status parameter to 'publish'.\n#'\n#' @param dir the directory to process R Markdown files.\n#' @param out_ext the file extention to use for processed files.\n#' @param in_ext the file extention of input files to process.\n#' @param recursive should rmd files in subdirectories be processed.\n#' @return nothing.\n#' @author Jason Bryer &lt;jason@bryer.org&gt;\nconvertRMarkdown &lt;- function(dir=getwd(), images.dir=dir, images.url='/images/',\n           out_ext='.markdown', in_ext='.rmd', recursive=FALSE) {\n  require(knitr, quietly=TRUE, warn.conflicts=FALSE)\n  files &lt;- list.files(path=dir, pattern=in_ext, ignore.case=TRUE, recursive=recursive   )\n  for(f in files) {\n    message(paste(\"Processing \", f, sep=''))\n    content &lt;- readLines(f)\n    frontMatter &lt;- which(substr(content, 1, 3) == '---')\n    if(length(frontMatter) == 2) {\n      statusLine &lt;- which(substr(content, 1, 7) == 'status:')\n      publishedLine &lt;- which(substr(content, 1, 10) == 'published:')\n      if(statusLine &gt; frontMatter[1] & statusLine &lt; frontMatter[2]) {\n        status &lt;- unlist(strsplit(content[statusLine], ':'))[2]\n        status &lt;- sub('[[:space:]]+$', '', status)\n        status &lt;- sub('^[[:space:]]+', '', status)\n        if(tolower(status) == 'process') {\n          #This is a bit of a hack but if a line has zero length (i.e. a\n          #black line), it will be removed in the resulting markdown file.\n          #This will ensure that all line returns are retained.\n          content[nchar(content) == 0] &lt;- ' '\n          message(paste('Processing ', f, sep=''))\n          content[statusLine] &lt;- 'status: publish'\n          content[publishedLine] &lt;- 'published: true'\n          outFile &lt;- paste(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext, sep='')\n          render_markdown(strict=TRUE)\n          opts_knit$set(out.format='markdown')\n          opts_knit$set(base.dir=images.dir)\n          opts_knit$set(base.url=images.url)\n          try(knit(text=content, output=outFile), silent=FALSE)\n        } else {\n          warning(paste(\"Not processing \", f, \", status is '\", status,\n                  \"'. Set status to 'process' to convert.\", sep=''))\n        }\n      } else {\n        warning(\"Status not found in front matter.\")\n      }\n    } else {\n      warning(\"No front matter found. Will not process this file.\")\n    }\n  }\n  invisible()\n}\nHere is the source to the rmd.sh shell script for calling the convertRMarkdown function. This file can be downloaded here.\nDIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\nRscript -e \"source('$DIR/rmarkdown.r'); convertRMarkdown(images.dir='$DIR/images')\"\n\nYAML Front Matter\nThere are two parameters you can specify in the YAML Front Matter to alter how the convertRMarkdown function handles particular files. First, the published parameter should be set to false so that Jekyll will not attempt to process the file. The convertRMarkdown function will change this parameter to true in the resulting Markdown file. The second parameter, status, must be set to process for the convertRMarkdown function to convert the file. This is useful when working a draft of a document and you wish to not have the file converted.\npublished: false\nstatus: process\nLastly, one difficulty with Jekyll is the inclusion of images in posts. The default behavior assumes that all images will be saved in the /images directory. This can of course be configured using parameters to convertRMarkdown and knitr options.\n\n\n\nExample\nThe source for this post can be downloaded from GitHub. In this example we will analyze the reading attitude items for North America from the Programme of International Student Assessment using the likert package. The first R chuck will load and recode the data.\nrequire(likert)\ndata(pisana)\n\n## Warning: data set 'pisana' not found\n\nitems &lt;- pisana[,c(\n    'ST24Q01', #Only if I have to\n    'ST24Q02', #Favourite hobbies\n    'ST24Q03', #Talk about books\n    'ST24Q04', #Hard to finish\n    'ST24Q05', #Happy as present\n    'ST24Q06', #Waste of time\n    'ST24Q07', #Enjoy library\n    'ST24Q08', #Need information\n    'ST24Q09', #Cannot sit still\n    'ST24Q10', #Express opinions\n    'ST24Q11'  #Exchange\n    )]\n\n## Error: object 'pisana' not found\n\nnames(items) &lt;- c(\"I read only if I have to.\",\n        \"Reading is one of my favorite hobbies.\",\n        \"I like talking about books with other people.\",\n        \"I find it hard to finish books.\",\n        \"I feel happy if I receive a book as a present.\",\n        \"For me, reading is a waste of time.\",\n        \"I enjoy going to a bookstore or a library.\",\n        \"I read only to get information that I need.\",\n        \"I cannot sit still and read for more than a few minutes.\",\n        \"I like to express my opinions about books I have read.\",\n        \"I like to exchange books with my friends\")\n\n## Error: object 'items' not found\n\nfor(i in 1:ncol(items)) {\n    items[,i] &lt;-  factor(items[,i], levels=c(1,2,3,4), ordered=TRUE,\n        labels=c('Strongly Disagree', 'Disagree', 'Agree', 'Strongly Agree'))\n}\n\n## Error: object 'items' not found\n\nl &lt;- likert(items, grouping=pisana$CNT)\n\n## Error: object 'items' not found\nOnce the likert has been called we can print the summary.\noptions(width=120)\nsummary(l)\n\nError: object 'l' not found\nAnd of course, we can include plots.\nplot(l, centered=TRUE)\n\n## Error: object 'l' not found\n\n\nFinal Thoughts\nThe conversion from Wordpress wasn’t necessarily trivial, but the benefits of using Jekyll have made the conversion worth while. The ability to embed R code within the site’s content makes writing posts about R much easier than executing R code, copy and paste to Wordpress (or Gists), and publishing in a database back system for a site that changes relatively infrequently. I will soon be publishing results from a large study and this exercise has shown that R Markdown is an ideal solution.\nLaslty, I must give a big thanks to Tal Galili who maintains R-Bloggers for his help and patience as I worked out the issues getting the RSS feed to work with his platform."
  },
  {
    "objectID": "posts/2012-08-13-User-Input-using-tcltk.html",
    "href": "posts/2012-08-13-User-Input-using-tcltk.html",
    "title": "User Input using tcl/tk",
    "section": "",
    "text": "I was inspired by Kay Cichini recent post on creating a a tcl/tk dialog box for users to enter variable values. I am going to have a use for this very soon so took some time to make it a bit more generic. What I wanted is a function that takes a vector (of variable names) of arbitrary length, create a dialog box for an input for each, and return the values in a list. While I was at it I also provided an optional separate vector for the labels (defaults to the variable names) that the user will see and a vector of functions used to convert the text input into another data format (e.g. convert characters to numeric values). Obviously using built in functions works great but one could also easily exploit this feature to write custom data validation functions. The function is hosted on Gist and embedded bellow. Here are some very basic examples:\nDialog box with two variables.\n&gt; vals &lt;- varEntryDialog(vars=c('Variable1', 'Variable2'))\n\n&gt; str(vals)\nList of 2\n$ Variable1: chr \"value 1\"\n$ Variable2: chr \"value 2\"\nDialog box with two variables, custom labels, and converts one to an integer.\n&gt; vals &lt;- varEntryDialog(vars=c('Var1', 'Var2'), labels=c('Enter an integer:', 'Enter a string:'), fun=c(as.integer, as.character))\n\n&gt; str(vals)\nList of 2\n$ Var1: int 5\n$ Var2: chr \"abc\"\nDialog box with validation.\n&gt; vals &lt;- varEntryDialog(vars=c('Var1'), labels=c('Enter an integer between 0 and 10:'), fun=c(\nfunction(x) {\n     x &lt;- as.integer(x)\n     if(x &gt;= 0 & x &lt;= 10) {\n         return(x)\n     } else {\n         stop(\"Why didn't you follow instruction!\\nPlease enter a number between 0 and 10.\")\n     }\n } ))\n\n\n\n&gt; str(vals)\nList of 1\n$ Var1: int 2\nUser inputs a comma separated list that is split into a character vector.\n&gt; vals &lt;- varEntryDialog(vars=c('Var1'),\n     labels=c('Enter a comma separated list of something:'),\n     fun=c(function(x) {\n         return(strsplit(x, split=','))\n }))\n\n&gt; vals$Var1\n[[1]]\n[1] \"a\" \"b\" \"c\"\n&gt; str(vals)\nList of 1\n$ Var1:List of 1\n..$ : chr [1:3] \"a\" \"b\" \"c\""
  },
  {
    "objectID": "posts/2012-01-20-object-oriented-programming-in-r.html",
    "href": "posts/2012-01-20-object-oriented-programming-in-r.html",
    "title": "Object Oriented Programming in R",
    "section": "",
    "text": "As someone who was a Java programmer for many years learning R’s object oriented programming framework has been frustrating to say the least. I like the simplicity of S3 but find it limiting when you wish to write methods that change the underlying data elements. That is, printing, summarizing, and plotting work great because they generally do not require changes to the data in the class passed to it. After much experimenting it occurred to me that perhaps I could achieve a more Java like behavior by adding functions to the class. For simple things this works great. To make it even more flexible I found that if you change the list to an environment before assigning the class allows one to change lists within the list.\nThe following example models the framework for an email class (without actually doing anything useful). That is, I want a class that contains an email address and name and the ability to send email. I would also like to save a history of the emails sent. As can be seen, functions that work with atomic variables are pretty straight forward. Working with lists require (well maybe required, if you know of a better way leave a comment) using the assign function. This may not produce the cleanest source code but (IMHO) provides a better experience for the user.\n\nConstructor\nEmailClass &lt;- function(name, email) {\n    nc = list(\n        name = name,\n        email = email,\n        get = function(x) nc[[x]],\n        set = function(x, value) nc[[x]] &lt;&lt;- value,\n        props = list(),\n        history = list(),\n        getHistory = function() return(nc$history),\n        getNumMessagesSent = function() return(length(nc$history))\n    )\n    #Add a few more methods\n    nc$sendMail = function(to) {\n        cat(paste(\"Sending mail to\", to, 'from', nc$email))\n        h &lt;- nc$history\n        h[[(length(h)+1)]] &lt;- list(to=to, timestamp=Sys.time())\n        assign('history', h, envir=nc)\n    }\n    nc$addProp = function(name, value) {\n        p &lt;- nc$props\n        p[[name]] &lt;- value\n        assign('props', p, envir=nc)\n    }\n    nc &lt;- list2env(nc)\n    class(nc) &lt;- \"EmailClass\"\n    return(nc)\n}\n\n\nDefine S3 generic method for the print function.\nprint.EmailClass &lt;- function(x) {\n    if(class(x) != \"EmailClass\") stop();\n    cat(paste(x$get(\"name\"), \"'s email address is \", x$get(\"email\"), sep=''))\n}\n\n\nTest code\ntest &lt;- EmailClass(name=\"Jason\", \"jason@bryer.org\")\ntest$addProp('hello', 'world')\ntest$props\ntest\nclass(test)\nstr(test)\ntest$get(\"name\")\ntest$get(\"email\")\ntest$set(\"name\", \"Heather\")\ntest$get(\"name\")\ntest\ntest$sendMail(\"jbryer@excelsior.edu\")\ntest$getHistory()\ntest$sendMail(\"test@domain.edu\")\ntest$getNumMessagesSent()\n\ntest2 &lt;- EmailClass(\"Nobody\", \"dontemailme@nowhere.com\")\ntest2\ntest2$props\ntest2$getHistory()\ntest2$sendMail('nobody@exclesior.edu')"
  },
  {
    "objectID": "posts/2011-11-11-visualizing-likert-items.html",
    "href": "posts/2011-11-11-visualizing-likert-items.html",
    "title": "Visualizing Likert Items",
    "section": "",
    "text": "I have become quite a big fan of graphics that combine the features of traditional figures (e.g. bar charts, histograms, etc.) with tables. That is, the combination of numerical results with a visual representation has been quite useful for exploring descriptive statistics. I have wrapped two of my favorites (build around ggplot2) and included them as part of my irutils R package (currently under development). Here is the code and results utilizing two item from the 2009 Programme of International Student Assessment (PISA).\nlibrary(devtools)\ninstall_github('irutils','jbryer')\nlibrary(irutils)\nlibrary(ggplot2)\n\ndata(pisa)\nitems29 = pisa[,substr(names(pisa), 1,5) == 'ST25Q']\nnames(items29) = c(\"Magazines\", \"Comic books\", \"Fiction\", \"Non-fiction books\", \"Newspapers\")\nfor(i in 1:ncol(items29)) {\n     items29[,i] = factor(items29[,i], levels=1:5,\n     labels=c('Never or almost never', 'A few times a year', 'About once a month',\n          'Several times a month', 'Several times a week'), ordered=TRUE)\n}\n\nplotHeatmapTable(items29) + opts(title=\"How often do you read these materials because you want to?\")\n\n\n\nimg\n\n\nitems28 = pisa[,substr(names(pisa), 1,5) == 'ST24Q']\nhead(items28); ncol(items28)\nnames(items28) = c(\"I read only if I have to.\",\n        \"Reading is one of my favorite hobbies.\",\n        \"I like talking about books with other people.\",\n        \"I find it hard to finish books.\",\n        \"I feel happy if I receive a book as a present.\",\n        \"For me, reading is a waste of time.\",\n        \"I enjoy going to a bookstore or a library.\",\n        \"I read only to get information that I need.\",\n        \"I cannot sit still and read for more than a few minutes.\",\n        \"I like to express my opinions about books I have read.\",\n        \"I like to exchange books with my friends\")\nfor(i in 1:ncol(items28)) {\n    items28[,i] = factor(items28[,i], levels=1:4,\n        labels=c('Strongly disagree', 'Disagree', 'Agree', 'Strongly Agree'), ordered=TRUE)\n}\n\nplotBarchartTable(items28, low.color='maroon', high.color='burlywood4')\n\n\n\nimg\n\n\n\n\n\nimg"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDownsampling for predictive modeling\n\n\n\n\n\nThis is an exploration of an approach to downsampling to address imbalance in a dependent variable while still leveraging all available data.\n\n\n\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap vs Standard Error Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nShinyQDA: R Package and Shiny Application for the Analysis of Qualitative Data\n\n\n\n\n\nI will be giving a talk at ShinyConf 2025 on a Shiny application desiged for doing qualitative data analysis.\n\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSample size and statistical significance for chi-squared tests\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a portfolio with Github and Quarto\n\n\n\n\n\nSlides for a talk on how to build a portfolio website using Github\n\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow many times do I need to take a test to randomly get all questions correct?\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nlogin: User Authentication for Shiny Applications\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Visual Introduction to Propensity Score Analysis\n\n\n\n\n\nRecording of A Visual Introduction to Propensity Score Analysis given for the nyhackr meetup group.\n\n\n\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Causality from Observational Data\n\n\n\n\n\nThe CUNY School of Professional Studies, Data Science and Information Systems department, is hosting a talk by Jason Bryer titled Estimating Causality from Observation Data. You can attend the talk in person at 119 West 31st Street, NY, New York, 10001 or watch the live stream. Please complete this form indicating your interest. Details on attending live or getting the Zoom link will be sent to the email address provided. Light refreshments will be provided for those attending in person.\n\n\n\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nR Package Development\n\n\n\n\n\nRecording for an introduction to R package development.\n\n\n\n\n\nMar 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Shiny\n\n\n\n\n\nRecording for an introduction to Shiny application development.\n\n\n\n\n\nNov 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMap my run in R\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFramework for Shiny Apps in R Packages\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nEditable DataTables for Shiny Applications\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nConducting Assessments and Surveys with Shiny\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nShiny App for Bayes Billiards Problem\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nData Caching\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nWomen Graduates in Math, Statistics, and Computer Information Systems\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nuseR 2014 Slides for PSAboot and version 1.1. on CRAN\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nstr Implementation for Data Frames\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nRgitbook Package for Using R Markdown with Gitbook\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nAlbany, NY R Users Group\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nFunction to Simplify Loading and Installing Packages\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2014\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping for Propensity Score Analysis\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop and Talk Slides from NEAIR Conference\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nGambler’s Run with Shiny\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nCut Dates into Quarters\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2013\n\n\n\n\n\n\n\n\n\n\n\n\ni Before e Except After c\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nConverting a list to a data frame\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nComparing two data frames with different number of rows\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.0 of sqlutils available on CRAN\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive SQL in R\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nReading Codebook Files in R\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nFunction for Generating LaTeX Tables with Decimal Aligned Numbers\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2013\n\n\n\n\n\n\n\n\n\n\n\n\nUsing (R) Markdown, Jekyll, & Github for a Website\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nFifty Shades of Grey in R\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Missing Data\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nUser Input using tcl/tk\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nGraphic Parameters (symbols, line types, and colors) for ggplot2\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nGiven a room with n people in it, what is the probability any two will have the same birthday?\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nObject Oriented Programming in R\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieving RSS Feeds Using Google Reader\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2012\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing Likert Items\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2011\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jason Bryer, Ph.D.",
    "section": "",
    "text": "Dr. Jason Bryer is currently an Assistant Professor and Associate Director in the Data Science and Information Systems department at the City University of New York. He is currently the Principal Investigator of the FIPSE ($3 million #P116F150077) and IES funded ($3.8 million R305A210269) Diagnostic Assessment and Achievement of College Skills (DAACS), which is a suite of technological and social supports designed to optimize student learning. Dr. Bryer’s other research interests include quasi-experimental designs with an emphasis on propensity score analysis, data systems to support formative assessment, and the use of open source software for conducting reproducible research. He is the author of over a dozen R packages, including three related to conducting propensity score analyses. When not crunching numbers, Jason is a wedding photographer and proud dad to three boys.\n\n\n Ph.D. in Educational Psychology & Methodology, 2014, University at Albany\n M.S. in Educational Psychology & Methodology, 2009, University at Albany\n B.S. in Mathematics, 1999, The College of Saint Rose\n\n\n\n\nPropensity Score Analysis\nFormative and Diagnostic Assessments\nPredictive Analytics\nTeaching Statistics\nData Visualization\nR, Shiny"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jason Bryer, Ph.D.",
    "section": "",
    "text": "Ph.D. in Educational Psychology & Methodology, 2014, University at Albany\n M.S. in Educational Psychology & Methodology, 2009, University at Albany\n B.S. in Mathematics, 1999, The College of Saint Rose"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Jason Bryer, Ph.D.",
    "section": "",
    "text": "Propensity Score Analysis\nFormative and Diagnostic Assessments\nPredictive Analytics\nTeaching Statistics\nData Visualization\nR, Shiny"
  },
  {
    "objectID": "posts/2012-01-13-retrieving-rss-feeds-using-google-reader.html",
    "href": "posts/2012-01-13-retrieving-rss-feeds-using-google-reader.html",
    "title": "Retrieving RSS Feeds Using Google Reader",
    "section": "",
    "text": "I have been working on a new package makeR to help manage Sweave projects where you wish to create multiple versions of documents that are based on a single source. For example, I create lots of monthly and quarterly reports using Sweave and the only differences between versions are a few variables. I have used GNU make and Apache ANT but wanted a 100% R solution. I will have more to write about that project in a few weeks. In the meantime I needed an example I could use publicly which led me to thinking about analyzing R-Bloggers. I wanted to use the RSS feed to get the frequency of posts and the tags and categories used. However, R-Bloggers, like most blogs, limits the RSS feeds to the latest few. Google Reader however keeps them all (or at least a lot more). The only downside is that you need to have a Google Reader account. The source code is hosted on Gist and provided below.\n\nSetup: retrieve the RSS feed and save it.\nsource('https://raw.github.com/gist/1606595/269d61dfcc7930f5275a212e11f3c43771ab2591/GoogleReader.R')\n\nrbloggers = getRSSFeed(feedURL=\"http://r-bloggers.com/feed\",\n               email=\"GOOGLE READER EMAIL\",\n               passwd=\"GOOGLE READER PASSWORD\",\n               posts=5000)\nentries = rbloggers[which(names(rbloggers) == \"entry\")]\nlength(entries)\nsaveXML(rbloggers, file='rbloggers.xml')\n\n#This will create a data frame with some of the information from the RSS feed\nposts = data.frame(title=character(0), author=character(0),\n                   link=character(0), stringsAsFactors=FALSE)\nposts[1:length(entries),1:ncol(posts)] = NA\nposts$published = as.Date(NA)\nposts.categories = list()\nfor(i in 1:length(entries)) {\n    entry = entries[[i]]\n    posts[i,]$title = unclass(xmlChildren(entry[['title']])$text)$value\n    posts[i,]$author = unclass(xmlChildren(entry[['author']][['name']])$text)$value\n    posts[i,]$link = xmlAttrs(entry[['link']])[['href']]\n    posts[i,]$published = as.Date(substr(unclass(\n        xmlChildren(entry[['published']])$text)$value, 1, 10))\n    categories = entry[which(names(entry) == \"category\")]\n    posts.categories[[i]] = list()\n    if(length(categories) &gt; 1) { #Ignore the first category as it is used for Google Reader\n        l = list()\n        for(j in 2:length(categories)) {\n            l[(j-1)] = xmlAttrs(categories[[j]])[['term']]\n        }\n        posts.categories[[i]] = l\n    }\n}\n\n\nWe’ll use Paul Bleicher’s calendarHeat function to visualize the number of posts per day\nsource('https://raw.github.com/tavisrudd/r_users_group_1/master/calendarHeat.R')\ncal = as.data.frame(table(posts$published))\ncal$Var1 = as.Date(cal$Var1)\ncalendarHeat(cal$Var1, cal$Freq, color=\"r2b\", varname=\"Number of Posts on R-Bloggers.com\")\n\n\n\nimg\n\n\n\n\nCreate a word cloud\nrequire(wordcloud)\nctab = unlist(posts.categories)\nctab = unlist(strsplit(ctab, ' '))\nctab = as.data.frame(table(ctab))\nctab = ctab[-which(ctab$ctab == 'Uncategorized'),]\nwordcloud(ctab$ctab, ctab$Freq, min.freq=10)\n\n\n\nimg\n\n\n######The getRSSFeed function. Note that this function is included in the makeR package.\nrequire(XML)\nrequire(RCurl)\n\n#' This function ruturns an XML tree of the RSS feed from the given URL.\n#'\n#' This function utilizes the (unofficial) Google Reader API to retrieve RSS\n#' feeds. The advantage of access RSS feeds through the Google Reader API is that\n#' you are not limited by the number of entries a website may included in their\n#' feed. That is, Google maintains generally maintains a complete history of\n#' entries from the RSS feed.\n#'\n#' Note that the contents of the results will be limited by what the website\n#' provides in their feeds. That is, Google does not contain more information\n#' per entry then what the website originally provided. If the initial feed\n#' contained only excerpts of the article, the feed from Google will too only\n#' contain excerpts. Be aware though that for sites that do provide the complete\n#' contents of posts will result in potentially very large downloads.\n#'\n#' @param feedURL the full URL to the RSS feed.\n#' @param email the email address for the Google Reader account\n#' @param passwd the password for the Google Reader account\n#' @param posts the number of posts to return\n#' @return the root \\code{XMLNode} for the RSS feed.\n#' @seealso \\code{\\link{/xmlRoot}} for the format of the returned XML tree\n#' @export\n#' @example\n#' \\dontrun{\n#' rbloggers = getRSSFeed(feedURL=\"http://r-bloggers.com/feed\",\n#'     email=\"USERNAME@gmail.com\", passwd=\"PASSWORD\")\n#' }\n#' @author Jason Bryer &lt;\\email{jason@@bryer.org}x&gt;\ngetRSSFeed &lt;- function(feedURL, email, passwd, posts=1000) {\n    #Authenticate with Google\n    curlHandle = getCurlHandle(cookiefile=\"rcookies\", ssl.verifyhost=FALSE, ssl.verifypeer=FALSE)\n    x = postForm(\"https://www.google.com/accounts/ClientLogin\",\n                 accountType=\"GOOGLE\",\n                 service=\"reader\",\n                 Email=email,\n                 Passwd=passwd,\n                 source=\"makeR\",\n                 curl = curlHandle)\n    gtoken = unlist(strsplit(x, \"\\n\"))\n    parsed.gtoken &lt;- unlist(strsplit(gtoken[3], \"Auth=\"))\n    if (length(parsed.gtoken) &gt;= 2) {\n        auth.token &lt;- unlist(strsplit(gtoken[3], \"Auth=\"))[[2]]\n    } else {\n        stop(\"Authentication failed.\")\n    }\n    google.auth &lt;- paste(\"GoogleLogin auth=\", auth.token, sep='')\n\n    #Function to retrieve up to 1000 posts\n    getDoc &lt;- function(n, c=NULL) {\n        feedURL = paste(\"http://www.google.com/reader/atom/feed/\", feedURL, \"?n=\", n,\n                        ifelse(is.null(c), \"\", paste(\"&c=\", c, sep='')),\n                        sep='')\n        feed = getURL(feedURL, .encoding = 'UTF-8', followlocation=TRUE,\n                      httpheader=c(\"Authorization\"=google.auth),\n                      curl=curlHandle)\n        doc = xmlTreeParse(feed, asText=TRUE)\n        return(xmlRoot(doc))\n    }\n\n    root = NULL\n    continueValue = NULL\n    for(i in 1:ceiling(posts / 1000)) {\n        r = getDoc(n=ifelse(i == ceiling(posts / 1000), (posts-1) %% 1000 + 1, 1000),\n                   c=continueValue)\n        if(is.null(root)) {\n            root = r\n        } else {\n            entries = which(xmlSApply(r, xmlName) == 'entry')\n            if(length(entries) &gt; 0) {\n                root = addChildren(root, kids=r[entries])\n            }\n        }\n        if(is.null(r[['continuation']])) {\n            break #No more posts to retrieve\n        } else {\n            continueValue = unclass(xmlChildren(r[['continuation']])$text)$value\n        }\n    }\n    return(root)\n}"
  },
  {
    "objectID": "posts/2012-01-31-given-a-room-with-n-people-in-it-what-is-the-probability-any-two-will-have-the-same-birthday.html",
    "href": "posts/2012-01-31-given-a-room-with-n-people-in-it-what-is-the-probability-any-two-will-have-the-same-birthday.html",
    "title": "Given a room with n people in it, what is the probability any two will have the same birthday?",
    "section": "",
    "text": "Revisiting a fun puzzle I remember first encountering as an undergraduate. Nice example of creating a plot in R using ggplot2. I also plot the probability of someone in the room having the same birthday as you.\n\n\n\nimg\n\n\n## See http://en.wikipedia.org/wiki/Birthday_problem for an explanation  of the problem\nrequire(ggplot2)\nrequire(reshape)\n\ntheme_update(panel.background=theme_blank(),\n             panel.grid.major=theme_blank(),\n             panel.border=theme_blank())\n\nbirthday &lt;- function(n) {\n    1 - exp( - n^2 / (2 * 365) )\n}\n\nmyBirthday &lt;- function(n) {\n    1 - ( (365 - 1) / 365 ) ^ n\n}\n\nd = 200\ndf = data.frame(n=1:d, AnyTwoSame=birthday(1:d), SameAsMine=myBirthday(1:d))\ndf = melt(df, id.vars='n')\n\nggplot(df, aes(x=n, y=value, colour=variable)) + geom_line() + scale_colour_hue('') +\n    xlab('Number of People in Group') + ylab('Probability')"
  },
  {
    "objectID": "posts/2012-11-17-visualizing-missing-data.html",
    "href": "posts/2012-11-17-visualizing-missing-data.html",
    "title": "Visualizing Missing Data",
    "section": "",
    "text": "There are several graphics available for visualizing missing data including the VIM package. However, I wanted a plot specifically for looking at the nature of missingness across variables and a clustering variable of interest to support data preparation in multilevel propensity score models (see the multilevelPSA package). The following examples uses data from the Programme of International Student Assessment (PISA; see pisa package).\nThe required packages can be downloaded from github. Note that the pisa package is approximately 75mb.\n&gt; require(devtools)\n&gt; install_github('multilevelPSA', 'jbryer')\n&gt; install_github('pisa', 'jbryer')\nThe following will setup the data to be plotted. There is a pisa.setup.R script included in the multilevelPSA package that is included to assist with a demo there. Among many things, it creates a vector psa.cols that defines the variables of interest in performing a propensity score analysis. These are the variables where missingness needs to be addressed.\n&gt; require(multilevelPSA)\n&gt; require(pisa)\n&gt; data(pisa.student)\n&gt; pkgdir = system.file(package='multilevelPSA')\n&gt; source(paste(pkgdir, '/pisa/pisa.setup.R', sep=''))\n&gt; student = pisa.student[,psa.cols]\n&gt; student$CNT = as.character(student$CNT)\nAnd finally, to create the graphic use the plot.missing command.\n&gt; plot.missing(student[,c(4:48)], student$CNT)\n\n\n\nMissing Plot for 2009 PISA"
  },
  {
    "objectID": "posts/2013-01-04-xtable_with_aligned_decimals.html",
    "href": "posts/2013-01-04-xtable_with_aligned_decimals.html",
    "title": "Function for Generating LaTeX Tables with Decimal Aligned Numbers",
    "section": "",
    "text": "The xtable package is tremendously useful for generating LaTeX tables from data frames. It is also pretty easy to customize the output to handle some special cases of LaTeX formatting. The xtable.decimal function will create a LaTeX table where numeric columns will be vertically aligned on the decimal point. In addition to specifying the LaTeX alignment code it will also create appropriate column titles so that the column name spans the two resulting columns. In the following example, we create a data frame with five columns, three of which are numeric that we want to display with aligned decimal places. We also have a column of type character and another of type integer.\n&gt; df &lt;- data.frame(Id=letters[1:10],\n        Split1=rnorm(10, mean=0, sd=10),\n        Numbers=1:10,\n        Split2=rnorm(10, mean=-1, sd=.5),\n        Split3=rnorm(10, mean=10, sd=.75))\n#A whole number because prettyNum will not print anything after the decimal point.\n&gt; df[5,'Split1'] = 2\n&gt; df\n    Id      Split1 Numbers     Split2    Split3\n 1   a -11.4957434       1 -1.0974334 10.895100\n 2   b  11.5985173       2 -1.6314018  9.539108\n 3   c   0.1062397       3 -1.1795816 10.788935\n 4   d   1.1586916       4 -0.4612825 10.462340\n 5   e   2.0000000       5 -0.5154210  9.279148\n 6   f  14.1508151       6 -1.1308398 10.001278\n 7   g  -6.7793246       7  0.3325152 10.026043\n 8   h   1.4984447       8 -1.3638784 10.373774\n 9   i   4.1131184       9 -1.2390835 10.996286\n10   j   5.4228637      10 -1.8332372  9.881770\n&gt; str(df)\n'data.frame':   10 obs. of  5 variables:\n $ Id     : Factor w/ 10 levels \"a\",\"b\",\"c\",\"d\",..: 1 2 3 4 5 6 7 8 9 10\n $ Split1 : num  -0.5072 13.3672 -0.0906 -9.5944 2 ...\n $ Numbers: int  1 2 3 4 5 6 7 8 9 10\n $ Split2 : num  -1.359 -0.722 -1.341 -0.359 -1.022 ...\n $ Split3 : num  9.87 10.29 9.55 8.51 10.36 ...\nThe xtable.decimal function (source code below) has five parameters:\n\nx the data frame to convert.\ncols the columns to align. This defaults to columns of type numeric but can be specified explicitly as a numeric vector specifying the column position within x.\ncolAlignment all non-aligned columns will be aligned left (i.e. l) by default. If you wish to align any columns right (r) or centered (c), then create a named vector where the name corresponds to the column name (as identified by names(x)) and the value the new alignment.\ntocharFun the function that will be used to convert the column to a character vector. This is prettyNum by default.\n... other parameters that are passed to tocharFun, xtable, and print.xtable.\n\nHere we will create the LaTeX table for the data frame created above.\n xtable.decimal(df, digits=3,\n     colAlignment=c(Numbers='c'),\n     caption.placement='bottom',\n     caption='Test Data Frame')\nAnd the resulting table as it appears in the PDF:\n\n\n\nxtable Output\n\n\nLink to xtable.decimal.r as a Gist.\nrequire(xtable)\n\n#' Prints a LaTeX table with numeric columns aligned on their decimal points.\n#'\n#' This function wraps the \\code{\\link{xtable}} and \\code{\\link{print.xtable}}\n#' functions in the \\code{xtable} package so that numeric columns are aligned\n#' on their decimal place.\n#'\n#' See \\url{http://jason.bryer.org/posts/2013-01-04/xtable_with_aligned_decimals.html}\n#' for more information.\n#'\n#' @author Jason Bryer &lt;jason@@bryer.org&gt;\n#' @param x a data frame to create a LaTeX table from.\n#' @param cols a numeric vector indicating which columns should be aligned on\n#'        decimal points. It defaults to all columns of type numeric.\n#' @param colAlignment named character vector where each element name corresponds to a\n#         column name and the value is the LaTeX alignment (i.e. l, r, or c).\n#' @param tocharFun the function used to convert the numeric vecotr to a character\n#'        vector. This defaults to \\code{\\link{prettyNum}}, but other possible\n#'        options are \\code{\\link{as.character}}, \\code{\\link{format}},\n#'        \\code{\\link{formatC}}, or some other custom function.\n#' @param ... other parameters passed to \\code{tocharFun}, \\code{\\link{xtable}},\n#'        and \\code{\\link{print.xtable}}.\n#' @seealso xtable\n#' @export\nxtable.decimal &lt;- function(x,\n            cols=which(lapply(x, class) == 'numeric'),\n            colAlignment,\n            tocharFun=prettyNum,\n            ...) {\n    splitCol &lt;- function(x, ...) {\n        s &lt;- strsplit(tocharFun(x, ...), split='.', fixed=TRUE)\n        right &lt;- sapply(s, FUN=function(x) { ifelse(length(x) == 2, x[2], '0') })\n        left &lt;- sapply(s, FUN=function(x) { x[1] })\n        data.frame(left=left, right=right, stringsAsFactors=FALSE)\n    }\n\n    cols &lt;- cols[order(cols, decreasing=TRUE)]\n    colnames &lt;- names(x)\n    for(i in cols) {\n        if(i == 1) {\n            tmp &lt;- cbind(splitCol(x[,1], ...), x[,2:ncol(x)])\n            names(tmp)[1:2] &lt;- paste(names(tmp)[1], c('left','right'), sep='.')\n            names(tmp)[3:ncol(x)] &lt;- names(x)[2:ncol(x)]\n            x &lt;- tmp\n        } else if(i == ncol(x)) {\n            tmp &lt;- cbind(x[,1:(ncol(x)-1)], splitCol(x[,ncol(x)], ...))\n            names(tmp)[1:(ncol(tmp)-2)] &lt;- names(x)[1:(ncol(x)-1)]\n            names(tmp)[(ncol(tmp)-1):ncol(tmp)] &lt;- paste(names(x)[ncol(x)],\n                        c('left','right'), sep='.')\n            x &lt;- tmp\n        } else {\n            tmp &lt;- cbind(x[,1:(i-1)], splitCol(x[,i], ...), x[,(i+1):ncol(x)])\n            names(tmp)[1:(i-1)] &lt;- names(x)[1:(i-1)]\n            names(tmp)[i:(i+1)] &lt;- paste(names(x)[i], c('left','right'), sep='.')\n            names(tmp)[(i+2):ncol(tmp)] &lt;- names(x)[(i+1):ncol(x)]\n            x &lt;- tmp\n        }\n    }\n\n    colnames[cols] &lt;- paste('\\\\multicolumn{2}{c}{', colnames[cols], '}', sep='')\n    colnames &lt;- paste(colnames, collapse=' & ')\n\n    addtorow &lt;- list()\n    addtorow$pos &lt;- list()\n    addtorow$pos[[1]] &lt;- c(0)\n    addtorow$command &lt;- paste( colnames, ' \\\\\\\\ ', sep='')\n\n    align &lt;- rep('l', ncol(x))\n    if(!missing(colAlignment)) {\n        for(i in seq_along(colAlignment)) {\n            align[names(x) == names(colAlignment)[i]] &lt;- colAlignment[i]\n        }\n    }\n    align[grep('.left$', names(x), perl=TRUE)] &lt;- 'r@{.}'\n    align &lt;- c('l', align) #Add an alignment for row names\n\n    xtab &lt;- xtable(x, align=align, ...)\n    print(xtab, add.to.row=addtorow, include.rownames=FALSE, include.colnames=FALSE, ...)\n}"
  },
  {
    "objectID": "posts/2013-01-12-Interactive_SQL_in_R.html",
    "href": "posts/2013-01-12-Interactive_SQL_in_R.html",
    "title": "Interactive SQL in R",
    "section": "",
    "text": "I recently taught a very basic introduction to SQL workshop and needed a way to have participants interact with SQL statements. Obviously there are lots of tools to interface with a database, but since we are all R users I thought it would be nice to be able interact without leaving R. Although this interface is fairly basic, the fact that we can type in a SQL statement and get the results as an R data frame provides all the advantages of having data in R. Moreover, I found this to be an interesting exercise in see the power of R as programming language, not just as statistical software. The function described here is part of the sqlutils package which was created to manage a library of SQL files. More information about that is provided on the project page and I will likely have a forthcoming blog post too.\nFirst we need to create a database to interact with. In this example we will use the students data frame from the retention package. We will save this data frame into a SQLite database using the RSQLite package. The R code to setup the database is provided as a demo in the package. Type demo('isql') to start.\nrequire(sqlutils)\nrequire(RSQLite)\nrequire(retention)\ndata(students)\nstudents$CreatedDate = as.character(students$CreatedDate)\nm &lt;- dbDriver(\"SQLite\")\ntmpfile &lt;- tempfile('students.db', fileext='.db')\nconn &lt;- dbConnect(m, dbname=tmpfile)\ndbWriteTable(conn, \"students\", students[!is.na(students$CreatedDate),])\nWe begin an interactive SQL environment with the isql function. The only required parameter is conn which is the connection to the database that SQL statements will be executed. The sql parameter is optional and sets the initial SQL statement for the session that can be edited or executed.\n&gt; hist &lt;- isql(conn=conn, sql=getSQL('StudentSummary'))\nInteractive SQL mode (type quit to exit, help for available commands)...\nSQL&gt;\nhelp\n   Command      Description\n   ___________  ______________________________________________________\n   quit         quit interactive mode\n   help         display this message\n   sql          enter SQL statement\n   edit         edit SQL in a separate text window\n   print        print the last entered SQL statement\n   exec         execute that last entered SQL statement\n   result       prints the last results\n   save [name]  save the last executed query to the global environment\nSLQ&gt;\nprint\nSELECT CreatedDate, count(StudentId) AS count FROM students GROUP BY CreatedDate ORDER BY CreatedDate\nSLQ&gt;\nedit\n\n\n\nSQL Edit Window\n\n\nSLQ&gt;\nprint\nSELECT CreatedDate, count(StudentId) AS count FROM students GROUP BY CreatedDate ORDER BY CreatedDate\nSLQ&gt;\nexec\nExecuting SQL...\n118 rows of 2 variables returned\nSLQ&gt;\nsave\nData frame sql.results saved to global environment\nSLQ&gt;\nquit\nThe isql function returns the history of the session invisibly (that is the results will not be printed but can be assigned to a variable). There are two elements in the returned list, commands is a character vector listing all the commands entered and sql is a character vector containing all the SQL statements entered.\n&gt; names(hist)\n[1] \"sql\"      \"commands\""
  },
  {
    "objectID": "posts/2013-01-24-Comparing_Two_Data_Frames.html",
    "href": "posts/2013-01-24-Comparing_Two_Data_Frames.html",
    "title": "Comparing two data frames with different number of rows",
    "section": "",
    "text": "I posted a question over on StackOverflow on an efficient way of comparing two data frames with the same column structure, but with different rows. What I would like to end up with is an n x m logical matrix where n and m are the number of rows in the first and second data frames, respectively; and the value at the ith row and jth column indicates whether all the values from row i from data frame one is equal to row j from data frame two. To provide some context, this will be used in a propensity score matching algorithm to identify candidate matches that match exactly on any number of covariates. In addition to the approaches I had, joran provided an approach using the Vectorize function (thanks again as I learned another nice function). I decided to put three approaches to a race…\nTo understand what I need, I’ll start with a small example with two data frames, one with 4 rows, the other with 3, and each has two variables, one logical and the other numeric. As an aside, I only need this to work for integers, factors, characters, and logical types therefore avoiding issues of comparing numerics.\n&gt; df1 &lt;- data.frame(row.names=1:4, var1=c(TRUE, TRUE, FALSE, FALSE), var2=c(1,2,3,4))\n&gt; df2 &lt;- data.frame(row.names=5:7, var1=c(FALSE, TRUE, FALSE), var2=c(5,2,3))\n&gt; df1\n   var1 var2\n1  TRUE    1\n2  TRUE    2\n3 FALSE    3\n4 FALSE    4\n&gt; df2\n   var1 var2\n5 FALSE    5\n6  TRUE    2\n7 FALSE    3\nFirst, let’s consider the case when there is only one variable:\n&gt; system.time({\n+   df3 &lt;- sapply(df2$var1, FUN=function(x) { x == df1$var1 })\n+   dimnames(df3) &lt;- list(row.names(df1), row.names(df2))\n+ })\n   user  system elapsed\n      0       0       0\n&gt; df3\n      5     6     7\n1 FALSE  TRUE FALSE\n2 FALSE  TRUE FALSE\n3  TRUE FALSE  TRUE\n4  TRUE FALSE  TRUE\nThis is pretty straight forward. Now I want the same type of result, but to compare more than one column (in the final implementation I need to handle any number of columns so not necessarily limited to one or two).\nThe first approach uses nested apply functions.\n&gt; system.time({\n+   m1 &lt;- t(as.matrix(df1))\n+   m2 &lt;- as.matrix(df2)\n+   df4 &lt;- apply(m2, 1, FUN=function(x) { apply(m1, 2, FUN=function(y) { all(x == y) } ) })\n+ })\n   user  system elapsed\n  0.001   0.000   0.001\n&gt; df4\n      5     6     7\n1 FALSE FALSE FALSE\n2 FALSE  TRUE FALSE\n3 FALSE FALSE  TRUE\n4 FALSE FALSE FALSE\nSecondly, using the Vectorize and outer functions.\n&gt; system.time({\n+   foo &lt;- Vectorize(function(x,y) { all(df1[x,] == df2[y,]) })\n+   df5 &lt;- outer(1:nrow(df1), 1:nrow(df2), FUN=foo)\n+ })\n   user  system elapsed\n  0.005   0.000   0.006\n&gt; df5\n      [,1]  [,2]  [,3]\n[1,] FALSE FALSE FALSE\n[2,] FALSE  TRUE FALSE\n[3,] FALSE FALSE  TRUE\n[4,] FALSE FALSE FALSE\nLastly, we’ll create a new character vector by pasting the other variables together.\n&gt; system.time({\n+   df1$var3 &lt;- apply(df1, 1, paste, collapse='.')\n+   df2$var3 &lt;- apply(df2, 1, paste, collapse='.')\n+   df6 &lt;- sapply(df2$var3, FUN=function(x) { x == df1$var3 })\n+   dimnames(df6) &lt;- list(row.names(df1), row.names(df2))\n+ })\n   user  system elapsed\n  0.000   0.000   0.001\n&gt; df6\n      5     6     7\n1 FALSE FALSE FALSE\n2 FALSE  TRUE FALSE\n3 FALSE FALSE  TRUE\n4 FALSE FALSE FALSE\nWe can already see with this small example that the Vectorize approach is the slowest. However, let’s try a larger example. First we’ll create two data frames, one with 1,000 rows and the second with 1,500. The resulting matrix will be 1,000 x 1,500.\nset.seed(2112)\ndf1 &lt;- data.frame(row.names=1:1000,\n                  var1=sample(c(TRUE,FALSE), 1000, replace=TRUE),\n                  var2=sample(1:10, 1000, replace=TRUE) )\ndf2 &lt;- data.frame(row.names=1001:2500,\n                  var1=sample(c(TRUE,FALSE), 1500, replace=TRUE),\n                  var2=sample(1:10, 1500, replace=TRUE))\nNested apply functions approach:\n&gt; system.time({\n+   m1 &lt;- t(as.matrix(df1))\n+   m2 &lt;- as.matrix(df2)\n+   df4 &lt;- apply(m2, 1, FUN=function(x) { apply(m1, 2, FUN=function(y) { all(x == y) } ) })\n+ })\n   user  system elapsed\n 10.807   0.043  11.096\nVectorize approach:\n&gt; system.time({\n+   foo &lt;- Vectorize(function(x,y) { all(df1[x,] == df2[y,]) })\n+   df5 &lt;- outer(1:nrow(df1), 1:nrow(df2), FUN=foo)\n+ })\n   user  system elapsed\n390.904   0.808 392.134\nCombined columns approach:\n&gt; system.time({\n+   df1$var3 &lt;- apply(df1, 1, paste, collapse='.')\n+   df2$var3 &lt;- apply(df2, 1, paste, collapse='.')\n+   df6 &lt;- sapply(df2$var3, FUN=function(x) { x == df1$var3 })\n+   dimnames(df6) &lt;- list(row.names(df1), row.names(df2))\n+ })\n   user  system elapsed\n  0.421   0.000   0.422\nThe combined column approach is by far the fasted way, and it makes good since. It is a bit surprising (at least to me), how much worse the Vectorize and outer functions are. Moreover, I am a bit concerned about potential issues with the paste method and doing comparisons on those results. Please feel free to leave comments below if there are other approaches."
  },
  {
    "objectID": "posts/2013-03-26-i_Before_e_Except_After_c.html",
    "href": "posts/2013-03-26-i_Before_e_Except_After_c.html",
    "title": "i Before e Except After c",
    "section": "",
    "text": "When I went to school we were always taught the “i before e, except after c” rule for spelling. But how accurate is this rule? Kevin Marks tweeted today the following:\n\n\n»@uberfacts: There are 923 words in the English language that break the “I before E” rule. Only 44 words actually follow that rule.« Science\n\n— Kevin Marks (@kevinmarks) March 25, 2013\n\nNot sure where he came up with that result, but seems simple enough to verify. First, download a English language word list compiled by Kevin Atkinson and available at SourceForge (I will use the Parts of Speech Database, or download my version from Github). I also create a data frame (from the README file) partsOfSpeech that maps the codes to descriptions that we will use later.\nrequire(ggplot2)\nrequire(reshape)\n\npartsOfSpeech &lt;- as.data.frame(matrix(c(\n    'N','Noun',\n    'P','Plural',\n    'h','Noun Phrase',\n    'V','Verb (usu participle)',\n    't','Verb (transitive)',\n    'i','Verb (intransitive)',\n    'A','Adjective',\n    'v','Adverb',\n    'C','Conjunction',\n    'P','Preposition',\n    '!','Interjection',\n    'r','Pronoun',\n    'D','Definite Article',\n    'I','Indefinite Article',\n    'o','Nominative'), ncol=2, byrow=TRUE), stringsAsFactors=FALSE)\nnames(partsOfSpeech) &lt;- c('Code','Description')\n\nwords &lt;- read.table('part-of-speech.txt', sep='\\t', header=FALSE, quote='',\n                    col.names=c('Word','POS'), stringsAsFactors=FALSE)\nnrow(words)\n\n## [1] 295172\nThe parts-of-speech is coded such that the letters before | character come from the original Moby database and letters after the | character come from WordNet. The first character corresponds to the primary classification. The following R code will split this field into two new variables, Moby and WordNet, and then strip the first character from WordNet to create a WordNetPrimary variable. We will use this classification later for plotting purposes.\ntmp &lt;- lapply(words$POS, FUN=function(x) {\n    x &lt;- unlist(strsplit(x, '|', fixed=TRUE) )\n    if(length(x) == 1) return(c(NA, x[[1]]))\n    else if(x[[1]] == '') return(c(NA, x[[2]]))\n    else return(c(x[[1]], x[[2]]))\n})\nwords$Moby &lt;- sapply(tmp, function(x) x[1])\nwords$WordNet &lt;- sapply(tmp, function(x) x[2])\nwords$WordNetPrimary &lt;- substr(words$WordNet, 1, 1)\ntable(words$WordNetPrimary, useNA='ifany')\n\n##\n##      !      A      C      D      h      i      N      p      P      r\n##    260  51914     54     60  71566   2239 119441   8506     99     85\n##      t      v      V   &lt;NA&gt;\n##  12399  13730  12124   2695\nWe use the grep function to get three vectors representing all the “ie”, “ei”, and “cei” words. We also print the number of each type word and the percentage of all words this represents.\nie &lt;- grep('ie', words$Word)\nei &lt;- grep('ei', words$Word)\ncei &lt;- grep('cei', words$Word)\ncie &lt;- grep('cie', words$Word)\n\nlength(ie); length(ie) / nrow(words) * 100\n\n## [1] 10647\n\n## [1] 3.607\n\nlength(ei); length(ei) / nrow(words) * 100\n\n## [1] 3542\n\n## [1] 1.2\n\nlength(cei); length(cei) / nrow(words) * 100\n\n## [1] 202\n\n## [1] 0.06843\n\nlength(cie); length(cie) / nrow(words) * 100\n\n## [1] 654\n\n## [1] 0.2216\nNumber of words that follow the rule, “i before e except after c”\nlength(ie) + length(cei) - length(cie)\n\n## [1] 10195\nNumber of i after e words that are not after c (first way to break the rule).\nlength(ei[!(ei %in% cei)])\n\n## [1] 3340\nNumber of i before e words that are after c (the other way to break the rule).\nlength(cie)\n\n## [1] 654\nPercentage of words that break the rule.\n(length(ei[!(ei %in% cei)]) + length(cie)) / sum(length(ie), length(ei)) * 100\n\n## [1] 28.15\nSo of the 14,189 “ie” and “ei” words, 3,994 break the “i before e, except after c” rule, or about 28.1%.\nLet’s see how this breaks out by part-of-speech.\nthewords &lt;- words[c(ie,ei),]\nthewords$BreakRule &lt;- TRUE\nthewords[which(row.names(thewords) %in% c(cei, ie[!(ie %in% cie)])),]$BreakRule &lt;- FALSE\n\n#Counts\ntab &lt;- as.data.frame(table(thewords$WordNetPrimary, thewords$BreakRule, useNA='ifany'))\ntab &lt;- merge(tab, partsOfSpeech, by.x='Var1', by.y='Code', all.x=TRUE)\n\nggplot(tab, aes(x=Description, y=Freq, fill=Var2)) +\n    geom_bar(stat='identity', position='dodge') +\n    ylab('Number of Words') + xlab('Part of Speech') +\n    scale_fill_hue('Break the Rule') +\n    ggtitle('i Before e, Except After c') + coord_flip()\n\n\n\nplot of chunk IbeforeE\n\n\n#Percentages\ntab2 &lt;- as.data.frame(prop.table(table(thewords$WordNetPrimary,\n                    thewords$BreakRule, useNA='ifany'), 1) * 100)\ntab2 &lt;- merge(tab2, partsOfSpeech, by.x='Var1', by.y='Code', all.x=TRUE)\nggplot(tab2, aes(x=Description, y=Freq, fill=Var2)) +\n    geom_bar(stat='identity', position='dodge') +\n    ylab('Percentage of Words by Part of Speech') + xlab('Part of Speech') +\n    scale_fill_hue('Break the Rule') +\n    ggtitle('i Before e, Except After c') + coord_flip()\n\n\n\nplot of chunk IbeforeE\n\n\nA few last details. Here is the proportional table of words that break the rule by part-of-speech. Lastly, the definite article and pronoun words (three of each) that all break the rule.\ncast(tab2, Description ~ Var2, mean, value='Freq')\n\n##              Description FALSE   TRUE\n## 1              Adjective 83.17  16.83\n## 2                 Adverb 66.56  33.44\n## 3            Conjunction 25.00  75.00\n## 4       Definite Article  0.00 100.00\n## 5           Interjection 55.56  44.44\n## 6                   Noun 65.84  34.16\n## 7            Noun Phrase 63.70  36.30\n## 8                Pronoun  0.00 100.00\n## 9    Verb (intransitive) 54.55  45.45\n## 10     Verb (transitive) 49.42  50.58\n## 11 Verb (usu participle) 65.45  34.55\n## 12                  &lt;NA&gt; 67.26  32.74\n\nthewords[which(thewords$WordNetPrimary == 'D'), ]\n\n##           Word POS Moby WordNet WordNetPrimary BreakRule\n## 113927  either DCv &lt;NA&gt;     DCv              D      TRUE\n## 182679 neither DCv &lt;NA&gt;     DCv              D      TRUE\n## 262111   their   D &lt;NA&gt;       D              D      TRUE\n\nthewords[which(thewords$WordNetPrimary == 'r'), ]\n\n##               Word POS Moby WordNet WordNetPrimary BreakRule\n## 262112      theirs   r &lt;NA&gt;       r              r      TRUE\n## 262113   theirself   r &lt;NA&gt;       r              r      TRUE\n## 262114 theirselves  rp &lt;NA&gt;      rp              r      TRUE\n\nPart II - Using only the 5,000 Most Frequently Used Words\nHere is an update using the list of 5,000 most commonly used words from http://www.wordfrequency.info/top5000.asp (note there really are only 4,354 unique words since the same word can be used in different parts-of-speech). Of the 4,354 unique words, 96, or about 2.2%, have an “ie” or “ei” in the word. Of those 96 words, 31, or 32.3% break the “i before e except after c” rule.\nwords &lt;- read.csv('MostUsedWords.csv')\ndups &lt;- words[words$Word %in% words[duplicated(words$Word),]$Word,]\nhead(dups[order(dups$Word),])\n\n##      Rank  Word Part.of.speech Frequency Dispersion\n## 47     46 about              i    874406       0.96\n## 180   179 about              r    208550       0.97\n## 897   896 above              i     44130       0.95\n## 1604 1599 above              r     23866       0.92\n## 1553 1548 abuse              n     24534       0.93\n## 3783 3778 abuse              v      7554       0.94\n\nlength(unique(words$Word))\n\n## [1] 4354\n\nwords &lt;- words[!duplicated(words$Word),]\n\nie &lt;- grep('ie', words$Word)\nei &lt;- grep('ei', words$Word)\ncei &lt;- grep('cei', words$Word)\ncie &lt;- grep('cie', words$Word)\n\n#Percentage of words that break the rule.\n(length(ei[!(ei %in% cei)]) + length(cie)) / sum(length(ie), length(ei)) * 100\n\n## [1] 32.29\n\n\nPart III - Weighted by Frequency of Words\nUsing the same list as part II above, let’s consider the word frequency. That is, we’ll weight each word by it’s frequency according to WordFrequency.info. Using this approach, 47% of “ie” words break the rule. Put another way, for each “ie” word you encounter reading, there is a 47% chance it does not follow the “i before e, except after c” rule.\nwords &lt;- read.csv('MostUsedWords.csv')\nie &lt;- grep('ie', words$Word)\nei &lt;- grep('ei', words$Word)\ncei &lt;- grep('cei', words$Word)\ncie &lt;- grep('cie', words$Word)\n(sum(words[ei[!(ei %in% cei)],'Frequency']) + sum(words[cie,'Frequency'])) /\n    sum(words[ie,'Frequency'], words[ei,'Frequency']) * 100\n\n## [1] 46.81"
  },
  {
    "objectID": "posts/2013-05-08-Gamblers_Run_With_Shiny.html",
    "href": "posts/2013-05-08-Gamblers_Run_With_Shiny.html",
    "title": "Gambler’s Run with Shiny",
    "section": "",
    "text": "I finally had an opportunity to play with Shiny, and I am very impressed. I have created a Github Project so head over there for the source code. There are a number of ways to distribute Shiny apps. If you are running R (and mostly likely you are if you are reading this), you can download and run Shiny apps using the runApp (if already downloaded), runGitHub, runGist, or runUrl functions. RStudio also make the Shiny Server available and you can also request an account on their servers. Also be sure to check out the excellent tutorial on Shiny.\nFirst, install shiny and shinyIncubator (for the ActionButton) packages, preferably the development versions.\nrequire(devtools)\ninstall_github('shiny', 'rstudio')\ninstall_github('shiny-incubator', 'rstudio')\nrequire(shiny)\n\nGambler’s Run\nThis simple app that lets you simulate a sequence of random events, for example coin flips, and plot the cummulative sum. This app allows you choose the odds of winning, the number of games to simulate, and the number of simulations to display simultaneously.\n\n\n\nGambler Shiny App\n\n\nTo run the app locally:\nshiny::runGitHub('ShinyApps', 'jbryer', subdir='gambler')\nOr from the RStudio server (note that RStudio does not guarantee the server will always be up so this link may or may not work).\n\n\nLottery Tickets\nSimilar to the gambler app, this simulates buying a series of lottery tickets with varying odds of winning different amounts. Each previous run is saved and plotted in light grey to show how the current run compares to past runs.\n\n\n\nLottery Tickets Shiny App\n\n\nTo run the app locally:\nshiny::runGitHub('ShinyApps', 'jbryer', subdir='lottery')\nOr from the RStudio server (note that RStudio does not guarantee the server will always be up so this link may or may not work).\nJust to try out all the ways to distribute Shiny apps, I also created a Gist for this app.\nshiny::runGist(\"5525690\")"
  },
  {
    "objectID": "posts/2013-11-26-Bootstrapping_for_Propensity_Score_Analysis.html",
    "href": "posts/2013-11-26-Bootstrapping_for_Propensity_Score_Analysis.html",
    "title": "Bootstrapping for Propensity Score Analysis",
    "section": "",
    "text": "I am happy to announce that version 1.0 of the PSAboot package has been released to CRAN. This package implements bootstrapping for propensity score analysis. This deviates from typical implementations such as boot in that it allows for separate sampling specifications for treatment and control units. For example, in the case where the ratio of treatment-to-control units is large, one can bootstrap only the control units while always using all available treatment units. Additionally, this package will estimate treatment effects using multiple methods for each bootstrap sample. In addition to adhering to Rosenbaum’s (2012) advise of “Testing One Hypothesis Twice in Observational Studies”, we can compare the performance of different methods across many samples. Lastly, a set of functions to estimate and visualize balance across bootstrap samples and methods are provided.\nYou can get more details on the project page and the vignette. The project is hosted on Github project page. Download the latest version or submit bugs there.\nThis package supports stratification using ctree (from the party package), rpart, and quintiles (using fitted values from logistic regression) and well as matching using the MatchIt and Matching packages. The project page outlines how to write custom methods.\nThe following example uses the tutoring dataset in the TriMatch package. This study examined the effects of tutoring on student grades in writing courses. The treatment group was defined a students who used tutoring services during their course. The control group are students in a course section with at least one student who used the tutoring services. The PSAboot performs the bootstrap analysis and returns an object of class PSAboot. The summary, plot, hist, boxplot, and matrixplot S3 methods are implemented.\nrequire(PSAboot)\n\n#  Loading required package: PSAboot\n#  Loading required package: PSAgraphics\n#  Loading required package: rpart\n\ndata(tutoring, package='TriMatch')\ntutoring$treatbool &lt;- tutoring$treat != 'Control'\ncovs &lt;- tutoring[,c('Gender', 'Ethnicity', 'Military', 'ESL', 'EdMother', 'EdFather',\n                    'Age', 'Employment', 'Income', 'Transfer', 'GPA')]\ntable(tutoring$treatbool)\n\n#\n#  FALSE  TRUE\n#    918   224\n\ntutoring.boot &lt;- PSAboot(Tr=tutoring$treatbool,\n                         Y=tutoring$Grade,\n                         X=covs,\n                         seed=2112)\n\n#  100 bootstrap samples using 5 methods.\n#  Bootstrap sample sizes:\n#     Treated=224 (100%) with replacement.\n#     Control=918 (100%) with replacement.\nThe summary function provides numeric results for each method including the overall estimate and confidence interval using the complete sample as well as the pooled estimates and confidence intervals with percentages of the number of confidence intervals that do not span zero.\nsummary(tutoring.boot)\n\n#  Stratification Results:\n#     Complete estimate = 0.482\n#     Complete CI = [0.3, 0.665]\n#     Bootstrap pooled estimate = 0.476\n#     Bootstrap pooled CI = [0.332, 0.62]\n#     100% of bootstrap samples have confidence intervals that do not span zero.\n#        100% positive.\n#        0% negative.\n#  ctree Results:\n#     Complete estimate = 0.458\n#     Complete CI = [0.177, 0.739]\n#     Bootstrap pooled estimate = 0.482\n#     Bootstrap pooled CI = [0.294, 0.67]\n#     99% of bootstrap samples have confidence intervals that do not span zero.\n#        99% positive.\n#        0% negative.\n#  rpart Results:\n#     Complete estimate = 0.475\n#     Complete CI = [0.165, 0.784]\n#     Bootstrap pooled estimate = 0.45\n#     Bootstrap pooled CI = [0.212, 0.689]\n#     84% of bootstrap samples have confidence intervals that do not span zero.\n#        84% positive.\n#        0% negative.\n#  Matching Results:\n#     Complete estimate = 0.479\n#     Complete CI = [0.388, 0.571]\n#     Bootstrap pooled estimate = 0.471\n#     Bootstrap pooled CI = [0.231, 0.711]\n#     100% of bootstrap samples have confidence intervals that do not span zero.\n#        100% positive.\n#        0% negative.\n#  MatchIt Results:\n#     Complete estimate = 0.5\n#     Complete CI = [0.253, 0.747]\n#     Bootstrap pooled estimate = 0.513\n#     Bootstrap pooled CI = [0.293, 0.734]\n#     100% of bootstrap samples have confidence intervals that do not span zero.\n#        100% positive.\n#        0% negative.\nThe plot function plots the estimate (mean difference) for each bootstrap sample. The default is to sort from largest to smallest estimate for each method separately. That is, rows do not correspond across methods. The sort parameter can be set to none for no sorting or the name of any method to sort only based upon the results of that method. In these cases the rows then correspond to matching bootstrap samples. The blue points correspond to the the estimate for each bootstrap sample and the horizontal line to the confidence interval. Confidence intervals that do not span zero are colored red. The vertical blue line and green lines correspond to the overall pooled estimate and confidence for each method, respectively.\nplot(tutoring.boot)\n\n\n\nplot of chunk tutoringplot\n\n\nThe hist function plots a histogram of the estimates across all bootstrap samples for each method.\nhist(tutoring.boot)\n\n#  stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.\n#  stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.\n#  stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.\n#  stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.\n#  stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.\n\n\n\nplot of chunk tutoringhist\n\n\nThe boxplot function depicts the distribution of estimates for each method along with confidence intervals in green. Additionally, the overall pooled estimate and confidence interval across all bootstrap samples and methods are represented by the vertical blue and green lines, respectively.\nboxplot(tutoring.boot)\n\n#  Loading required package: ggthemes\n\n\n\nplot of chunk tutoringboxplot\n\n\nThe matrixplot summarizes the estimates across methods for each bootstrap sample. The lower half of the matrix are scatter plots where each point represents the one bootstrap sample. The red line is a Loess regression line. The main diagonal depicts the distribution of effects and the upper half provides the correlation of estimates.\nmatrixplot(tutoring.boot)\n\n\n\nplot of chunk tutoringmatrixplot\n\n\nThe balance function will provide balance statistics. The print, plot, and boxplot S3 methods are implemented.\ntutoring.balance &lt;- balance(tutoring.boot)\ntutoring.balance\n\n#  Unadjusted balance: 0.117875835338968\n#                 Complete Bootstrap\n#  Stratification  0.02923   0.03795\n#  ctree           0.04385   0.06913\n#  rpart           0.07846   0.08698\n#  Matching        0.04522   0.06668\n#  MatchIt         0.05078   0.05790\n\n\nplot(tutoring.balance)\n\n\n\nplot of chunk tutoringbalanceplot\n\n\nboxplot(tutoring.balance)\n\n\n\nplot of chunk tutoringbalanceboxplot"
  },
  {
    "objectID": "posts/2014-03-20-Albany_R_Users_Group.html",
    "href": "posts/2014-03-20-Albany_R_Users_Group.html",
    "title": "Albany, NY R Users Group",
    "section": "",
    "text": "I have started an R Users Group for the Albany, NY area. Hopefully we get enough interest that we can host a meeting in the next couple of months. Please feel free to share with your colleagues and friends.\nwww.meetup.com/Albany-R-Users-Group\nFeel free to email me or leave comment on this page or on the Meetup page if you are interested in giving a talk or hosting some future meeting."
  },
  {
    "objectID": "posts/2014-06-05-str_Implementation_for_Data_Frames.html",
    "href": "posts/2014-06-05-str_Implementation_for_Data_Frames.html",
    "title": "str Implementation for Data Frames",
    "section": "",
    "text": "The str function is perhaps the most useful function in R. It provides great information about the structure of some object. When I teach R, especially for those coming from SPSS, the str function for data frames provides the information they are use to seeing on the variable view tab. However, sometimes I want to display the information str returns in a better format (e.g. as an HTML or LaTeX table). I wrote a function, strtable that provides the information str.data.frame does but returns the results as a data.frame. This provides much more flexibility for controlling how the output is formatted. Specifically, it will return a data.frame with four columns: variable, class, levels, and examples.\nThe function can be sourced from Gist using the devtools package.\ndevtools::source_gist('4a0a5ab9fe7e1cf3be0e')\nFor the first example, we’ll use the iris data frame.\ndata(iris)\nstr(iris)\n## 'data.frame':    150 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\nThe strtable has five parameters:\n\nn the first n element to show\nwidth maximum width in characters for the examples to show\nn.levels the first n levels of a factor to show.\nwidth.levels maximum width in characters for the number of levels to show.\nfactor.values function defining how factor examples should be printed. Possible values are as.character or as.integer.\n\nprint(strtable(iris), na.print='')\n##      variable              class                              levels\n##  Sepal.Length            numeric\n##   Sepal.Width            numeric\n##  Petal.Length            numeric\n##   Petal.Width            numeric\n##       Species Factor w/ 3 levels \"setosa\", \"versicolor\", \"virginica\"\n##                                     examples\n##                      5.1, 4.9, 4.7, 4.6, ...\n##                        3.5, 3, 3.2, 3.1, ...\n##                      1.4, 1.4, 1.3, 1.5, ...\n##                      0.2, 0.2, 0.2, 0.2, ...\n##  \"setosa\", \"setosa\", \"setosa\", \"setosa\", ...\nprint(strtable(iris, factor.values=as.integer), na.print='')\n##      variable              class                              levels\n##  Sepal.Length            numeric\n##   Sepal.Width            numeric\n##  Petal.Length            numeric\n##   Petal.Width            numeric\n##       Species Factor w/ 3 levels \"setosa\", \"versicolor\", \"virginica\"\n##                 examples\n##  5.1, 4.9, 4.7, 4.6, ...\n##    3.5, 3, 3.2, 3.1, ...\n##  1.4, 1.4, 1.3, 1.5, ...\n##  0.2, 0.2, 0.2, 0.2, ...\n##          1, 1, 1, 1, ...\nHere’s a second example using the diamonds data from the ggplot2 package.\ndata(diamonds)\nstr(diamonds)\n## 'data.frame':    53940 obs. of  10 variables:\n##  $ carat  : num  0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n##  $ cut    : Ord.factor w/ 5 levels \"Fair\"&lt;\"Good\"&lt;..: 5 4 2 4 2 3 3 3 1 3 ...\n##  $ color  : Ord.factor w/ 7 levels \"D\"&lt;\"E\"&lt;\"F\"&lt;\"G\"&lt;..: 2 2 2 6 7 7 6 5 2 5 ...\n##  $ clarity: Ord.factor w/ 8 levels \"I1\"&lt;\"SI2\"&lt;\"SI1\"&lt;..: 2 3 5 4 2 6 7 3 4 5 ...\n##  $ depth  : num  61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n##  $ table  : num  55 61 65 58 58 57 57 55 61 61 ...\n##  $ price  : int  326 326 327 334 335 336 336 337 337 338 ...\n##  $ x      : num  3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n##  $ y      : num  3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n##  $ z      : num  2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\nprint(strtable(diamonds), na.print='')\n##  variable              class                                      levels\n##     carat            numeric\n##       cut Factor w/ 5 levels \"Fair\", \"Good\", \"Very Good\", \"Premium\", ...\n##     color Factor w/ 7 levels                     \"D\", \"E\", \"F\", \"G\", ...\n##   clarity Factor w/ 8 levels              \"I1\", \"SI2\", \"SI1\", \"VS2\", ...\n##     depth            numeric\n##     table            numeric\n##     price            integer\n##         x            numeric\n##         y            numeric\n##         z            numeric\n##                                    examples\n##                 0.23, 0.21, 0.23, 0.29, ...\n##  \"Ideal\", \"Premium\", \"Good\", \"Premium\", ...\n##                     \"E\", \"E\", \"E\", \"I\", ...\n##             \"SI2\", \"SI1\", \"VS1\", \"VS2\", ...\n##                 61.5, 59.8, 56.9, 62.4, ...\n##                         55, 61, 65, 58, ...\n##                     326, 326, 327, 334, ...\n##                  3.95, 3.89, 4.05, 4.2, ...\n##                 3.98, 3.84, 4.07, 4.23, ...\n##                 2.43, 2.31, 2.31, 2.63, ...\nprint(strtable(diamonds, factor.values=as.integer), na.print='')\n##  variable              class                                      levels\n##     carat            numeric\n##       cut Factor w/ 5 levels \"Fair\", \"Good\", \"Very Good\", \"Premium\", ...\n##     color Factor w/ 7 levels                     \"D\", \"E\", \"F\", \"G\", ...\n##   clarity Factor w/ 8 levels              \"I1\", \"SI2\", \"SI1\", \"VS2\", ...\n##     depth            numeric\n##     table            numeric\n##     price            integer\n##         x            numeric\n##         y            numeric\n##         z            numeric\n##                     examples\n##  0.23, 0.21, 0.23, 0.29, ...\n##              5, 4, 2, 4, ...\n##              2, 2, 2, 6, ...\n##              2, 3, 5, 4, ...\n##  61.5, 59.8, 56.9, 62.4, ...\n##          55, 61, 65, 58, ...\n##      326, 326, 327, 334, ...\n##   3.95, 3.89, 4.05, 4.2, ...\n##  3.98, 3.84, 4.07, 4.23, ...\n##  2.43, 2.31, 2.31, 2.63, ...\nHere’s the source code from Gist:"
  },
  {
    "objectID": "posts/2014-07-04-Women_Graduates_in_Math_Stats_CIS.html",
    "href": "posts/2014-07-04-Women_Graduates_in_Math_Stats_CIS.html",
    "title": "Women Graduates in Math, Statistics, and Computer Information Systems",
    "section": "",
    "text": "One of the more interesting talks at this year’s useR! Conference was the heR Panel discussing the role of women in the R community. They estimate that fewer than 15% of package authors are women. One of the points brought up was that this is less than the percentage of women in statistics. Perhaps this is more related to the computer science aspect of R that that of statistics. By way of comparison, the United States Department of Labor estimates there are between 7.5% (computer network architects) and 39.5% (web developers) of computer related fields. This as compared to women holding 47% of all occupations (source).\n\n(Stephanie Kovalchik, https://github.com/skoval/her2014/blob/master/representation.R)\nHere, I am going to provide another data point to help think about this issue. Specifically, what percentage of math, statistics, and computer information systems baccalaureate degrees do women earn. Using the ipeds package I wrote a while back to get data from the Integrated Postsecondary Education Data System (IPEDS), we can quickly get data for the last 13 years of degrees awarded.\nFirst, this histogram depicts the total number of Baccalaureate Degrees awarded in CIS, Math, and Statistics. Interestingly we see there is a steady increase in math and statistics degrees, whereas there was quite a dip in CIS degrees in the mid 2000s (perhaps due to the dot com bubble burst?).\n\n\n\nNumber of Baccalaureate Degrees Awarded by Year\n\n\nThe following figure shows the percentage of those Baccalaureate Degrees awarded to women. For comparison, I have included a line showing the total percentage of Baccalaureate Degrees awarded to women. The bad news, there is still a ways to go to shrink the gender gap. Math and statistics is doing better, but not as bad as CIS degrees. The worse news, it appears there is a downward trend in the percentage of Baccalaureate Degrees awarded to women in CIS.\n\n\n\nPercent of Female Percent of Female Baccalaureate Degrees Awarded’, ’by Year for CIS, Math, and Statistics Majors Degrees Awarded by Year for CIS, Math, and Statistics Majors\n\n\nThe source code is on Gist:"
  },
  {
    "objectID": "posts/2016-02-21-Bayes_Billiards_Shiny.html",
    "href": "posts/2016-02-21-Bayes_Billiards_Shiny.html",
    "title": "Shiny App for Bayes Billiards Problem",
    "section": "",
    "text": "Consider a pool table of length one. An 8-ball is thrown such that the likelihood of its stopping point is uniform across the entire table (i.e. the table is perfectly level). The location of the 8-ball is recorded, but not known to the observer. Subsequent balls are thrown one at a time and all that is reported is whether the ball stopped to the left or right of the 8-ball. Given only this information, what is the position of the 8-ball? How does the estimate change as more balls are thrown and recorded?\n\nYou can run the app from RStudio’s shinyapps.io service at jbryer.shinyapps.io/BayesBilliards.\nThe Shiny App is included in the DATA606 package on Github and can be run, once installed, using the DATA606::shiny_demo('BayesBilliards') function.\nOr, run the app directly from Github using the shiny::runGitHub('DATA606', 'jbryer', subdir='inst/shiny/BayesBilliards') function.\nSource code is located here: https://github.com/jbryer/DATA606/tree/master/inst/shiny/BayesBilliards"
  },
  {
    "objectID": "posts/2018-22-26-DTedit.html",
    "href": "posts/2018-22-26-DTedit.html",
    "title": "Editable DataTables for Shiny Applications",
    "section": "",
    "text": "RStudio recently updated Shiny to allow for editable DataTables. Their implementations allows for editing cells direclty with in the DataTable view. This is fine for many advanced applications, however, for many applications more fine tuned control of what the user can edit is necessary. For example, you may want to only allow a subset of columns to be editable. Or you want to view a subset of columns in a spreadsheet view but allow other columns to be editable. The DTedit package takes the editing out of the table view and instead presents the user with a modal dialog for editing table contents (see screenshot below).\nTo get started, use the devtools package to install the latest development version of DTedit:\nThe dtedit_demo will run a sample shiny app with to editable data tables."
  },
  {
    "objectID": "posts/2018-22-26-DTedit.html#getting-started-with-dtedit",
    "href": "posts/2018-22-26-DTedit.html#getting-started-with-dtedit",
    "title": "Editable DataTables for Shiny Applications",
    "section": "Getting Started with DTedit",
    "text": "Getting Started with DTedit\nYou can download a simple shiny app using DTedit from Github.\nThere are three steps to using DTedit in your shiny application.\n\n1. Define callback function for inserting, updating, and deleting data.\nNOTE: These callback functions assume that mydata is already defined somewhere outside the callback functions. See the template for the complete example using data.frames, or this demo for an example using RSQLite.\nmy.insert.callback &lt;- function(data, row) {\n    mydata &lt;- rbind(data, mydata)\n    return(mydata)\n}\n\nmy.update.callback &lt;- function(data, olddata, row) {\n    mydata[row,] &lt;- data[1,]\n    return(mydata)\n}\n\nmy.delete.callback &lt;- function(data, row) {\n    mydata &lt;- mydata[-row,]\n    return(mydata)\n}\nTypically these functions would interact with a database. As written here, the data would be lost between shiny sessions.\n\n\n2. Create the dtedit object within your server function.\nDTedit::dtedit(input, output,\n       name = 'mycontacts',\n       thedata = mydata,\n       edit.cols = c('name', 'email', 'useR', 'notes'),\n       edit.label.cols = c('Name', 'Email Address', 'Are they an R user?', 'Additional notes'),\n       input.types = c(notes='textAreaInput'),\n       view.cols = c('name', 'email', 'useR'),\n       callback.update = my.update.callback,\n       callback.insert = my.insert.callback,\n       callback.delete = my.delete.callback)\nThe input and output are passed from the server function. The name parameter will define the name of the object available to the uiOutput. The thedata is a data.frame for the initial view of the data table. This can be an empty (i.e. no rows) data.frame. The structure of the data.frame will define the inputs (e.g. factors will be drop down, Date will use dateInput, numerics will use numericInput, etc.). There are many other parameters to custom the behavior of dtedit, see ?dtedit for the full list.\n\n\n3. Use uiOutput in your UI to display the editable data table.\nThe name you will pass to uiOutput is the name you passed to the dtedit created on the server side.\nuiOutput('mycontacts')"
  },
  {
    "objectID": "projects/DTedit.html",
    "href": "projects/DTedit.html",
    "title": "DTedit",
    "section": "",
    "text": "Github: https://github.com/jbryer/DTedit"
  },
  {
    "objectID": "projects/FutureMapping.html",
    "href": "projects/FutureMapping.html",
    "title": "FutureMapping",
    "section": "",
    "text": "FutureMapping: https://futuremapping.org\nAmplify Dashboard: https://amplifydata.org Amplify Website: https://amplifyapp.org/\nAmplify was made by and for youth. Take our surveys, earn points and enter raffles to win big. Your ideas, along with others, will help shape policy and spark action."
  },
  {
    "objectID": "projects/PSAboot.html",
    "href": "projects/PSAboot.html",
    "title": "PSAboot",
    "section": "",
    "text": "Github: https://github.com/jbryer/PSAboot"
  },
  {
    "objectID": "projects/ShinyQDA.html",
    "href": "projects/ShinyQDA.html",
    "title": "ShinyQDA",
    "section": "",
    "text": "Github: https://github.com/jbryer/ShinyQDA"
  },
  {
    "objectID": "projects/VisualStats.html",
    "href": "projects/VisualStats.html",
    "title": "VisualStats",
    "section": "",
    "text": "Github: github.com/jbryer/VisualStats\nWebsite: visualstats.bryer.org"
  },
  {
    "objectID": "projects/clav.html",
    "href": "projects/clav.html",
    "title": "clav",
    "section": "",
    "text": "Github: https://github.com/jbryer/clav\nThe clav package provides utilities for conducting cluster (profile) analysis with an emphasis on the validating the stability of the profiles both within a given data set as well as across data sets. Unlike supervised models where the known class is measured, validation of unsupervised models where there is no known class can be challenging. The approach implemented here attempts to compare the cluster results across many random samples."
  },
  {
    "objectID": "projects/likert.html",
    "href": "projects/likert.html",
    "title": "likert",
    "section": "",
    "text": "Github: https://github.com/jbryer/likert"
  },
  {
    "objectID": "projects/medley.html",
    "href": "projects/medley.html",
    "title": "medley",
    "section": "",
    "text": "Github: https://github.com/jbryer/medley\nMost predictive modeling strategies require there to be no missing data for model estimation. When there is missing data, there are generally two strategies for working with missing data: 1.) exclude the variables (columns) or observations (rows) where there is missing data; or 2.) impute the missing data. However, data is often missing in systematic ways. Excluding data from training is ignoring potentially predictive information and for many imputation procedures the missing completely at random (MCAR) assumption is violated.\nThe medley package implements a solution to modeling when there are systematic patterns of missingness. A working example of predicting student retention from a larger study of the Diagnostic Assessment and Achievement of College Skills (DAACS) will be explored. In this study, demographic data was collected at enrollment from all students and then students completed diagnostic assessments in self-regulated learning (SRL), writing, mathematics, and reading during their first few weeks of the semester. Although all students were expected to complete DAACS, there were no consequence and therefore a large percentage of student completed none or only some of the assessments. The resulting dataset has three predominate response patterns: 1.) students who completed all four assessments, 2.) students who completed only the SRL assessment, and 3). students who did not complete any of the assessments.\nThe goal of the medley algorithm is to take advantage of missing data patterns. For this example, the medley algorithm trained three predictive models: 1.) demographics plus all four assessments, 2.) demographics plus SRL assessment, and 3.) demographics only. For both training and prediction, the model used for each student is based upon what data is available. That is, if a student only completed SRL, model 2 would be used. The medley algorithm can be used with most statistical models. For this study, both logistic regression and random forest are used. The accuracy of the medley algorithm was 3.5% better than using only the complete data and 3.1% better than using a dataset where missing data was imputed using the mice package.\nThe medley package provides an approach for predictive modeling using the same training and prediction framework R users are accustomed to using. There are numerous parameters that can be modified including what underlying statistical models are used for training. Additional diagnostic functions are available to explore missing data patterns."
  },
  {
    "objectID": "projects/multilevelPSA.html",
    "href": "projects/multilevelPSA.html",
    "title": "multilevelPSA",
    "section": "",
    "text": "Resources\n\nGithub: https://github.com/jbryer/multilevelPSA\nThis package was developed for my dissertation, A National Study Comparing Charter and Traditional Public Schools Using Propensity Score Analysis. Analysis scripts and manuscript are available at that Github repo.\nApplied Propensity Score Analysis with R book and R package.\n\nAbstract\nAs can be seen from the recent Special Issue of MBR on propensity score analysis (PSA) methods, the use of PSA has gained increasing popularity for estimating causal effects in observational studies. However, PSA use with multilevel or clustered data has been limited, and to date there seems to have been no development of specialized graphics for such data. This paper introduces the multilevelPSA (http://multilevelPSA.r-forge.r-project.org) package for R that provides cluster-based functions for estimating propensity scores as well as graphics to exhibit results for multilevel data. This work extends to the multilevel case the framework for visualizing propensity score analysis introduced by Helmreich and Pruzek (2009). International data from the Programme for International Student Assessment (Organization for Economic Co-operation and Development, 2009) are comprehensively examined to compare private with public schools on reading, mathematics, and science outcomes after adjusting for covariate differences in the multilevel context.\nParticularly for analyses of large data sets, focusing on statistical significance is limiting. As can readily be seen, overall results favor “private” over “public” schools, at least for end of secondary school math achievement. But the graphics provide a more nuanced understanding of the nature and magnitude of adjusted differences for countries. Furthermore, the graphics are readily interpreted by a nontechnical audience. Broadly speaking, it is seen that modern graphics can enhance and extend conventional numerical summaries by focusing on details of what data have to say for multilevel comparisons of many countries based on propensity score methods."
  },
  {
    "objectID": "projects/pisa.html",
    "href": "projects/pisa.html",
    "title": "pisa",
    "section": "",
    "text": "Github: https://github.com/jbryer/pisa"
  },
  {
    "objectID": "projects/qualtrics.html",
    "href": "projects/qualtrics.html",
    "title": "qualtrics",
    "section": "",
    "text": "Github: https://github.com/jbryer/qualtrics"
  },
  {
    "objectID": "projects/sqlutils.html",
    "href": "projects/sqlutils.html",
    "title": "sqlutils",
    "section": "",
    "text": "Github: http://github.com/jbryer/sqlutils"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nShinyQDA: R Package and Shiny Application for the Analysis of Qualitative Data\n\n\nI will be giving a talk at ShinyConf 2025 on a Shiny application desiged for doing qualitative data analysis.\n\n\n\nJason Bryer\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a portfolio with Github and Quarto\n\n\nSlides for a talk on how to build a portfolio website using Github\n\n\n\nJason Bryer\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlogin: User Authentication for Shiny Applications\n\n\n\n\n\n\nJason Bryer\n\n\nApr 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Visual Introduction to Propensity Score Analysis\n\n\nRecording of A Visual Introduction to Propensity Score Analysis given for the nyhackr meetup group.\n\n\n\nJason Bryer\n\n\nNov 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating Causality from Observational Data\n\n\nThe CUNY School of Professional Studies, Data Science and Information Systems department, is hosting a talk by Jason Bryer titled Estimating Causality from Observation Data.…\n\n\n\nJason Bryer\n\n\nApr 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Package Development\n\n\nRecording for an introduction to R package development.\n\n\n\nJason Bryer\n\n\nMar 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Shiny\n\n\nRecording for an introduction to Shiny application development.\n\n\n\nJason Bryer\n\n\nNov 30, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-11-30-Intro_to_Shiny.html",
    "href": "posts/2021-11-30-Intro_to_Shiny.html",
    "title": "Introduction to Shiny",
    "section": "",
    "text": "Shiny is an R package designed to create web based applications using the R programming language. This talk will introduce the basic framework for creating interactive Shiny applications including user inputs, dynamic plots and tables, reactive programming, and creating data dashboards.\nSlides can be viewed here: https://r.bryer.org/shiny/Intro_to_Shiny/\nSource code and resources are on Github here: https://github.com/jbryer/AlbanyRUsers/tree/main/meetups/2021-11-30-Intro_to_Shiny"
  },
  {
    "objectID": "posts/2023-04-24-Estimating_Causailty_from_Observational_Data.html",
    "href": "posts/2023-04-24-Estimating_Causailty_from_Observational_Data.html",
    "title": "Estimating Causality from Observational Data",
    "section": "",
    "text": "Download slides\nThe use of propensity score methods (Rosenbaum & Rubin, 1983) for estimating causal effects in observational studies or certain kinds of quasi-experiments has been increasing in the social sciences (Thoemmes & Kim, 2011) and in medical research (Austin, 2008) in the last decade. Propensity score analysis (PSA) attempts to adjust selection bias that occurs due to the lack of randomization. Analysis is typically conducted in three phases where in phase I, the probability of placement in the treatment is estimated to identify matched pairs or clusters so that in phase II, comparisons on the dependent variable can be made between matched pairs or within clusters, and phase III, robustness to unobserved covariates is estimated. R (R Core Team, 2023) is ideal for conducting PSA given its wide availability of the most current statistical methods vis-à-vis add-on packages as well as its superior graphics capabilities.\nThis talk will provide participants with a theoretical overview of propensity score methods as well as illustrations and discussion of PSA applications. Methods used in phase I of PSA (i.e. models or methods for estimating propensity scores) include logistic regression, classification trees, and matching. Discussions on appropriate comparisons and estimations of effect size and confidence intervals in phase II will also be covered. The use of graphics for diagnosing covariate balance as well as summarizing overall results will be emphasized."
  },
  {
    "objectID": "posts/2025-02-19-Github_Portfolio.html",
    "href": "posts/2025-02-19-Github_Portfolio.html",
    "title": "Building a portfolio with Github and Quarto",
    "section": "",
    "text": "The slides for the talk given for the CUNY SPS Data Science and Information Systems department are below. The example website can be viewed here and the repository containing the code to generate the website is here.\n\n    View slides in full screen\n       \n      \n    \n  \nNOTE: I am using the a Quarto extension to add the revealjs shortcode. The package documentation is here: https://github.com/coatless-quarto/embedio To install the extension run the following command in the console:\nquarto add coatless-quarto/embedio"
  },
  {
    "objectID": "posts/2012-11-27-FIfty_Shades_of_Grey.html",
    "href": "posts/2012-11-27-FIfty_Shades_of_Grey.html",
    "title": "Fifty Shades of Grey in R",
    "section": "",
    "text": "My wife went out to her book group tonight and their book of the month was 50 Shades of Grey. Sadly, I could think of is that plotting 50 shades in R would be a neat exercise.\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\ngrey50 &lt;- data.frame(\n    x = rep(1:10, 5),\n    y = rep(1:5, each=10),\n    c = unlist(lapply(seq(10,255,5), FUN=function(x) { rgb(x,x,x, max=255) })),\n    t = unlist(lapply(seq(10,255,5), FUN=function(x) { ifelse(x &gt; 255/2, 'black', 'white') }))\n)\nggplot(grey50, aes(x=x, y=y, fill=c, label=c, color=t)) +\n    geom_tile() + \n    geom_text(size=3) +\n    scale_fill_identity() + scale_color_identity() + ylab(NULL) + xlab(NULL) +\n    theme_void()"
  },
  {
    "objectID": "posts/2024-04-17-ShinyConf2024.html",
    "href": "posts/2024-04-17-ShinyConf2024.html",
    "title": "login: User Authentication for Shiny Applications",
    "section": "",
    "text": "The login package provides a framework for adding user authentication to Shiny applications. This is unique to other authentication frameworks such as ShinyManager and shinyauthr in that it provides tools for users to create their own accounts and reset passwords. This is particularly useful for Shiny applications used to collect data without a pre-existing user management system. User credentials are stored in any database that supports the DBI interface. Passwords are hashed using MD5 in the browser so that unencrypted passwords are never available to the Shiny server. For an extra layer of security, you can salt the password before storing it in the database. Cookie support is provided so that users do not have to re-enter their credentials when revisiting the application and user login and logout actives are logged to the database. Examples of how this package is used for collecting data from students will be presented.\nDownload slides\nFor more information about the project, visit: https://github.com/jbryer/login"
  },
  {
    "objectID": "posts/2021-02-12-Shiny_Apps_in_R_Packages.html",
    "href": "posts/2021-02-12-Shiny_Apps_in_R_Packages.html",
    "title": "Framework for Shiny Apps in R Packages",
    "section": "",
    "text": "TL;DR: You can test this approach using this Github Gist.\nR Shiny Apps have become a popular way of creating web applications in R. There are many ways of running Shiny Apps including locally in RStudio, on Shinyapps.io or installing the server software on your own host. I have been increasingly using Shiny apps as a way to demonstrate and interact with R Packages, especially packages I write for teaching purposes. Adding a Shiny app to an R package is relatively easy. In my use cases, I first put the application files (server.R, ui.R, and global.R) in the inst/shiny directory of my R package. I can then write a package function to run the Shiny app from the installed package directory using a function like this:\n\n#' My Shiny App\n#' @export\nmy_shiny_app &lt;- function() {\n    shiny::runApp(appDir = system.file('shiny', package='MY_PACKAGE_NAME'))\n}\n\nThis works very well when the entire app is self-contained. However, this does not work if you want to pass parameters to the Shiny app. In my situation, I want to be able to pass different data frames that I can interact with, but still have the Shiny app work if not parameters are passed. The first step to get this to work is to convert the server.R and ui.R scripts to functions within the R package. The code is largely the same, but instead of calling the functions we are going to assign them to shiny_server and shiny_ui, respectively. I have also included some minimal roxygen2 documentation. In particular, the functions need to be in the package’s export file.\n\n#' The Shiny App Server.\n#' @param input input set by Shiny.\n#' @param output output set by Shiny.\n#' @param session session set by Shiny.\n#' @export\nshiny_server &lt;- function(input, output, session) {\n    if(!exists('thedata', envir = parent.env(environment()), inherits = FALSE)) {\n        message('thedata not available, using default faithful...')\n        data(faithful, envir = environment())\n        thedata &lt;- faithful\n    }\n\n    output$environment &lt;- renderPrint(\n        print(ls(envir = parent.env(environment())))\n    )\n\n    output$thedata &lt;- renderTable({\n        return(thedata)\n    })\n}\n\n#' The Shiny App UI.\n#' @export\nshiny_ui &lt;- function() {\n    fluidPage(\n        titlePanel('Shiny Parameter Test'),\n        verbatimTextOutput('environment'),\n        tableOutput('thedata')\n    )\n}\n\nThis Shiny App doesn’t do a lot. It has one user variable, thedata, and the user interface includes the output of ls (i.e. what is in the executing environment) and the contents of thedata (presumed to be a data frame). The important feature here is the first five lines of the shiny_server. I first check to see if thedata exists using the !exists('thedata', envir = parent.env(environment()), inherits = FALSE) command. In short, if thedata is not present, I want to set it to a reasonable default value.\nWhen encapsulating the Shiny app in R scripts, using the runApp function with the appDir parameter is sufficient. In order to pass variables to the Shiny app, we need to control the environment the app is started in. Below, is a rewrite of the my_shiny_app app. First, we create a new environment that will contain all of our parameters. Since specifying the parameter is optional, we use the missing function to check to see if it has a value, and if so assign it to the new environment. We then set the environment to our server and ui functions the newly created environment that now contains our parameters. The rest is similar to creating Shiny apps in a single app.R file; create the app with the shinyApp function and start it with the runApp function, but with the app instead of a directory.\n\nmy_shiny_app &lt;- function(thedata, ...) {\n    shiny_env &lt;- new.env()\n    if(!missing(thedata)) {\n        print('Setting parameters')\n        assign('thedata', thedata, shiny_env)\n    }\n    environment(shiny_ui) &lt;- shiny_env\n    environment(shiny_server) &lt;- shiny_env\n    app &lt;- shiny::shinyApp(\n        ui = shiny_ui,\n        server = shiny_server\n    )\n    runApp(app, ...)\n}\n\nWe can now start the Shiny app with the my_shiny_app() function call. In the first instance, no parameters are passed to the app so the faithful data frame will be printed. The second and third calls will use the iris and mtcars data frames, respectively.\n\nmy_shiny_app()\nmy_shiny_app(thedata = iris)\nmy_shiny_app(thedata = mtcars)\n\n\nRunning on a Shiny server\nThe one disadvantage of this approach is that it is more difficult to run the Shiny app outside the package and maintaining the app in two formats is inconvenient. There are two approaches to this:\nOption 1: A simple app.R script\nWhen using a single R script for shiny (i.e. app.R) the key is that teh script must call shiny::shinyApp. Similar to the my_shiny_app function above, we can simply call the function with our UI and server functions. In this script we are sourcing the shiny_param_test.R script but when in a package you would replace that with loading the package and references the UI and shiny functions in the package when calling shinyApp.\n\nlibrary(shiny)\nsource('shiny_param_test.R')\nshiny::shinyApp(ui = shiny_ui,\n                server = shiny_server)\n\nOption 2: Generate R scripts\nThe save_shiny_app function below will save the server and ui functions in the package to a server.R and ui.R script files in the specified directory. Additionally, it will create a global.R file that loads the shiny package and any other required packages as specified in the pkgs parameter.\n\n#' Save the Shiny App to ui.R, server.R, and global.R file.\n#'\n#' This function will create three files in the \\code{out_dir}: \\code{server.R},\n#' \\code{ui.R}, and \\code{global.R}. The contents of \\code{server.R} and\n#' \\code{ui.R} will be the source code of the \\code{server_function} and\n#' \\code{ui_function}, respectively. The \\code{global.R} file will only contain\n#' \\code{library} calls for \\code{shiny} and any other packages specified in\n#' the \\code{pkgs} parameter.\n#'\n#' If \\code{run_app = TRUE} the function will start the Shiny app once the\n#' files are written. This is recommended to ensure all the necessary packages\n#' are loaded for the Shiny app to run.\n#'\n#' @param ui_function the function for the UI.\n#' @param server_function the function for the server.\n#' @param pkgs any packages that need to be loaded for the app to work. At\n#'        minimum the package containing the shiny app should be included.\n#' @param out_dir the directory to save the shiny app files.\n#' @param run_app whether to run the app once the files are saved.\nsave_shiny_app &lt;- function(ui_function,\n                           server_function,\n                           pkgs,\n                           out_dir = 'shiny',\n                           run_app = interactive()) {\n    server_txt &lt;- capture.output(server_function)\n    ui_txt &lt;- capture.output(ui_function)\n    # Remove the bytecode and environment info\n    server_txt &lt;- server_txt[1:(length(server_txt)-2)]\n    ui_txt &lt;- ui_txt[3:(length(ui_txt)-3)]\n    # Fix the function assignment\n    server_txt[1] &lt;- 'shinyServer(function(input, output, session)'\n    server_txt[length(server_txt)] &lt;- '})'\n    global_txt &lt;- c(\"library('shiny')\")\n    if(!missing(pkgs)) {\n        global_txt &lt;- c(global_txt, paste0(\"library('\", pkgs, \"')\"))\n    }\n    # Save the shiny app files\n    cat(server_txt, sep = '\\n', file = paste0(out_dir, '/server.R'))\n    cat(ui_txt, sep = '\\n', file = paste0(out_dir, '/ui.R'))\n    cat(global_txt, sep = '\\n', file = paste0(out_dir, '/global.R'))\n    # Start the app\n    if(run_app) {\n        runApp(appDir = out_dir)\n    }\n}"
  },
  {
    "objectID": "posts/2021-02-15-Map_my_run_in_R.html",
    "href": "posts/2021-02-15-Map_my_run_in_R.html",
    "title": "Map my run in R",
    "section": "",
    "text": "First, I want to give a plug to the RStats Strava Running Club. If you are into running, it is a great group that provides lots of support.\nThis post is inspired by this streetmaps tutorial over at ggplot2tutor.com on creating map artwork/posters. This post shows how to overlay running (which could be biking) routes.\nThe key for this to work is to get access to GPX (GPS Exchange format) files. I use an Apple watch to track my runs and the HealthFit App to sync my runs to Strava and Dropbox (note there are a lot of export options) where it will upload GPX files. For this post, I extracted two GPX files for when I ran the NYC Marathon in 2019 and the Disney Marathon in 2020.\nTo begin, I load the necessary R packages and define some variables so that the script can easily be modified for other maps.\n\nlibrary(tidyverse)\nlibrary(osmdata)\nlibrary(tmaptools)\nlibrary(XML)\n\npalette.nyc &lt;- c(\n    background = '#0850A4',\n    water = '#0850A4',\n    streets = '#3D88C7',\n    small_streets = '#3D88C7',\n    rivers = '#0850A4',\n    route = '#1B295A',\n    labels = '#3D88C7',\n    title = '#A8BACA'\n)\n\ngpx.file &lt;- '2019-11-03-NYC-Marathon.gpx'\npalette &lt;- palette.nyc\ntitle &lt;- 'NYC Marathon'\nsubtitle &lt;- 'November 3, 2009'\ntitle.hjust &lt;- 0 # 0 = left align; 1 = right aling\ndistance &lt;- \"mi\" # Distance unit, one of: \"m\", \"km\", \"mi\", and \"ft\"\n\nThe following R code reads in the GPX file (which is an XML file) and converts it to a matrix of longitude and latitude coordinates. The bb variable defines the minimum bounding rectangle that encompasses the entire route. This will define the bounds of the street data we will download.\n\ngpx &lt;- XML::htmlTreeParse(gpx.file, \n                     error = function (...) {}, useInternalNodes = T)\n\ncoords &lt;- xpathSApply(gpx, path = \"//trkpt\", xmlAttrs)\nlats &lt;- as.numeric(coords[\"lat\",])\nlons &lt;- as.numeric(coords[\"lon\",])\npath &lt;- data.frame(x = lons, y = lats)\nbb &lt;- matrix(c(min(path$x), min(path$y), max(path$x), max(path$y)),\n             nrow = 2, ncol = 2,\n             dimnames = list(c('x','y'), c('min', 'max')))\nbb\n\n        min       max\nx -74.06163 -73.92439\ny  40.60176  40.81475\n\n\nThe next chunk calculates the distance between each coordinate and the cumulative sum/distance. Note that this is an inefficient chunk since I used a for loop to calculate the distances.\n\npath$distance &lt;- 0\nfor(i in 2:nrow(path)) { \n    # Probably shouldn't use a loop, this is slow. Not sure what to use instead.\n    path[i,]$distance &lt;- as.numeric(\n        approx_distances(unlist(path[i - 1,,drop=TRUE]), unlist(path[i,,drop=TRUE]), \n                         target = distance, projection = 4326)\n    )\n}\npath$cum_distance &lt;- cumsum(path$distance)\n\nNext, we create a separate data frame for the mile markers. We could round up or down here. For NYC, my watch registered just under 26 miles so to ensure a 26 mile marker is shown, we will round up here. I presume the GPS lost accuracy when running on the lower level of bridges and/or through the buildings.\n\n# markers &lt;- path[!duplicated(floor(path$cum_distance)),][-1,]\nmarkers &lt;- path[!duplicated(ceiling(path$cum_distance), fromLast = TRUE),][-1,]\n\nThe next block of R code (which is largely copied from the ggplot2tutor tutorial) downloads map data from OpenStreet Map.\n\nstreets &lt;- bb %&gt;%\n    opq() %&gt;%\n    add_osm_feature(key = \"highway\", \n                    value = c(\"motorway\", \"primary\", \"trunk\",\n                              \"secondary\", \"tertiary\")) %&gt;%\n    osmdata_sf()\n\nsmall_streets &lt;- bb %&gt;%\n    opq() %&gt;%\n    add_osm_feature(key = \"highway\",\n                    value = c(\"residential\", \"living_street\",\n                              \"unclassified\",\n                              \"service\", \"footway\")) %&gt;%\n    osmdata_sf()\n\nriver &lt;- bb %&gt;%\n    opq() %&gt;%\n    add_osm_feature(key = \"waterway\", value = \"river\") %&gt;%\n    osmdata_sf()\n\nwater &lt;- bb %&gt;%\n    opq() %&gt;%\n    add_osm_feature(key = \"natural\", value = c('water')) %&gt;%\n    osmdata_sf()\n\nNow that the data is downloaded, we can begin building the map using ggplot2.\n\nmap &lt;- ggplot() +\n    geom_sf(data = water$osm_multipolygons,\n            inherit.aes = FALSE,\n            fill = palette['water'],\n            color = NA,\n            alpha = .3) +\n    geom_sf(data = streets$osm_lines,\n            inherit.aes = FALSE,\n            color = palette['streets'],\n            size = .3,\n            alpha = .6) +\n    geom_sf(data = small_streets$osm_lines,\n            inherit.aes = FALSE,\n            color = palette['small_streets'],\n            size = .1,\n            alpha = .5) +\n    geom_sf(data = river$osm_lines,\n            inherit.aes = FALSE,\n            color = palette['rivers'],\n            size = .2,\n            alpha = .3) +\n    geom_path(data = path, aes(x = x, y = y),\n              color = palette['route'],\n              size = 1) +\n    geom_point(data = markers, aes(x = x, y = y),\n               inherit.aes = FALSE,\n               color = palette['labels'],\n               fill = palette['route'],\n               shape = 21, stroke = 1, size = 5) +\n    geom_point(data = path[1,], aes(x = x, y = y),\n               inherit.aes = FALSE,\n               color = palette['labels'],\n               fill = 'green',\n               shape = 21, stroke = 1, size = 5) +\n    geom_point(data = path[nrow(path),], aes(x = x, y = y),\n               inherit.aes = FALSE,\n               color = palette['labels'],\n               fill = 'red',\n               shape = 21, stroke = 1, size = 5) +\n    geom_text(data = markers, aes(x = x, y = y, label = ceiling(cum_distance)),\n              inherit.aes = FALSE,\n              color = palette['labels'],\n              size = 2) +\n    coord_sf(xlim = bb[1,],\n             ylim = bb[2,]) +\n    theme_void() +\n    theme(plot.background = element_rect(fill = palette['background']))\n\nThe last modifies the them and adds a title. For the poster I printed, I excluded the title and instead added the marathon logo in Photoshop.\n\nmap + ggtitle(title,   \n              subtitle = subtitle) +\n    theme(panel.background = element_rect(fill = palette['background'], color = palette['background']),\n          panel.spacing = margin(0,0,0,0),\n          plot.margin = margin(-45,10,10,10),\n          plot.title = element_text(color = palette['title'],\n                                  size = 20,\n                                  hjust = title.hjust, vjust = -10,\n                                  family = 'Helvetica'),\n          plot.subtitle = element_text(color = palette['title'],\n                                     size = 16,\n                                     hjust = title.hjust, vjust = -12,\n                                     family = 'Helvetica'))\n\n\n\n\n\n\n\n\nThe ggsave will save the map to a file.\n\nggsave(filename = paste0(gsub(' ', '_', title), '.png'), width = 8)\n\nWith a few modifications to the variables set above, we can easily create another map for another race.\n\npalette.disney &lt;- c(\n    background = '#2775AE',\n    water = '#92C0E6',\n    streets = '#1A365D',\n    small_streets = '#1A365D',\n    rivers = '#92C0E6',\n    route = '#E6A356',\n    labels = '#1A365D',\n    title = '#1A365D'\n)\n\ngpx.file &lt;- '2020-01-12-Disney-Marathon.gpx'\npalette &lt;- palette.disney\ntitle &lt;- 'Walt Disney World Marathon'\nsubtitle &lt;- 'January 12, 2020'\ntitle.hjust &lt;- 1 # 0 = left align; 1 = right aling\ndistance &lt;- \"mi\" # Distance unit, one of: \"m\", \"km\", \"mi\", and \"ft\""
  },
  {
    "objectID": "posts/2025-03-23-Bootstrap_vs_Standard_Error.html",
    "href": "posts/2025-03-23-Bootstrap_vs_Standard_Error.html",
    "title": "Bootstrap vs Standard Error Confidence Intervals",
    "section": "",
    "text": "A student recently asked whether bootstrap confidence intervals were more robust than confidence intervals estimated using the standard error (i.e. \\(SE = \\frac{s}{\\sqrt{n}}\\)). In order to answer this question I wrote a function to simulate taking a bunch of random samples from a population, calculate the confidence interval for that sample using the standard error approach (the t distribution is used by default, see the cv parameter. To use the normal distribution, for example, set cv = 1.96.), and then also calculating a confidence interval using the boostrap.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n#' Simulate random samples to estimate confidence intervals and bootstrap\n#' estimates.\n#'\n#' @param pop a numeric vector representing the population.\n#' @param n sample size for each random sample from the population.\n#' @param n_samples the number of random samples.\n#' @param n_boot number of bootstrap samples to take for each sample.\n#' @param seed a seed to use for the random process.\n#' @param cv critical value to use for calculating confidence intervals.\n#' @return a data.frame with the sample and bootstrap mean and confidence\n#'        intervals along with a logical variable indicating whether a Type I\n#'        error would have occurred with that sample.\nbootstrap_clt_simulation &lt;- function(\n        pop,\n        n = 30,\n        n_samples = 500,\n        n_boot = 500,\n        cv = abs(qt(0.025, df = n - 1)),\n        seed,\n        verbose = interactive()\n) {\n    if(missing(seed)) {\n        seed &lt;- sample(100000)\n    }\n    results &lt;- data.frame(\n        seed = 1:n_samples,\n        samp_mean = numeric(n_samples),\n        samp_se = numeric(n_samples),\n        samp_ci_low = numeric(n_samples),\n        samp_ci_high = numeric(n_samples),\n        samp_type1 = logical(n_samples),\n        boot_mean = numeric(n_samples),\n        boot_ci_low = numeric(n_samples),\n        boot_ci_high = numeric(n_samples),\n        boot_type1 = logical(n_samples)\n    )\n    if(verbose) {\n        pb &lt;- txtProgressBar(min = 0, max = n_samples, style = 3)\n    }\n    for(i in 1:n_samples) {\n        if(verbose) {\n            setTxtProgressBar(pb, i)\n        }\n        set.seed(seed + i)\n        samp &lt;- sample(pop, size = n)\n        boot_samp &lt;- numeric(n_boot)\n        for(j in 1:n_boot) {\n            boot_samp[j] &lt;- sample(samp, size = length(samp), replace = TRUE) |&gt;\n                mean()\n        }\n        results[i,]$seed &lt;- seed + i\n        results[i,]$samp_mean &lt;- mean(samp)\n        results[i,]$samp_se &lt;- sd(samp) / sqrt(length(samp))\n        results[i,]$samp_ci_low &lt;- mean(samp) - cv * results[i,]$samp_se\n        results[i,]$samp_ci_high &lt;- mean(samp) + cv * results[i,]$samp_se\n        results[i,]$samp_type1 &lt;- results[i,]$samp_ci_low &gt; mean(pop) |\n            mean(pop) &gt; results[i,]$samp_ci_high\n        results[i,]$boot_mean &lt;- mean(boot_samp)\n        results[i,]$boot_ci_low &lt;- mean(boot_samp) - cv * sd(boot_samp)\n        results[i,]$boot_ci_high &lt;- mean(boot_samp) + cv * sd(boot_samp)\n        results[i,]$boot_type1 &lt;- results[i,]$boot_ci_low &gt; mean(pop) |\n            mean(pop) &gt; results[i,]$boot_ci_high\n    }\n    if(verbose) {\n        close(pb)\n    }\n    return(results)\n}\n\nUniform distribution for the population\nLet’s start with a uniform distribution for our population.\n\npop_unif &lt;- runif(1e5, 0, 1)\nggplot(data.frame(x = pop_unif), aes(x = x)) + geom_density()\n\n\n\n\n\n\n\n\nThe mean of the population is 0.5008915. We can now simulate samples and their corresponding bootstrap estimates.\n\nresults_unif &lt;- bootstrap_clt_simulation(pop = pop_unif, seed = 42, verbose = FALSE)\n\n5.8% of our samples did not contain the population mean in the confidence interval (i.e. Type I error rate) compared to rmean(results_unif$boot_type1) * 100`% of the bootstrap estimates. The following table compares the Type I errors for each sample compared to the bootstrap estiamted from that sample.\n\ntab &lt;- table(results_unif$samp_type1, results_unif$boot_type1, useNA = 'ifany')\ntab\n\n       \n        FALSE TRUE\n  FALSE   470    1\n  TRUE      1   28\n\n\nIn general committing a type I error is the same regardless of method, though there were 1 instances where the bootstrap would have led to a type I error rate where the standard error approach would not.\nThe following plots show the relationship between the estimated mean (left) and condifence interval width (right) for each sample and its corresponding bootstrap.\nresults_unif |&gt;\n    ggplot(aes(x = samp_mean, y = boot_mean)) +\n    geom_vline(xintercept = mean(pop_unif), color = 'blue') +\n    geom_hline(yintercept = mean(pop_unif), color = 'blue') +\n    geom_abline() +\n    geom_point() +\n    ggtitle(\"Sample mean vs bootstrap mean\")\nresults_unif |&gt;\n    dplyr::mutate(samp_ci_width = samp_ci_high - samp_ci_low,\n                  boot_ci_width = boot_ci_high - boot_ci_low) |&gt;\n    ggplot(aes(x = samp_ci_width, y = boot_ci_width)) +\n    geom_abline() +\n    geom_point() +\n    ggtitle('Sample vs boostrap confidence interval width')\n\n\n\n\n\n\n\n\n\n\nSkewed distribution for the population\nWe will repeat the same analysis using a positively skewed distribution.\n\npop_skewed &lt;- rnbinom(1e5, 3, .5)\nggplot(data.frame(x = pop_skewed), aes(x = x)) + geom_density(bw = 0.75)\n\n\n\n\n\n\n\n\nThe mean of the population for this distribution is 2.99792\n\nresults_skewed &lt;- bootstrap_clt_simulation(pop = pop_skewed, seed = 42, verbose = FALSE)\nmean(results_skewed$samp_type1) # Percent of samples with Type I error\n\n[1] 0.05\n\nmean(results_skewed$boot_type1) # Percent of bootstrap estimates with Type I error\n\n[1] 0.052\n\n# CLT vs Bootstrap Type I error rate\ntable(results_skewed$samp_type1, results_skewed$boot_type1, useNA = 'ifany')\n\n       \n        FALSE TRUE\n  FALSE   473    2\n  TRUE      1   24\n\n\nresults_skewed |&gt;\n    ggplot(aes(x = samp_mean, y = boot_mean)) +\n    geom_vline(xintercept = mean(pop_skewed), color = 'blue') +\n    geom_hline(yintercept = mean(pop_skewed), color = 'blue') +\n    geom_abline() +\n    geom_point() +\n    ggtitle(\"Sample mean vs bootstrap mean\")\nresults_skewed |&gt;\n    dplyr::mutate(samp_ci_width = samp_ci_high - samp_ci_low,\n                  boot_ci_width = boot_ci_high - boot_ci_low) |&gt;\n    ggplot(aes(x = samp_ci_width, y = boot_ci_width)) +\n    geom_abline() +\n    geom_point() +\n    ggtitle('Sample vs boostrap confidence interval width')\n\n\n\n\n\n\n\n\n\n\nWe can see the results are very similar to that of the uniform distirubtion. Exploring the one case where the bootstrap would have resulted in a Type I error where the standard error approach would not reveals that it is very close with the difference being less than 0.1.\n\nresults_differ &lt;- results_skewed |&gt;\n    dplyr::filter(!samp_type1 & boot_type1)\nresults_differ\n\n  seed samp_mean   samp_se samp_ci_low samp_ci_high samp_type1 boot_mean\n1  443  3.866667 0.4516466    2.942946     4.790388      FALSE  3.924733\n2  474  3.933333 0.4816956    2.948155     4.918511      FALSE  3.956800\n  boot_ci_low boot_ci_high boot_type1\n1    3.044802     4.804665       TRUE\n2    3.018549     4.895051       TRUE\n\n\n\nset.seed(results_differ[1,]$seed)\nsamp &lt;- sample(pop_skewed, size = 30)\nboot_samp &lt;- numeric(500)\nfor(j in 1:500) {\n    boot_samp[j] &lt;- sample(samp, size = length(samp), replace = TRUE) |&gt;\n        mean()\n}\ncv = abs(qt(0.025, df = 30 - 1))\nmean(pop_skewed)\n\n[1] 2.99792\n\nci &lt;- c(mean(samp) - cv * sd(samp) / sqrt(30), mean(samp) + cv * sd(samp) / sqrt(30))\nci\n\n[1] 2.942946 4.790388\n\nmean(pop_skewed) &lt; ci[1] | mean(pop_skewed) &gt; ci[2]\n\n[1] FALSE\n\nci_boot &lt;- c(mean(boot_samp) - cv * sd(boot_samp), mean(boot_samp) + cv * sd(boot_samp))\nci_boot\n\n[1] 3.044802 4.804665\n\nmean(pop_skewed) &lt; ci_boot[1] | mean(pop_skewed) &gt; ci_boot[2]\n\n[1] TRUE\n\n\n\nAdding an outlier\nLet’s consider a sample that forces the largest value from the population to be in the sample.\n\nset.seed(2112)\nsamp_outlier &lt;- c(sample(pop_skewed, size = 29), max(pop_skewed))\nboot_samp &lt;- numeric(500)\nfor(j in 1:500) {\n    boot_samp[j] &lt;- sample(samp, size = length(samp), replace = TRUE) |&gt;\n        mean()\n}\n\nci &lt;- c(mean(samp_outlier) - cv * sd(samp_outlier) / sqrt(30), mean(samp_outlier) + cv * sd(samp_outlier) / sqrt(30))\nci\n\n[1] 1.647006 4.952994\n\nmean(pop_skewed) &lt; ci[1] | mean(pop_skewed) &gt; ci[2]\n\n[1] FALSE\n\nci_boot &lt;- c(mean(boot_samp) - cv * sd(boot_samp), mean(boot_samp) + cv * sd(boot_samp))\nci_boot\n\n[1] 2.905153 4.781381\n\nmean(pop_skewed) &lt; ci_boot[1] | mean(pop_skewed) &gt; ci_boot[2]\n\n[1] FALSE\n\n\nIn this example we do see that the presense of the outlier does have a bigger impact on the confidence interval with the bootstrap confidence interval being much smaller.\n\n\nSample and bootstrap size related to standard error\nLet’s also explore the relationship of n, number of bootstrap samples, and standard error. Recall the formula for the standard error is:\n\\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\]\nThe figure below plots the standard error against the standard error assuming sigma (standard deviation) is one. As you can see, simply increasing the sample size will decrease the standard error (and therefore the confidence interval).\n\nse &lt;- function(n, sigma = 1) {\n    sigma / sqrt(n)\n}\nggplot() + stat_function(fun = se) + xlim(c(0, 100)) +\n    ylab('Standard Error') + xlab('Sample Size (n)')\n\n\n\n\n\n\n\n\nConsidering again a population with a uniform distribution, the following code will draw random samples with n ranging from 30 to 50 in increments of 15. For each of those random samples, we will also estimate boostrap standard errors with the number of bootstrap samples ranging from 50 to 1,000 in increments of 50.\n\nn &lt;- seq(30, 500, by = 15)\nn_boots &lt;- seq(50, 1000, by = 50)\n\nresults &lt;- expand.grid(n, n_boots)\nattributes(results) &lt;- NULL\nresults &lt;- as.data.frame(results)\nnames(results) &lt;- c('n', 'n_boots')\nresults$samp_mean &lt;- NA\nresults$samp_se &lt;- NA\nresults$boot_mean &lt;- NA\nresults$boot_se &lt;- NA\n\nfor(i in seq_len(nrow(results))) {\n    samp &lt;- sample(pop_unif, size = results[i,]$n)\n    results[i,]$samp_mean &lt;- mean(samp)\n    results[i,]$samp_se &lt;- sd(samp) / sqrt(length(samp))\n    boot_samp_dist &lt;- numeric(results[i,]$n_boots)\n    for(j in seq_len(results[i,]$n_boots)) {\n        boot_samp_dist[j] &lt;- sample(samp, size = length(samp), replace = TRUE) |&gt; mean()\n    }\n    results[i,]$boot_mean &lt;- mean(boot_samp_dist)\n    results[i,]$boot_se &lt;- sd(boot_samp_dist)\n}\n\nThe figure to the left plots the sample size against the standard error which, like above, shows that as the sample size increases the standard error decreases. On the right is a plot of the number of bootstrap samples against the standard error where the point colors correspond to the sample size. Here we see the standard error is constant. That is, the number of bootstrap samples is not related to the standard error. The variability in standard error is accounted for by the sample size.\n\ny_limits &lt;- c(0, 0.075)\np_samp_size_se &lt;- ggplot(results, aes(x = n, y = samp_se)) + \n    geom_point(fill = '#9ecae1', color = 'grey50', shape = 21) + \n    geom_smooth(color = 'darkgreen', se = FALSE, method = 'loess', formula = y ~ x) +\n    ylim(y_limits) +\n    ylab('Standard Error') +\n    xlab('Sample size (n)') +\n    ggtitle(latex2exp::TeX(\"Standard Error (SE = \\\\frac{\\\\sigma}{\\\\sqrt{n}})\")) +\n    scale_fill_gradient(low = '#deebf7', high = '#3182bd') +\n    theme(legend.position = 'bottom')\n\np_boot_size_se &lt;- \n    ggplot(results, aes(x = n_boots, y = boot_se)) + \n    geom_point(aes(fill = n), color = 'grey50', shape = 21) +\n    geom_smooth(color = 'darkgreen', se = FALSE, method = 'loess', formula = y ~ x) +\n    ylim(y_limits) +\n    ylab('Standard Error') +\n    xlab('Number of Bootstrap Samples') +\n    ggtitle('Bootstrap Standard Error',\n            subtitle = '(i.e. standard deviation of the bootstrap sample)') +\n    scale_fill_gradient(low = '#deebf7', high = '#3182bd') #+ theme(legend.position = 'none')\n\ncowplot::plot_grid(p_samp_size_se, p_boot_size_se)\n\n\n\n\n\n\n\n\nLastly we can plot the relationship between the two standard error estimates; the correlation of which is extremely high with r = 0.99.\n\nggplot(results, aes(x = samp_se, y = boot_se)) +\n    geom_abline() +\n    geom_point() +\n    xlab('Sample Standard Error') +\n    ylab('Boostrap Standard Error') +\n    ggtitle(paste0('Correlation between standard errors = ', round(cor(results$samp_se, results$boot_se), digits = 2))) +\n    coord_equal()"
  }
]